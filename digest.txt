Directory structure:
‚îî‚îÄ‚îÄ droid-dataset-droid/
    ‚îú‚îÄ‚îÄ README.md
    ‚îú‚îÄ‚îÄ Makefile
    ‚îú‚îÄ‚îÄ pyproject.toml
    ‚îú‚îÄ‚îÄ setup.py
    ‚îú‚îÄ‚îÄ .pre-commit-config.yaml
    ‚îú‚îÄ‚îÄ config/
    ‚îÇ   ‚îú‚îÄ‚îÄ fr3/
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ franka_hardware.yaml
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ franka_panda.yaml
    ‚îÇ   ‚îî‚îÄ‚îÄ panda/
    ‚îÇ       ‚îú‚îÄ‚îÄ franka_hardware.yaml
    ‚îÇ       ‚îî‚îÄ‚îÄ franka_panda.yaml
    ‚îú‚îÄ‚îÄ docs/
    ‚îÇ   ‚îú‚îÄ‚îÄ _config.yml
    ‚îÇ   ‚îú‚îÄ‚îÄ contribution-guidelines.md
    ‚îÇ   ‚îú‚îÄ‚îÄ Gemfile
    ‚îÇ   ‚îú‚îÄ‚îÄ index.md
    ‚îÇ   ‚îú‚îÄ‚îÄ the-droid-dataset.md
    ‚îÇ   ‚îú‚îÄ‚îÄ _sass/
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ color_schemes/
    ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ droid.scss
    ‚îÇ   ‚îú‚îÄ‚îÄ example-workflows/
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ calibrating-cameras.md
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data-collection.md
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ example-workflows.md
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ teleoperation.md
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ training-policies.md
    ‚îÇ   ‚îú‚îÄ‚îÄ hardware-setup/
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ assembly.md
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ hardware-setup.md
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ shopping-list.md
    ‚îÇ   ‚îî‚îÄ‚îÄ software-setup/
    ‚îÇ       ‚îú‚îÄ‚îÄ docker.md
    ‚îÇ       ‚îú‚îÄ‚îÄ host-installation.md
    ‚îÇ       ‚îî‚îÄ‚îÄ software-setup.md
    ‚îú‚îÄ‚îÄ droid/
    ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
    ‚îÇ   ‚îú‚îÄ‚îÄ robot_env.py
    ‚îÇ   ‚îú‚îÄ‚îÄ calibration/
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ calibration_info.json
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ calibration_utils.py
    ‚îÇ   ‚îú‚îÄ‚îÄ camera_utils/
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ info.py
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ camera_readers/
    ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
    ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ zed_camera.py
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ recording_readers/
    ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ mp4_reader.py
    ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ svo_reader.py
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ wrappers/
    ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
    ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ multi_camera_wrapper.py
    ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ recorded_multi_camera_wrapper.py
    ‚îÇ   ‚îú‚îÄ‚îÄ controllers/
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ oculus_controller.py
    ‚îÇ   ‚îú‚îÄ‚îÄ data_loading/
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data_loader.py
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dataset.py
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tf_data_loader.py
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ trajectory_sampler.py
    ‚îÇ   ‚îú‚îÄ‚îÄ data_processing/
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data_transforms.py
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ timestep_processing.py
    ‚îÇ   ‚îú‚îÄ‚îÄ evaluation/
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ eval_launcher.py
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ eval_launcher_robomimic.py
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ policy_wrapper.py
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ rt1_wrapper.py
    ‚îÇ   ‚îú‚îÄ‚îÄ franka/
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ launch_gripper.sh
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ launch_robot.sh
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ robot.py
    ‚îÇ   ‚îú‚îÄ‚îÄ misc/
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ parameters.py
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pointcloud_utils.py
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ server_interface.py
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ subprocess_utils.py
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ time.py
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ transformations.py
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ version_control/
    ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ 1_0.json
    ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ 1_1.json
    ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ loader.py
    ‚îÇ   ‚îú‚îÄ‚îÄ plotting/
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ analysis_func.py
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ misc.py
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ text.py
    ‚îÇ   ‚îú‚îÄ‚îÄ postprocessing/
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ parse.py
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ schema.py
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ stages.py
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ util/
    ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
    ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ svo2depth.py
    ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ svo2mp4.py
    ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ validate.py
    ‚îÇ   ‚îú‚îÄ‚îÄ robot_ik/
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ arm.py
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ robot_ik_solver.py
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ franka/
    ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ fr3.xml
    ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ panda.xml
    ‚îÇ   ‚îú‚îÄ‚îÄ training/
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ model_trainer.py
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ models/
    ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ policy_network.py
    ‚îÇ   ‚îú‚îÄ‚îÄ trajectory_utils/
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ misc.py
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ trajectory_reader.py
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ trajectory_writer.py
    ‚îÇ   ‚îî‚îÄ‚îÄ user_interface/
    ‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
    ‚îÇ       ‚îú‚îÄ‚îÄ data_collector.py
    ‚îÇ       ‚îú‚îÄ‚îÄ eval_gui.py
    ‚îÇ       ‚îú‚îÄ‚îÄ gui.py
    ‚îÇ       ‚îú‚îÄ‚îÄ gui_info.json
    ‚îÇ       ‚îú‚îÄ‚îÄ gui_parameters.py
    ‚îÇ       ‚îú‚îÄ‚îÄ misc.py
    ‚îÇ       ‚îî‚îÄ‚îÄ text.py
    ‚îú‚îÄ‚îÄ scripts/
    ‚îÇ   ‚îú‚îÄ‚îÄ README.md
    ‚îÇ   ‚îú‚îÄ‚îÄ main.py
    ‚îÇ   ‚îú‚îÄ‚îÄ postprocess.py
    ‚îÇ   ‚îú‚îÄ‚îÄ convert/
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ svo_to_depth.py
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ svo_to_mp4.py
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ to_tfrecord.py
    ‚îÇ   ‚îú‚îÄ‚îÄ evaluation/
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ evaluate_policy.py
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ evaluate_rt1.py
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ results.ipynb
    ‚îÇ   ‚îú‚îÄ‚îÄ labeling/
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ label_data.py
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ task_label_filepath.json
    ‚îÇ   ‚îú‚îÄ‚îÄ server/
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ launch_server.sh
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ run_server.py
    ‚îÇ   ‚îú‚îÄ‚îÄ setup/
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ README.md
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ intro.txt
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ laptop_setup.sh
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ nuc_setup.sh
    ‚îÇ   ‚îú‚îÄ‚îÄ tests/
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ calibrate_cameras.py
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ collect_trajectory.py
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ memory_leak.py
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ replay_trajectory.py
    ‚îÇ   ‚îú‚îÄ‚îÄ training/
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sweepable_train_policy [wip].py
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train_policy.py
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ sanity_check/
    ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ image_obs.py
    ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ state_obs.py
    ‚îÇ   ‚îî‚îÄ‚îÄ visualizations/
    ‚îÇ       ‚îú‚îÄ‚îÄ create_plots.py
    ‚îÇ       ‚îú‚îÄ‚îÄ visualize_data.py
    ‚îÇ       ‚îú‚îÄ‚îÄ visualize_day.py
    ‚îÇ       ‚îî‚îÄ‚îÄ visualize_trajectory.py
    ‚îú‚îÄ‚îÄ .docker/
    ‚îÇ   ‚îú‚îÄ‚îÄ README.md
    ‚îÇ   ‚îú‚îÄ‚îÄ laptop/
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ docker-compose-laptop-data-upload.yaml
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ docker-compose-laptop.yaml
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile.laptop
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile.laptop_fr3
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile.laptop_panda
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ entrypoint.sh
    ‚îÇ   ‚îî‚îÄ‚îÄ nuc/
    ‚îÇ       ‚îú‚îÄ‚îÄ docker-compose-nuc.yaml
    ‚îÇ       ‚îú‚îÄ‚îÄ Dockerfile.nuc
    ‚îÇ       ‚îú‚îÄ‚îÄ Dockerfile.nuc_fr3
    ‚îÇ       ‚îî‚îÄ‚îÄ Dockerfile.nuc_panda
    ‚îî‚îÄ‚îÄ .github/
        ‚îî‚îÄ‚îÄ workflows/
            ‚îú‚îÄ‚îÄ build_container_laptop_fr3.yaml
            ‚îú‚îÄ‚îÄ build_container_laptop_panda.yaml
            ‚îú‚îÄ‚îÄ build_container_nuc_fr3.yaml
            ‚îú‚îÄ‚îÄ build_container_nuc_panda.yaml
            ‚îú‚îÄ‚îÄ pages.yaml
            ‚îî‚îÄ‚îÄ pre-commit.yaml

================================================
FILE: README.md
================================================
# The DROID Robot Platform

This repository contains the code for setting up your DROID robot platform and using it to collect teleoperated demonstration data. This platform was used to collect the [DROID dataset](https://droid-dataset.github.io), a large, in-the-wild dataset of robot manipulations.

If you are interested in using the DROID dataset for training robot policies, please check out our [policy learning repo](https://github.com/droid-dataset/droid_policy_learning).
For more information about DROID, please see the following links: 

[**[Homepage]**](https://droid-dataset.github.io) &ensp; [**[Documentation]**](https://droid-dataset.github.io/droid) &ensp; [**[Paper]**](https://arxiv.org/abs/2403.12945) &ensp; [**[Dataset Visualizer]**](https://droid-dataset.github.io/dataset.html).

![](https://droid-dataset.github.io/droid/assets/index/droid_teaser.jpg)

---------
## Setup Guide

We assembled a step-by-step guide for setting up the DROID robot platform in our [developer documentation](https://droid-dataset.github.io/droid).
This guide has been used to set up 18 DROID robot platforms over the course of the DROID dataset collection. Please refer to the steps in this guide for setting up your own robot. Specifically, you can follow these key steps:

1. [Hardware Assembly and Setup](https://droid-dataset.github.io/droid/docs/hardware-setup)
2. [Software Installation and Setup](https://droid-dataset.github.io/droid/docs/software-setup)
3. [Example Workflows to collect data or calibrate cameras](https://droid-dataset.github.io/droid/docs/example-workflows)

If you encounter issues during setup, please raise them as issues in this github repo.



================================================
FILE: Makefile
================================================
.PHONY: help check autoformat
.DEFAULT: help

# Generates a useful overview/help message for various make features - add to this as necessary!
help:
	@echo "make check"
	@echo "    Run code style and linting (black, ruff) *without* changing files!"
	@echo "make autoformat"
	@echo "    Run code styling (black, ruff) and update in place - committing with pre-commit also does this."

check:
	black --check .
	ruff check --show-source .

autoformat:
	black .
	ruff check --fix --show-fixes .



================================================
FILE: pyproject.toml
================================================
[build-system]
requires = ["setuptools"]
build-backend = "setuptools.build_meta"

[project]
name = "droid"
authors = [
    {name = "Alexander Khazatsky "}
]
description = "Distributed Robot Interaction Dataset"
version = "0.0.1"
readme = "README.md"
requires-python = ">=3.7"
keywords = ["robotics"]
dependencies = [
    "boto3",
    "customtkinter",
    "dm-control==1.0.5",
    "gym",
    "h5py",
    "imageio",
    "matplotlib",
    "mujoco==2.3.2",
    "open3d",
    "opencv-python==4.6.0.66",
    "opencv-contrib-python==4.6.0.66",
    "Pillow",
    "protobuf==3.20.1",
    "psutil",
    "pyrallis",
    "scipy",
    "tqdm",
    "zerorpc",
]

[project.optional-dependencies]
dev = [
    "black[jupyter]",
    "ipython",
    "pre-commit",
    "ruff",
]
postprocessing = [
    "scikit-image",
]

[project.urls]
homepage = "https://github.com/droid-dataset/droid"
repository = "https://github.com/droid-dataset/droid"
documentation = "https://github.com/droid-dataset/droid"

[tool.black]
line-length = 121
target-version = ["py37", "py38", "py39", "py310"]
preview = true

[tool.ruff]
line-length = 121
target-version = "py38"
select = ["A", "B", "C90", "E", "F", "I", "RUF", "W"]
ignore = ["F405", "B006", "C901", "F403", "E722"]
[tool.ruff.per-file-ignores]
"__init__.py" = ["E402", "F401"]

[tool.isort]
profile = "black"
line_length = 121
skip = ["__init__.py"]
filter_files = true
py_version = "all"

[tool.setuptools.packages.find]
where = ["."]
exclude = ["cache"]



================================================
FILE: setup.py
================================================
"""
setup.py
PEP 621 switches most of Packaging to `pyproject.toml` -- yet keep a "dummy" setup.py for external code that has not
yet upgraded.
"""
from setuptools import setup

setup()



================================================
FILE: .pre-commit-config.yaml
================================================
# See https://pre-commit.com for more information
# See https://pre-commit.com/hooks.html for more hooks
exclude: ".git"
default_stages:
  - commit

repos:
  - repo: https://github.com/charliermarsh/ruff-pre-commit
    rev: v0.0.252
    hooks:
      - id: ruff
        args: [ --fix, --exit-non-zero-on-fix ]

  - repo: https://github.com/timothycrosley/isort
    rev: 5.12.0
    hooks:
      - id: isort

  - repo: https://github.com/psf/black
    rev: 23.1.0
    hooks:
      - id: black

  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.4.0
    hooks:
      - id: check-added-large-files
        args: ["--maxkb=40000"]
      - id: check-ast
      - id: check-case-conflict
      - id: check-merge-conflict
      - id: check-toml
      - id: check-yaml
      - id: end-of-file-fixer
      - id: trailing-whitespace
      - id: debug-statements



================================================
FILE: config/fr3/franka_hardware.yaml
================================================
hz: 1000
use_real_time: true
exec: franka_panda_client

robot_client:
  _target_: polymetis.robot_client.executable_robot_client.ExecutableRobotClient
  use_real_time: ${use_real_time}
  metadata_cfg:
    _target_: polymetis.robot_client.metadata.RobotClientMetadata
    default_Kq: [40, 30, 50, 25, 35, 25, 10]
    default_Kqd: [4, 6, 5, 5, 3, 2, 1]
    default_Kx: [400, 400, 400, 15, 15, 15]
    default_Kxd: [37, 37, 37, 2, 2, 2]
    hz: ${hz}
    robot_model_cfg: ${robot_model}
  executable_cfg:
    robot_ip: "172.16.0.1"
    control_ip: ${ip}
    control_port: ${port}
    readonly: false
    mock: false
    use_real_time: ${use_real_time}
    hz: ${hz}
    num_dofs: ${robot_model.num_dofs}
    exec: ${exec}
    robot_client_metadata_path: ???

    limit_rate: true
    lpf_cutoff_frequency: 100

    limits: 
      # bounding box of the workspace
      cartesian_pos_upper:
        - 1.0
        - 1.0
        - 1.0
      cartesian_pos_lower:
        - -1.0
        - -1.0
        - -1.0

      # the remaining limits are set to the original franka limits minus a margin
      joint_pos_upper: #margin: 0.1 rad
        - 2.65
        - 1.68
        - 2.80
        - -0.16
        - 2.70
        - 4.40
        - 2.90
      joint_pos_lower: #margin: 0.1 rad
        - -2.65
        - -1.68
        - -2.80
        - -2.95
        - -2.70
        - 0.45
        - -2.90
      joint_vel: #margin: 0.1 rad/s
        - 2.075
        - 2.075
        - 2.075
        - 2.075
        - 2.51
        - 2.51
        - 2.51
      elbow_vel: 2.075      #margin: 0.1 rad/s
      joint_torques:  #margin: 1N for first 4 joints, 0.5N for last 3 joints
        - 86.0
        - 86.0
        - 86.0
        - 86.0
        - 11.5
        - 11.5
        - 11.5

    collision_behavior:
      lower_torque: [40.0, 40.0, 40.0, 40.0, 40.0, 40.0, 40.0]
      upper_torque: [40.0, 40.0, 40.0, 40.0, 40.0, 40.0, 40.0]
      lower_force: [40.0, 40.0, 40.0, 40.0, 40.0, 40.0]
      upper_force: [40.0, 40.0, 40.0, 40.0, 40.0, 40.0]

    safety_controller:
      is_active: true
      margins: # margin from hard safety limits at which safety controllers start to kick in
        cartesian_pos: 0.05
        joint_pos: 0.2
        joint_vel: 0.5
      stiffness:
        cartesian_pos: 200.0
        joint_pos: 50.0
        joint_vel: 20.0



================================================
FILE: config/fr3/franka_panda.yaml
================================================
# @package _group_
robot_description_path: "franka_panda/panda_arm.urdf"
controlled_joints:  [0, 1, 2, 3, 4, 5, 6]
num_dofs: 7
ee_link_idx: 7
ee_link_name: panda_link8
rest_pose: [-0.13935425877571106, -0.020481698215007782, -0.05201413854956627, -2.0691256523132324, 0.05058913677930832, 2.0028650760650635, -0.9167874455451965]
joint_limits_low: [-2.7437, -1.7837, -2.9007, -3.0421, -2.8065, 0.5445, -3.0159]
joint_limits_high: [2.7437, 1.7837, 2.9007, -0.1518, 2.8065, 4.5169, 3.0159]
joint_damping: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
torque_limits: [87., 87., 87., 87., 12., 12., 12.]



================================================
FILE: config/panda/franka_hardware.yaml
================================================
hz: 1000
use_real_time: true
exec: franka_panda_client

robot_client:
  _target_: polymetis.robot_client.executable_robot_client.ExecutableRobotClient
  use_real_time: ${use_real_time}
  metadata_cfg:
    _target_: polymetis.robot_client.metadata.RobotClientMetadata
    default_Kq: [40, 30, 50, 25, 35, 25, 10]
    default_Kqd: [4, 6, 5, 5, 3, 2, 1]
    default_Kx: [400, 400, 400, 15, 15, 15]
    default_Kxd: [37, 37, 37, 2, 2, 2]
    hz: ${hz}
    robot_model_cfg: ${robot_model}
  executable_cfg:
    robot_ip: "172.16.0.2"
    control_ip: ${ip}
    control_port: ${port}
    readonly: false
    mock: false
    use_real_time: ${use_real_time}
    hz: ${hz}
    num_dofs: ${robot_model.num_dofs}
    exec: ${exec}
    robot_client_metadata_path: ???

    limit_rate: true
    lpf_cutoff_frequency: 100

    limits: 
      # bounding box of the workspace
      cartesian_pos_upper:
        - 1.0
        - 1.0
        - 1.0
      cartesian_pos_lower:
        - -1.0
        - -1.0
        - -1.0

      # the remaining limits are set to the original franka limits minus a margin
      joint_pos_upper: #margin: 0.1 rad
        - 2.80
        - 1.66
        - 2.80
        - -0.17
        - 2.80
        - 3.65
        - 2.80
      joint_pos_lower: #margin: 0.1 rad
        - -2.80
        - -1.66
        - -2.80
        - -2.97
        - -2.80
        - 0.08
        - -2.80
      joint_vel: #margin: 0.1 rad/s
        - 2.075
        - 2.075
        - 2.075
        - 2.075
        - 2.51
        - 2.51
        - 2.51
      elbow_vel: 2.075      #margin: 0.1 rad/s
      joint_torques:  #margin: 1N for first 4 joints, 0.5N for last 3 joints
        - 86.0
        - 86.0
        - 86.0
        - 86.0
        - 11.5
        - 11.5
        - 11.5

    collision_behavior:
      lower_torque: [40.0, 40.0, 40.0, 40.0, 40.0, 40.0, 40.0]
      upper_torque: [40.0, 40.0, 40.0, 40.0, 40.0, 40.0, 40.0]
      lower_force: [40.0, 40.0, 40.0, 40.0, 40.0, 40.0]
      upper_force: [40.0, 40.0, 40.0, 40.0, 40.0, 40.0]

    safety_controller:
      is_active: true
      margins: # margin from hard safety limits at which safety controllers start to kick in
        cartesian_pos: 0.05
        joint_pos: 0.2
        joint_vel: 0.5
      stiffness:
        cartesian_pos: 200.0
        joint_pos: 50.0
        joint_vel: 20.0



================================================
FILE: config/panda/franka_panda.yaml
================================================
# @package _group_
robot_description_path: "franka_panda/panda_arm.urdf"
controlled_joints:  [0, 1, 2, 3, 4, 5, 6]
num_dofs: 7
ee_link_idx: 7
ee_link_name: panda_link8
rest_pose: [-0.13935425877571106, -0.020481698215007782, -0.05201413854956627, -2.0691256523132324, 0.05058913677930832, 2.0028650760650635, -0.9167874455451965]
joint_limits_low: [-2.8973, -1.7628, -2.8973, -3.0718, -2.8973, -0.0175, -2.8973]
joint_limits_high: [2.8973, 1.7628, 2.8973, -0.0698, 2.8973, 3.7525, 2.8973]
joint_damping: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
torque_limits: [87., 87., 87., 87., 12., 12., 12.]



================================================
FILE: docs/_config.yml
================================================
title: DROID Docs
description: Developer documentation for recreating the DROID platform setup.
theme: just-the-docs

url: https://droid-dataset.github.io/droid # this url not being used right now
color_scheme: droid # TODO: update name to DROID

aux_links:
  Official GitHub Repository: https://github.com/droid-dataset/droid



================================================
FILE: docs/contribution-guidelines.md
================================================
---
layout: default
title: Contribution Guidelines
nav_order: 6
---

# Contribution Guidelines

Community contributions to broader robotics community data collection efforts are welcomed. The platform developed in the DROID project can be used to curate datasets to be contributed to community data collection efforts such as the [Open X-Embodiment](https://robotics-transformer-x.github.io/) data collection effort. 

To contribute to the Open X-Embodiment project consider enrolling your dataset using the following [form](https://docs.google.com/forms/d/e/1FAIpQLSeYinS_Y5Bf1ufTnlROULVquD4gw6xY_wUBssfVYkHNaPp4LQ/viewform) and/or reaching out to the associated project via the email <open-x-embodiment@googlegroups.com>.



================================================
FILE: docs/Gemfile
================================================
source 'https://rubygems.org'

gem "jekyll", "~> 4.3.3" # installed by `gem jekyll`
# gem "webrick"        # required when using Ruby >= 3 and Jekyll <= 4.2.2

gem "just-the-docs", "0.7.0" # pinned to the current release
# gem "just-the-docs"        # always download the latest release



================================================
FILE: docs/index.md
================================================
---
layout: default
title: Introduction
nav_order: 1
---

# ü§ñ **D**istributed **RO**bot **I**nteraction **D**ataset

![](./assets/index/droid_teaser.jpg)


## üëã Welcome to the DROID Developer Documentation

This goal of this documentation site is to enable robotics researchers to:

1. Replicate the hardware of the DROID data collection platform.
2. Configure software to make the DROID data collection platform operational.
3. Onboard developers/users on using the platform and contributing data to the DROID.

The guides for accomplishing these goals are split into the following high-level sections:

* üî® [**Hardware Setup:**](https://droid-dataset.github.io/droid/docs/hardware-setup) a list of the required hardware (with links to suppliers) and a guide for assembling the platform.
* üñ•Ô∏è [**Software Setup:**](https://droid-dataset.github.io/droid/docs/software-setup) guides on configuring device settings and software. 
* üìà [**The DROID Dataset:**](https://droid-dataset.github.io/droid/the-droid-dataset) an in-the-wild robot manipulation dataset.
* ü§ñ [**Example Workflows:**](https://droid-dataset.github.io/droid/docs/example-workflows) tutorials for common workflows (e.g. data collection, policy deployment).
* üìñ [**Contribution Guidelines:**](https://droid-dataset.github.io/droid/contribution-guidelines) guidelines for contributing data to DROID. 






================================================
FILE: docs/the-droid-dataset.md
================================================
---
layout: default
title: The DROID Dataset
nav_order: 4
---

## üîç Exploring the Dataset

It is possible to interactively explore the DROID via the following [interactive dataset visualizer](https://droid-dataset.github.io/dataset.html). This is a great way to start understanding the DROID dataset and is a highly recommended starting point.

<a href="https://droid-dataset.github.io/dataset.html"><img src="./assets/the-droid-dataset/droid_data_visualizer.png" alt="image" width="90%" height="auto"></a>

Additionally, we provide a [Dataset Colab](https://colab.research.google.com/drive/1b4PPH4XGht4Jve2xPKMCh-AXXAQziNQa?usp=sharing) that demonstrates how to load and visualize samples from our dataset.

## üìà Using the Dataset

The DROID dataset is hosted within a Google Cloud Bucket and is offered in two formats:

1. [RLDS](https://github.com/google-research/rlds): ideal for dataloading for the purpose of training policies
2. Raw Data: ideal for those who wish to manipulate the raw data, use high-resolution images or stereo/depth information.

The DROID dataset is hosted on a Google cloud bucket. To download it, install the [gsutil package](https://cloud.google.com/storage/docs/gsutil_install). We provide three different versions of the dataset for download:
```
# Full DROID dataset in RLDS (1.7TB)
gsutil -m cp -r gs://gresearch/robotics/droid <path_to_your_target_dir>

# Example 100 episodes from the DROID dataset in RLDS for debugging (2GB)
gsutil -m cp -r gs://gresearch/robotics/droid_100 <path_to_your_target_dir>

# Raw DROID dataset in stereo HD, stored as MP4 videos (8.7TB)
gsutil -m cp -r gs://gresearch/robotics/droid_raw <path_to_your_target_dir>

# Raw DROID dataset, non-stereo HD video only (5.6TB, excluding stereo video & raw SVO cam files)
gsutil -m rsync -r -x ".*SVO.*|.*stereo.*\.mp4$" "gs://gresearch/robotics/droid_raw" <path_to_your_target_dir>
```

### Accessing RLDS Dataset

We provide a [Dataset Colab](https://colab.research.google.com/drive/1b4PPH4XGht4Jve2xPKMCh-AXXAQziNQa?usp=sharing) that walks you through the process of loading and visualizing a few samples from the DROID dataset. 

We also provide an example of a "training-ready" data loader that allows for efficient loading of DROID data for policy training (in PyTorch and JAX), including parallelized loading, normalization and augmentation in our [policy learning repo](https://github.com/droid-dataset/droid_policy_learning/blob/master/examples/droid_dataloader.py).


## üìù Dataset Schema

The following fields are contained in every RLDS episode:
```python
DROID = {
        "episode_metadata": {
                "recording_folderpath": tf.Text, # path to the folder of recordings
                "file_path": tf.Text, # path to the original data file
                },
	"steps": {
		"is_first": tf.Scalar(dtype=bool), # true on first step of the episode
                "is_last": tf.Scalar(dtype=bool), # true on last step of the episode
        	"is_terminal": tf.Scalar(dtype=bool), # true on last step of the episode if it is a terminal step, True for demos
                                
                "language_instruction": tf.Text, # language instruction
                "language_instruction_2": tf.Text, # alternative language instruction
                "language_instruction_3": tf.Text, # alternative language instruction
                "observation": {
                                "gripper_position": tf.Tensor(1, dtype=float64), # gripper position state
                                "cartesian_position": tf.Tensor(6, dtype=float64), # robot Cartesian state
                                "joint_position": tf.Tensor(7, dtype=float64), # joint position state
                                "wrist_image_left": tf.Image(180, 320, 3, dtype=uint8), # wrist camera RGB left viewpoint        
                                "exterior_image_1_left": tf.Image(180, 320, 3, dtype=uint8), # exterior camera 1 left viewpoint
                                "exterior_image_2_left": tf.Image(180, 320, 3, dtype=uint8), # exterior camera 2 left viewpoint
                		},                            
                "action_dict": {
                                "gripper_position": tf.Tensor(1, dtype=float64), # commanded gripper position
                                "gripper_velocity": tf.Tensor(1, dtype=float64), # commanded gripper velocity
                                "cartesian_position": tf.Tensor(6, dtype=float64), # commanded Cartesian position
                                "cartesian_velocity": tf.Tensor(6, dtype=float64), # commanded Cartesian velocity
                                "joint_position": tf.Tensor(7, dtype=float64),  # commanded joint position
                        	"joint_velocity": tf.Tensor(7, dtype=float64), # commanded joint velocity
                		},
		"discount": tf.Scalar(dtype=float32), # discount if provided, default to 1
                "reward": tf.Scalar(dtype=float32), # reward if provided, 1 on final step for demos
                "action": tf.Tensor(7, dtype=float64), # robot action, consists of [6x joint velocities, 1x gripper position]
	},
}
```

### Accessing Raw Data

You can download the raw DROID data using the gsutil command listed above. It contains full-HD stereo videos for all three cameras, alongside with all other information contained in the RLDS dataset. Concretely, each episode folder contains the following information:
```
episode:
   |
   |---- metadata_*.json: Episode metadata like building ID, data collector ID etc.
   |---- trajectory.h5: All low-dimensional information like action and proprioception trajectories.
   |---- recordings:
             |
             |---- MP4:
             |      |
             |      |---- *.mp4: High-res video of single (left) camera view.
             |      |---- *-stereo.mp4: High-res video of concatenated stereo camera views.
             |
             |---- SVO:
                    |
                    |---- *.svo: Raw ZED SVO file with encoded camera recording information (contains some additional metadata)

```

**Note**: We realized that we missed 20% of episodes when face-blurring & copying the *raw* DROID data to the release bucket and are working on uploading the remainder of the dataset. This only affects the *raw* version of DROID, the RLDS version is complete. This should be fixed within a few days -- please reach out to pertsch@berkeley.edu if you have any concerns in the meantime!

## üìÑ Data Analysis and Further Information
Please consult the [paper](https://droid-dataset.github.io/paper.pdf) for detailed data analysis and further information about the dataset.



================================================
FILE: docs/_sass/color_schemes/droid.scss
================================================
@import "./color_schemes/light";

// add custom colours here
$cardinal-red: #8C1515; // can be updated assumed primary author's university color scheme

$link-color: $cardinal-red;
$btn-primary-color: $cardinal-red;



================================================
FILE: docs/example-workflows/calibrating-cameras.md
================================================
---
layout: default
title: Calibrating Cameras
parent: Example Workflows
nav_order: 2
---

# Prerequisites

This guide assumes you have already setup DROID application software to run on your host machine or through Docker. Proceed with this guide having launched the control server on the NUC and the GUI application on the laptop.

# Calibrating Cameras

The GUI will let you know if any of your camera‚Äôs have yet to be calibrated, or if any of your camera‚Äôs haven‚Äôt been calibrated in a while (this is to make sure you don‚Äôt accidentally move them and forget). Since the hand camera is relatively fixed, it is okay to calibrate significantly less frequently than the other cameras, but it is still good practice to calibrate it every now and then.

Useful Information:
* If calibration is successful, you will be brought back to the calibration hub. If it is unsuccessful, the GUI will inform you as such.
* During calibration, you will see a visualization of the pose estimation of the charuco board. Stable, green boxes along with stable axes lines are good! When these are not present, a calibration is likely to fail. If things start to significantly jumble during the automated calibration procedure, feel free to press B and try again.
* Because the board is heavy, the robot may move differently when the board is attached to the gripper.
* **WARNING:** If A button click is taking a really long time. The camera might have failed. You can confirm this by looking in the terminal for a line that resembles ‚Äúcan‚Äôt claim interface‚Ä¶‚Äù. The solution is to simply close the GUI, and load it up again. Your previous calibration info will not be lost.

## Mounting Calibration Board

Please follow the instructions in the assembly guide for mounting the calibration board.

## Calibrating a 3rd-Person Camera

<iframe src="https://drive.google.com/file/d/1kcE4YzeJYJLbKanY2N8R66tiCqWjk6ij/preview" width="640" height="480" allow="autoplay"></iframe>

## Calibration a Hand-Mounted Camera

<iframe src="https://drive.google.com/file/d/1Raqb35VDrr4YvSBaola5kSR-QFNKZ32w/preview" width="640" height="480" allow="autoplay"></iframe>



================================================
FILE: docs/example-workflows/data-collection.md
================================================
---
layout: default
title: Collecting Data
parent: Example Workflows
nav_order: 3
---

# Prerequisites

**Important:** Before proceeding please ensure you have followed the camera calibration guide and calibrated your cameras. 

This guide assumes you have already setup DROID application software to run on your host machine or through Docker. Proceed with this guide having launched the control server on the NUC and the GUI application on the laptop.

# Collecting Data

## Using the GUI

* The GUI will start by prompting you to enter your name.
	* Note 1: Your trajectories will be associated with the name you type in. So, make sure to be consistent, and type in your full name! The login page will not let you pass until you have done so.
	* Note 2: Press shift to see the camera feed. Anytime you see a camera feed, confirm that there are exactly 6 images on screen. If there is anything different, halt data collection, unplug and replug the 3 camera wires, and restart the GUI.

* After this, you will be presented with the task configuration page. This is where you enter all the tasks that are currently doable from the scene you have created for your robot. You may select from the predefined tasks using the checkboxes, or enter your own tasks in the text box. For your convenience, use the shift button to toggle between this page and the camera feed. This can be useful for checking what is in view of your cameras.
	* In the upper left corner are 3 tabs:
	* Task Ideas: A google doc of task ideas for inspiration split by room type (ex: bedroom, kitchen, etc)
	* Preferred Tasks: We will be collecting demonstrations for 20 abstract tasks, and 20 specific tasks as a backup. These latter 20 tasks are specific instances of the 20 abstract tasks. We ask that you make sure that around once every couple xdays, you collect some trajectories for as many of these tasks as possible. Clicking this tab will bring you to a page that lists all of the specific tasks and allows you to keep track of which ones you have collected data for.
	* Franka Website: This will bring you to your Franka‚Äôs website, where you can lock / unlock robot joints, as well as activate FCI mode.

* In the lower right corner are another 3 tabs:
	* Collect Trajectory: This will bring you to the requested task page, where you will be prompted with your task for the next trajectory. You may press A to begin the trajectory, or B to sample a new task if it‚Äôs necessary.
	* Calibrate: This will bring you to the camera calibration page. Click the camera that you would like to calibrate. Note that because we need to turn the cameras on in high resolution, button clicks may take a while here. See the section below for more information on camera calibration.
	* Practice: This will allow you to collect a trajectory without the data being saved. You can use this tool however you see fit.

* Periodically, the GUI will prompt you with desired scene changes. Proceed with them as instructed. When the scene changes involve altering a camera position, make sure to recalibrate the associated camera!

* Miscellaneous Notes:
	* Finish trajectories in such a way that the robot can be reset (ex: nothing in gripper, as it will be dropped.
	* Try to create scenes with as many available tasks as possible.
	* Although we want you to stick to the requested tasks, use your best judgment.
	* At any time, hold 'Return' for 5 seconds to reset the robot

# Uploading Data

Instructions for uploading data can be found in [this](https://github.com/droid-dataset/droid/tree/main/scripts) readme.




================================================
FILE: docs/example-workflows/example-workflows.md
================================================
---
layout: default
title: Example Workflows
nav_order: 5
has_children: true
has_toc: false
permalink: /docs/example-workflows
---

## ü§ñ Welcome to the DROID example workflows guides

The goal of the workflow guides is to enable users/developers to operate the DROID platform once both the hardware and software have been configured. Guides within this section cover the following topics:

1. [Teleoperating the Robot](https://droid-dataset.github.io/droid/example-workflows/teleoperation.html): a guide on using the Oculus Quest 2 to teleoperate the robot. 
2. [Calibration Cameras](https://droid-dataset.github.io/droid/example-workflows/calibrating-cameras.html): a guide on calibrating the extrinsic parameter of cameras.
3. [Collecting Data](https://droid-dataset.github.io/droid/example-workflows/data-collection.html): a guide on using the GUI to collect task demonstrations.
4. [Training & Evaluating Policies](https://droid-dataset.github.io/droid/example-workflows/training-policies.html): a guide on training and evaluating policies using the DROID.



================================================
FILE: docs/example-workflows/teleoperation.md
================================================
---
layout: default
title: Teleoperating the Robot
parent: Example Workflows
nav_order: 1
---

# Prerequisites

This guide assumes you have already setup DROID application software to run on your host machine or through Docker. Proceed with this guide having launched the control server on the NUC and the GUI application on the laptop.

# Teleoperating the Robot

* To teleoperate the robot, your oculus controller should be plugged into your laptop, and the permissions prompt should be accepted (done by putting the headset on and clicking ‚ÄúAccept‚Äù. Also, the controller has to be in view of the headset cameras. It detects the pose of the handle via infrared stickers on the handle.
* To control the robot, we will use only the right controller. If you would like to use the left controller for teleoperation instead, change [this](https://github.com/droid-dataset/droid/blob/5f2f96b5cf9d95dde67fda21a8ab776683aeeae7/droid/controllers/oculus_controller.py#L16) parameter.
* Teleoperation works by applying the changes to the oculus handle‚Äôs pose to the robot gripper‚Äôs pose. The trigger on the front of the controller is used to control the gripper. Actions are only applied when the trigger on the side of the controller is being held.
* It is important for intuitive control that the controller‚Äôs definition of forward is aligned with the direction of the robot. The controller defines ‚Äúforward‚Äù on the first step where the side button of the controller is held. At any point, you can redefine the forward direction by pressing down on the joystick until you hear a click. At some point, try changing the definition of forward to get a better feel for its purpose.
* To practice, select `Practice` from the GUI application.

<iframe src="https://drive.google.com/file/d/1Rg10T5rVaK9m_0BYXq2EkVdNgLpZnj3P/preview" width="640" height="480" allow="autoplay"></iframe>




================================================
FILE: docs/example-workflows/training-policies.md
================================================
---
layout: default
title: Training Policies
parent: Example Workflows
nav_order: 4
---

# Training Policies

* Please follow the guide in our [policy learning repo](https://github.com/droid-dataset/droid_policy_learning) to learn how to train and evaluate policies for your own DROID setup!




================================================
FILE: docs/hardware-setup/assembly.md
================================================
---
layout: default
title: Assembly
parent: Hardware Setup
nav_order: 2
---

This assembly guide starts from constructing the standing desk, before progressing to mounting the robot on the desk and installing various other components to the platform. There are a number of design options/variations for the mobile platform, these are discussed under each of the major headings.

<details open markdown="block">
  <summary>
    Table of contents
  </summary>
  {: .text-delta }
1. TOC
{:toc}
</details>

## Building the Desk

It is expected that you have procured a standing desk similar to the one listed in the shopping list. The desk you procured should be accompanied with an assembly guide please follow this guide to construct your standing desk.

**Important:** at this stage, don't attach any extras such as trays and/or cable management items to the bottom of the desk as we will be installing components to the underside of the table in future sections of the guide.

<!---
### Optional: Custom Frame and Wheels

In some instances it may be beneficial to perform additional modifications on your standing desk. A possible modification is adding a custom frame and wheel system to increase the overall stability of the platform... (Peter to complete UoE mod details here). 

<TODO: complete this section>
-->

## Mounting Robot on the Desk

### Option 1: Breadboard Base

A flexible option for mounting the robot on the desk is to bolt a set of rails to a breadboard and subsequently mount the robot on these rails. The main advantage of this form of mount is that it is possible to reposition the rails and mount position of the robot.

The first thing we will do is construct the rails upon which the robot will be mounted:

* Take a 1 foot aluminum bar, and slide the bracket circled in red in the below image (yours may look different) into the left and right side of it.
* Put two of the screws (1 cm long, 5mm wide) into a corner bracket and loosely screw one side into the bracket within the 1 foot aluminum bar on either side.
* Use this technique to create a rail that looks like the rail in the third image below, and another that looks is a mirrored version of this (ie. one corner bracket in the upper right and one in the lower right)

<p>
<img src="../assets/hardware-setup/breadboard_base1.jpg" alt="image" width="45%" height="auto"> 
<img src="../assets/hardware-setup/breadboard_base2.jpg" alt="image" width="45%" height="auto">
<img src="../assets/hardware-setup/breadboard_base3.jpg" alt="image" width="45%" height="auto">
</p>


Next we will mount these rails on to our breadboard:

* Screw the aluminum bars into the breadboard in the configuration shown below. Unfortunately it‚Äôs not possible to achieve perfect symmetry when bolting the rails to the breadboard. Feel free to nudge the centering a row or column over for diversity (ex: configure it so there are two holes on the right instead of the left).
    * Make sure this is one hole between the bars on the top, and 5 holes between the bars on the bottom
    * **Hint:** to keep the screws from colliding when screwing them in, (lift up the bar before screwing so the screw sits at the bottom. (Insert picture)
* Slide two brackets into the top levels of each aluminum bar, we will be using these to attach the Franka.

<p>
<img src="../assets/hardware-setup/breadboard_base4.jpg" alt="image" width="45%" height="auto"> 
<img src="../assets/hardware-setup/breadboard_base5.jpg" alt="image" width="45%" height="auto">
</p>

<img src="../assets/hardware-setup/breadboard_base6.jpg" alt="image" width="45%" height="auto">


Now we mount the robot:

* Align the brackets with the Franka screw holes, and loosely screw the robot to the base.
    * If you‚Äôre using an FR3, use the[ ‚Öù‚Äù long screws](https://www.mcmaster.com/92949A539/) to mount the robot instead of the original screws.

* Once everything is loosely screwed in, tighten ALL SCREWS until everything is entirely fixed!

<img src="../assets/hardware-setup/breadboard_base7.jpg" alt="image" width="45%" height="auto">

Next place the breadboard on your standing desk:

* Center the broadboard as much as you can on top of the desk. It should look roughly like the below image (minus the wires on the robot):
    * Important: Make sure that the franka is facing the side without any desk protrusions. An example of a desk protrusion is the height adjuster on the recommended desk (seen on the back, left side of the desk below).

<img src="../assets/hardware-setup/mobile_base1.jpg" alt="image" width="45%" height="auto">

* Use four clamps (two on either side) to connect the breadboard to the desk.

<p>
<img src="../assets/hardware-setup/mobile_base2.jpg" alt="image" width="45%" height="auto">
<img src="../assets/hardware-setup/mobile_base3.jpg" alt="image" width="45%" height="auto">
</p>

* Place the Franka power box, laptop, and NUC in the following configuration:

<p>
<img src="../assets/hardware-setup/mobile_base4.jpg" alt="image" width="45%" height="auto">
<img src="../assets/hardware-setup/mobile_base5.jpg" alt="image" width="45%" height="auto">
<img src="../assets/hardware-setup/mobile_base6.jpg" alt="image" width="45%" height="auto">
</p>

<!---
### Option 2: Fixed Mount Plate

<TODO: complete this section>
--->

## Mounting Additional Components on the Desk

A requirement of the DROID platform is that it can easily be transported from one location to the next. In this section, we outline configurations for components on the desk in a way that enables greater ease of transportation. Some important points to this end: 

* Throughout this section, take time to organize loose wiring with zip ties, velcro strips and/or sticky hooks in order to keep your desk organised, it is important that wiring be as permanent as possible.
* Getting rid of as much loose wiring and making things as organized as possible during this step will make your life much easier later on!
* Cameras are mounted to the sides of the desk ensure your organise wiring such that you keep space open for moving around the camera mounting location!
* When this doc discusses the location under the desk, we will define forward using the robot. For example ‚ÄúAttach X to the back right side of the desk‚Äù means with respect to the direction that the robot is facing.
* Be generous with the amount of industrial strength velcro you use on power boxes, they can be heavy!
* Scroll down to see what the end product should look like. It would be a good idea to frequently come back to these images and compare.


### Option 1: Configuring Components on Underside of Table 

* Use velcro to attach the large power strip to the back left of the desk.
* Make sure you face the protruding wiring towards the back of the desk.
* Use velcro to attach the standing desk power box to the desk, organize the loose wiring, and plug it it into the power strip:

<img src="../assets/hardware-setup/miscellaneous_wiring1.jpg" alt="image" width="45%" height="auto">

* Use velcro to attach the Franka emergency stop (the white button) to the the bottom of the desk. Attach it on the right hand side directly under the Franka towards the outside of the desk so it is easily accessible. Face the protruding wire towards the center of the desk to keep it from protruding:

<p>
<img src="../assets/hardware-setup/miscellaneous_wiring2.jpg" alt="image" width="45%" height="auto">
<img src="../assets/hardware-setup/miscellaneous_wiring3.jpg" alt="image" width="45%" height="auto">
<img src="../assets/hardware-setup/miscellaneous_wiring4.jpg" alt="image" width="45%" height="auto">
</p>

* Connect the emergency stop wire to the X3 port on the Franka

<img src="../assets/hardware-setup/miscellaneous_wiring5.jpg" alt="image" width="45%" height="auto">

* Use Velcro to attach the power unit for the Robotiq gripper. Face the built in wire towards the Franka. Plug the other side in and connect it to the power strip.

<p>
<img src="../assets/hardware-setup/miscellaneous_wiring6.jpg" alt="image" width="45%" height="auto">
<img src="../assets/hardware-setup/miscellaneous_wiring7.jpg" alt="image" width="45%" height="auto">
</p>

* Use Velcro to attach the power unit for the NUC directly under it (middle right side of the desk). Face the built in cable towards the outside of the desk. Plug in the other side and connect it to the power strip.

<p>
<img src="../assets/hardware-setup/miscellaneous_wiring8.jpg" alt="image" width="45%" height="auto">
<img src="../assets/hardware-setup/miscellaneous_wiring9.jpg" alt="image" width="45%" height="auto">
</p>

* Connect the Franka power box power cable to the power strip:

<img src="../assets/hardware-setup/miscellaneous_wiring10.jpg" alt="image" width="45%" height="auto">

* Connect the cable from the X1 port on the Franka to the power box

<p>
<img src="../assets/hardware-setup/miscellaneous_wiring11.jpg" alt="image" width="45%" height="auto">
<img src="../assets/hardware-setup/miscellaneous_wiring12.jpg" alt="image" width="45%" height="auto">
<img src="../assets/hardware-setup/miscellaneous_wiring13.jpg" alt="image" width="45%" height="auto">
</p>

* Use Velcro to attach the Ethernet switch under the desk on the back right hand side of the desk (under the Franka control box).
* Connect and organize the Ethernet switch power cable, the switch to Franka power box ethernet cable, the switch to NUC ethernet cable, and the switch the laptop Ethernet cable.

<p>
<img src="../assets/hardware-setup/miscellaneous_wiring14.jpg" alt="image" width="45%" height="auto">
<img src="../assets/hardware-setup/miscellaneous_wiring15.jpg" alt="image" width="45%" height="auto">
</p>

* If you‚Äôre using our recommended laptop, use velcro to attach the power box to the bottom of the desk on the back left side, with the built-in cable facing the robot. Plug in the detachable cable and connect it to the power strip.

<img src="../assets/hardware-setup/miscellaneous_wiring16.jpg" alt="image" width="45%" height="auto">

* You‚Äôre done! Your setup should look something like the photos below. Ignore the wires going down the robot, and the hand camera, you‚Äôll set those up later.
* Before proceeding, please set up your robot with the Franka software as we will need to put it into zero gravity mode for the next step. Once you‚Äôve done this, familiarize yourself with the text in the ‚ÄúPowering Up Franka‚Äù section of this document.

<p>
<img src="../assets/hardware-setup/miscellaneous_wiring17.jpg" alt="image" width="45%" height="auto">
<img src="../assets/hardware-setup/miscellaneous_wiring18.jpg" alt="image" width="45%" height="auto">
<img src="../assets/hardware-setup/miscellaneous_wiring19.jpg" alt="image" width="45%" height="auto">
</p>

<!---
### Option 2: Configuring Components on Custom Shelving

<TODO: determine if we actually want to add this to the docs>
--->


## Mounting Hand Camera on Robot 

In this section, we will specify how to mount the zed mini camera on the Franka robot arm.It will be easier to perform this part of the assembly with the Franka robot arm in a position that you have easy access to the last link of the robot (often referred to as link8).


* Secure the camera in the custom mount specified in the shopping list, insert a nut into the hole on the back of the mount. Insert the 10mm screw on the other side, and tighten the screw in until the attachment is tight.
	
<img src="../assets/hardware-setup/camera_mount1.jpg" alt="image" width="45%" height="auto">

* Remove the back two screws in the Franka wrist:

<img src="../assets/hardware-setup/camera_mount2.jpg" alt="image" width="45%" height="auto">

* Use 30mm screws to attach the hand camera to the gripper (**Important:** these are different from the default screws that come with the arm).

<img src="../assets/hardware-setup/camera_mount3.jpg" alt="image" width="45%" height="auto">

## Mounting Robotiq Gripper on Robot

In this section, we will first prepare the Robotiq gripper wiring as it is non-trivial. Following this we will specify how to mount the gripper on the arm.

### Preparing Wires

* You should have a thick cable that has 5 exposed wires colored red, black, white, green, and silver. We are going to connect them to ports as demonstrated in the below images. 

<p>
<img src="../assets/hardware-setup/robotiq_wiring1.jpg" alt="image" width="45%" height="auto">
<img src="../assets/hardware-setup/robotiq_wiring2.jpg" alt="image" width="45%" height="auto">
<img src="../assets/hardware-setup/robotiq_wiring3.jpg" alt="image" width="45%" height="auto">
</p>

* To connect these wires into these ports, you need to cut off some of the rubber to expose ~5mm of metal thread. Then, unscrew the bolts up top to create some space for the metal threads. Jam the metal threads into the empty space, and screw the bolts tight again on top of them. \

* Connect the wires as follows:

<p>
<img src="../assets/hardware-setup/robotiq_wiring4.jpg" alt="image" width="45%" height="auto">
<img src="../assets/hardware-setup/robotiq_wiring5.jpg" alt="image" width="45%" height="auto">
</p>

* Cover these wires with a plastic of some sort to keep things from pulling on them. Then, use a Velcro strip to tightly bound everything together so that there is no pressure on the loose open wires. Otherwise, they will come loose and the gripper will stop working. Make sure to leave the outlet uncovered.

<p>
<img src="../assets/hardware-setup/robotiq_wiring6.jpg" alt="image" width="45%" height="auto">
<img src="../assets/hardware-setup/robotiq_wiring7.jpg" alt="image" width="45%" height="auto">
<img src="../assets/hardware-setup/robotiq_wiring8.jpg" alt="image" width="45%" height="auto">
<img src="../assets/hardware-setup/robotiq_wiring9.jpg" alt="image" width="45%" height="auto">
</p>

* Try connecting the gripper to the AC cable. Make sure that the gripper light shines red.

### Mount Procedure

* To start, you should collect these screws from the box (4 x 10mm, 4 x 30mm)

<img src="../assets/hardware-setup/robotiq_attach1.jpg" alt="image" width="45%" height="auto">

* Align the gripper mount with the screw holes on the Franka wrist, with the protruding wiring facing the right of the robot. The USB port on the camera should be facing the same direction and parallel to the protruding wire.
* Use the four smaller screws to connect the gripper mount to the Franka wrist.

<img src="../assets/hardware-setup/robotiq_attach2.jpg" alt="image" width="45%" height="auto">

* Align the Robotiq gripper with the metal pins on the wrist mount similar to the picture above. Then, use the long screws to attach the gripper in all four corners.

<p>
<img src="../assets/hardware-setup/robotiq_attach3.jpg" alt="image" width="45%" height="auto">
<img src="../assets/hardware-setup/robotiq_attach4.jpg" alt="image" width="45%" height="auto">
</p>

* Place a small piece of thick Velcro (soft side) over the light on the gripper. Otherwise, it will shine into the camera.
* **Important:** Gripper is rotated 180 degrees in the below for better view.

<p>
<img src="../assets/hardware-setup/robotiq_attach5.jpg" alt="image" width="45%" height="auto">
<img src="../assets/hardware-setup/robotiq_attach6.jpg" alt="image" width="45%" height="auto">
</p>

## Robot Cable Management

In this section, we will outline how to manage cables on the Franka robot arm. You‚Äôll want to put the Franka robot arm into zero gravity mode for this section. It is also strongly recommended that you use the 12 foot Zed Mini wire that came with the camera.

Throughout this section we‚Äôll be strapping the wires to the robot with the velcro strips. Use the best length for each connection, and be aware that for some connections you may need to connect two strips together:

<img src="../assets/hardware-setup/arm_wire1.jpg" alt="image" width="45%" height="auto">


It's important to keep the following points in mind as we work through this section:

* Leave enough slack between each wire-to-robot connection for any possible joint configuration. Joint limits are frequently reached, so make sure you do this CAREFULLY and TEST to make sure there‚Äôs enough slack meticulously! Otherwise you could damage the equipment.
* Keep the wires organized. We do not want them catching onto things during data collection!
* When you strap the Velcro straps, strap them as tight as possible! You also may want to put zip ties on either side of the wire to keep it from slipping around. Otherwise, you will compromise the amount of slack you allotted each joint.
* After you attach each strap, you may want to move the robot around to extreme positions to make sure there‚Äôs never tension on the wire.
* As you move along the wire, use zip ties about every 5cm!

* When you plug in the camera wire, make sure:
    * You use the long Zed wire that came with it! Other wires will not support fast enough data transfer.
    * When plugging the wire into the camera the side with the arrow MUST face the side with the lenses. Otherwise you will get a segmentation fault when reading the camera, even though it is a USB C connection and plugs in either way!
    * It‚Äôs a bit hard to see the arrows in the pictures below, but if you look at the ZED Mini wire in real life you‚Äôll see what I am pointing at.

<img src="../assets/hardware-setup/arm_wire2.jpg" alt="image" width="45%" height="auto">

* Follow the pictures below to see the joint positions to move to before adding a tight velcro strap at the specified location.
* First picture is to visualize the neutral joint position, second picture is the extreme joint position that we need enough slack for, third picture is the wire velcro strapped with enough slack for that extreme joint position.

<p>
<img src="../assets/hardware-setup/arm_wire3.jpg" alt="image" width="45%" height="auto">
<img src="../assets/hardware-setup/arm_wire4.jpg" alt="image" width="45%" height="auto">
<img src="../assets/hardware-setup/arm_wire5.jpg" alt="image" width="45%" height="auto">
</p>

* This second velcro strap is just to manage loose wire along a joint.

<img src="../assets/hardware-setup/arm_wire6.jpg" alt="image" width="45%" height="auto">

* Visualization of the second joint extreme, and the next velcro strap location.

<img src="../assets/hardware-setup/arm_wire7.jpg" alt="image" width="45%" height="auto">

* Visualization of the third joint extreme, and the next velcro strap location.

<img src="../assets/hardware-setup/arm_wire8.jpg" alt="image" width="45%" height="auto">

* Visualization of the fourth joint extreme, and the next velcro strap location.

<img src="../assets/hardware-setup/arm_wire9.jpg" alt="image" width="45%" height="auto">

* Visualization of the final wiring:

<p>
<img src="../assets/hardware-setup/arm_wire10.jpg" alt="image" width="45%" height="auto">
<img src="../assets/hardware-setup/arm_wire11.jpg" alt="image" width="45%" height="auto">
<img src="../assets/hardware-setup/arm_wire12.jpg" alt="image" width="45%" height="auto">
<img src="../assets/hardware-setup/arm_wire13.jpg" alt="image" width="45%" height="auto">
</p>

* Now, we will connect these wires to the rest of the mobile base.
* Organize the Robotiq wire slack left over under the desk. If you bought our suggested desk, you can use the wire holders that came with the desk to hold the slack:

<p>
<img src="../assets/hardware-setup/arm_wire14.jpg" alt="image" width="45%" height="auto">
<img src="../assets/hardware-setup/arm_wire15.jpg" alt="image" width="45%" height="auto">
</p>


* Plug in the AC adapter to the power slot you left open in the Robotiq Gripper Cable Management section.

<p>
<img src="../assets/hardware-setup/arm_wire16.jpg" alt="image" width="45%" height="auto">
<img src="../assets/hardware-setup/arm_wire17.jpg" alt="image" width="45%" height="auto">
</p>

* Organize the Zed Mini wire under the desk, so it only protrudes near the laptop as follows:

<img src="../assets/hardware-setup/arm_wire18.jpg" alt="image" width="45%" height="auto">

## Mounting Third-Person Cameras

In this section, we will clamp mounts for third-person cameras to desk.

* Clamp a camera stand to either side of the desk such that the camera attached to the stand can be positioned with a view of the scene. Take the following additional points into consideration when clamping the stand to the desk.
	* The clamp and camera position of the stands should be randomized as much as possible during data collection. Don't fix the stand to a single position each time.
	* The robot arm can shake the table slightly as it moves. Whenever setting up the camera stand, tighten the clamp and all joints as much as possible to prevent the camera from shaking.

<p>
<img src="../assets/hardware-setup/third_person1.png" alt="image" width="45%" height="auto">
<img src="../assets/hardware-setup/third_person2.jpg" alt="image" width="45%" height="auto">
</p>

## General Electrical Wiring

### Power Cable Wiring

### Ethernet Cable Wiring

### Camera Wiring
 
* To keep the Alienware laptop PCI boards from getting overloaded, it is important to connect the cameras in a very specific manner. Otherwise, you will get a Segmentation Fault.
* Plug all cameras directly into the laptop ports, without the use of any Many2One USB converters OR extension chords!
* In other words, directly plug two cameras in through the USB ports on the right side of the computer, and one directly in through the usb port on the back side of the computer.
* This should leave one USB-C port open for the oculus wiring.


## Mounting Calibration Board

In this section we will outline how to mount the calibration board.


### Option 1: Using Magnets

* Split long magnets into three groups of 6 stacked magnets, and put a sticky tab (provided with them) on both magnet stacks.
* Attach either (NOT THE STICKY SIDE!) to the gripper, aligning them along the metal screws as visualized below (again, the sticky side should be pointing away from the gripper!), and additionally one of the two . Be careful to evenly space the magnets so the screws are in the middle.
* **Important:** Sticky adhesive not visualized in this photo

<p>
<img src="../assets/hardware-setup/calibration_board1.jpg" alt="image" width="45%" height="auto">
<img src="../assets/hardware-setup/calibration_board2.jpg" alt="image" width="45%" height="auto">
<img src="../assets/hardware-setup/calibration_board3.jpg" alt="image" width="45%" height="auto">
</p>

* Firmly press the Charuco board into the sticky side of the magnets, and hold it there for 30 seconds.

<p>
<img src="../assets/hardware-setup/calibration_board4.jpg" alt="image" width="45%" height="auto">
<img src="../assets/hardware-setup/calibration_board5.jpg" alt="image" width="45%" height="auto">
<img src="../assets/hardware-setup/calibration_board6.jpg" alt="image" width="45%" height="auto">
</p>

* Try moving the gripper around manually to make sure the board stays on during movement.
* **Important:** The board dents easily. Always be very careful not to drop it.
* During data collection, you can stick the charuco board to the standing desk (for transportation) using the magnets.
* Optionally, place 3 singular magnets in sequence near the other end of the board. This can be used to stick the board to the leg of the standing desk during transportation.

<p>
<img src="../assets/hardware-setup/calibration_board7.jpg" alt="image" width="45%" height="auto">
<img src="../assets/hardware-setup/calibration_board8.jpg" alt="image" width="45%" height="auto">
</p>




================================================
FILE: docs/hardware-setup/hardware-setup.md
================================================
---
layout: default
title: Hardware Setup
nav_order: 2
has_children: true
has_toc: false
permalink: /docs/hardware-setup
---

## üî® Welcome to the DROID hardware guides

The goal of the hardware guides are to: 

1. Procure all the required hardware components
2. Assemble the hardware components into a functional platform

### Step 1: Procure Hardware Components

The first step to replicate the DROID platform is to procure the required hardware. In the [shopping list](https://droid-dataset.github.io/droid/hardware-setup/shopping-list), we provide an exhaustive list of required components to assemble the platform along with a list of suppliers which you can reference when procuring items. 

### Step 2: Assemble Hardware Components

Once you have acquired the required hardware components, the [assembly guide](https://droid-dataset.github.io/droid/hardware-setup/assembly) will walk you through assembling these components into a functional data collection platform. This includes, mounting the robot, cameras and grippers and a general cable management guide. 

### Final Result

By the end of both guides you should have a platform that resembles the image at the bottom of this page. If/when you get to this point congratulations, you are now ready to progress to the [software setup](https://droid-dataset.github.io/droid/docs/software-setup)

<img src="../assets/hardware-setup/droid_setup.jpg" alt="image" width="90%" height="auto"> 




================================================
FILE: docs/hardware-setup/shopping-list.md
================================================
---
layout: default
title: Shopping List
parent: Hardware Setup
nav_order: 1
---

# Shopping List

## Approximate Total Cost: $20,000

## Robot:

| Component | Quantity | Approximate Component Cost | Suppliers |
| --------- | -------- | -------------------------- | --------- |
| Franka Emika Panda (or) Franka Research 3 | 1 | $10,500 | [Franka Robotics](https://lp.franka.de/request-a-quote) |

## Gripper:

| Component | Quantity | Approximate Component Cost | Suppliers |
| --------- | -------- | -------------------------- | --------- |
| 2F-85 Robotiq Gripper | 1 | $4,700 | [Robotiq Official Site](https://robotiq.com/products/2f85-140-adaptive-robot-gripper) |
| Power Supply | 1 | $13 | [Amazon US](https://www.amazon.com/SHNITPWR-Converter-Transformer-100-240V-5-5x2-5mm/dp/B07SDRDV5B/ref=sr_1_5?gclid=Cj0KCQjwkOqZBhDNARIsAACsbfLAkpfvkq7tgnpHcJb2H4Eg7Q8Df5htlSW5inOVLktYBrb2sOOB500aAsY5EALw_wcB&hvadid=174221512760&hvdev=c&hvlocphy=9031969&hvnetw=g&hvqmt=e&hvrand=18374790677584297399&hvtargid=kwd-16464769873&hydadcr=19109_9441157&keywords=24v%2B2a%2Bpower%2Bsupply&qid=1664826676&qu=eyJxc2MiOiIzLjg2IiwicXNhIjoiMy41NyIsInFzcCI6IjMuNDIifQ%3D%3D&sr=8-5&th=1) |

## Computers:

| Component | Quantity | Approximate Component Cost | Suppliers |
| --------- | -------- | -------------------------- | --------- |
| NUC | 1 | $500 | [Amazon US](https://www.amazon.com/NUC11PAHi7-Mainsteam-Barebone%EF%BC%8CIntel-i7-1165G7-Components/dp/B09BKRRT2Y/ref=sr_1_3?crid=MA71JDIBHYBB&keywords=NUC%2B11%2Bi7&qid=1669688069&sprefix=nuc%2B11%2Bi7%2Caps%2C192&sr=8-3&th=1) |
| Razer Blade 15 - QHD 240 Hz - GeForce RTX 4070 - Black  | 1 | $2,400 | [Razer](https://www.razer.com/gaming-laptops/Razer-Blade-15/RZ09-0485ZED3-R3U1) |
| Oculus Quest | 1 | $249 | [Meta Store](https://www.meta.com/us/quest/products/quest-2/) |
| Monitor and Keyboard (no specific specification) | 1 | $100 |  |

## Cameras:

| Component | Quantity | Approximate Component Cost | Suppliers |
| --------- | -------- | -------------------------- | --------- |
| Zed 2 | 2 | $449 | [StereoLabs](https://store.stereolabs.com/en-gb/products/zed-2?_gl=1*1corjfm*_ga*MTE0NDI2NjE3LjE2NDk5NjcwMjM.*_ga_LQLTWBS792*MTY1MDQwMjQ2NC40LjAuMTY1MDQwMjQ2NC42MA..&_ga=2.198936172.278922144.1650231923-114426617.1649967023) |
| Zed Mini | 1 | $400 | [StereoLabs](https://store.stereolabs.com/products/zed-mini?_ga=2.58413544.391576102.1670373235-292338975.1668109772&_gl=1*1ep4wi8*_ga*MjkyMzM4OTc1LjE2NjgxMDk3NzI.*_ga_LQLTWBS792*MTY3MDcwODcxNC4xMS4xLjE2NzA3MDkzMjIuNjAuMC4w) |
| ULANZI Camera Mount | 2 | $80 | [Amazon US](https://www.amazon.com/Flexible-Adjustable-Articulated-Rotatable-Aluminum/dp/B08LV7GZVB/ref=sr_1_2?crid=2HACR1JZEBPRV&keywords=camera%2Bmount%2Bsturdy&qid=1670995104&s=electronics&sprefix=camera%2Bmount%2Bsturd%2Celectronics%2C183&sr=1-2&ufe=app_do%3Aamzn1.fos.006c50ae-5d4c-4777-9bc0-4513d670b6bc&th=1) |
| Charuco Board | 1 | $114 | [Calib.io](https://calib.io/products/charuco-targets?variant=9400454807599) |
| Camera Mount Files | 1 | $0 ($2 for print materials) | [STL files](https://drive.google.com/drive/folders/1k56XVdlfrXCX4iOlFlTlkoTh-2Px6CyD) |

## Mobile Base:

| Component | Quantity | Approximate Component Cost | Suppliers |
| --------- | -------- | -------------------------- | --------- |
| Screw Kit | 1 | $27 | [Amazon US](https://www.amazon.com/dp/B083SGJ7BD?ref_=cm_sw_r_cp_ud_dp_AQE4AW6MK6QAQ00SE417&th=1) |
| Standing Desk | 1 | $130 | [Amazon US](https://www.amazon.com/SHW-Electric-Height-Adjustable-Computer/dp/B08668Y49C/ref=mp_s_a_1_13?crid=21JB1RSHEAYFJ&keywords=40%2Brolling%2Bstanding%2Bdesk&qid=1644355171&sprefix=40%2Brolling%2Bstanding%2Bdesk%2Caps%2C298&sr=8-13&th=1) |
| Breadboard | 1 | $100 | [Newport](https://www.newport.com/p/SA2-13) |
| Clamps | 4 | $12 | [MSCDirect](https://www.mscdirect.com/product/details/67105742?cid=ppc-google-New+-+Clamping%2C+Workholding+%26+Positioning+-+PLA_sxxmVRNtt___164124448499_c_S&mkwid=sxxmVRNtt%7Cdc&pcrid=164124448499&rd=k&product_id=67105742&gclid=CjwKCAjw8e7mBRBsEiwAPVxxiGVA1SyGmWreg4kKH1lWPj3AiQYlmnwmHUBFVUUj3xBSkY_9NFEHZxoC_XYQAvD_BwE) |
| Aluminum Bars (12 inches) | 2 | $0.35 (per inch) | [8020](https://8020.net/1010.html) |
| Screws + T-Nuts | 12 | $0.66 | [8020](https://8020.net/3393.html) |
| Screws (For FR3 Only) | 1 | $10 | [McMaster-Carr](https://www.mcmaster.com/92949A539/) |

## Chords:

| Component | Quantity | Approximate Component Cost | Suppliers |
| --------- | -------- | -------------------------- | --------- |
| USB port | 1 | $10 | [Amazon US](https://www.amazon.com/BYEASY-Portable-Applicable-MacBook-Notebook/dp/B07FH7XJCD/ref=mp_s_a_1_1_sspa?crid=13LJFDB63AIUY&keywords=many%2Bto%2Bone%2Busb&qid=1674798390&sprefix=many%2Bto%2Bone%2Busb%2Caps%2C689&sr=8-1-spons&spLa=ZW5jcnlwdGVkUXVhbGlmaWVyPUE1TVhTSDhZS0dXRTImZW5jcnlwdGVkSWQ9QTA0Njg0MjVSMEpFNzBUMzBOQ1cmZW5jcnlwdGVkQWRJZD1BMDc3NDUxOTFSTFdQOFNMQjFWMEcmd2lkZ2V0TmFtZT1zcF9waG9uZV9zZWFyY2hfYXRmJmFjdGlvbj1jbGlja1JlZGlyZWN0JmRvTm90TG9nQ2xpY2s9dHJ1ZQ&th=1) |
| Ethernet Cable | 3 | $5 | [Amazon US](https://www.amazon.com/AmazonBasics-Cat-6-Gigabit-Ethernet-Internet/dp/B089MGH8T5/ref=sr_1_1_sspa?crid=NNVN8X9N4SR4&keywords=short+ethernet+cable&qid=1669691593&sprefix=short+ethernet+cable+.%2Caps%2C227&sr=8-1-spons&psc=1&spLa=ZW5jcnlwdGVkUXVhbGlmaWVyPUEyMEtMVEtCWE5ZUERLJmVuY3J5cHRlZElkPUEwOTk4Njk5SlQ1UUdBQkRBMkU3JmVuY3J5cHRlZEFkSWQ9QTAyMDk5NDkxMzdOTlRaM1c4VFlWJndpZGdldE5hbWU9c3BfYXRmJmFjdGlvbj1jbGlja1JlZGlyZWN0JmRvTm90TG9nQ2xpY2s9dHJ1ZQ==) |

## Miscellaneous:

| Component | Quantity | Approximate Component Cost | Suppliers |
| --------- | -------- | -------------------------- | --------- |
| Magnets | 1 | $20 | [Amazon US](https://www.amazon.com/Neodymium-Magnets-Double-sided-Adhesive-Rare-Earth/dp/B078KTLWQ9/ref=sr_1_6?crid=1IOM068UZ4RDK&keywords=long%2Bmagnet&qid=1670802719&sprefix=long%2Bmagne%2Caps%2C133&sr=8-6&th=1) |
| Industrial Strength Velcro | 1 | $20 | [Amazon US](https://www.amazon.com/VELCRO-Brand-Industrial-Fasteners-VEL-30838-USA/dp/B0B74YZVSN/ref=sr_1_5?crid=3U9Y5NFUBJ3IH&keywords=industrial+velcro&qid=1669691954&sprefix=industtrial+velcro%2Caps%2C138&sr=8-5) |
| Chord Wraps | 1 | $18 | [Amazon US](https://www.amazon.com/VELCRO-Brand-Industrial-Fasteners-VEL-30838-USA/dp/B0B74YZVSN/ref=sr_1_5?crid=3U9Y5NFUBJ3IH&keywords=industrial+velcro&qid=1669691954&sprefix=industtrial+velcro%2Caps%2C138&sr=8-5) |
| Chord Hanger | 1 | $18 | [Amazon US](https://www.amazon.com/VELCRO-Brand-Industrial-Fasteners-VEL-30838-USA/dp/B0B74YZVSN/ref=sr_1_5?crid=3U9Y5NFUBJ3IH&keywords=industrial+velcro&qid=1669691954&sprefix=industtrial+velcro%2Caps%2C138&sr=8-5) |
| Zip Ties | 1 | $25 | [Amazon US](https://www.amazon.com/inch-Tensile-Strength-Resistant-Self-locking/dp/B09VL2RDH9/ref=sr_1_2_sspa?crid=3VSNR598R3WIB&keywords=zip+ties&qid=1669693628&sprefix=zip+tie%2Caps%2C205&sr=8-2-spons&psc=1&spLa=ZW5jcnlwdGVkUXVhbGlmaWVyPUExOU42T1k4Q0VYUk5HJmVuY3J5cHRlZElkPUEwNDM4NjAyMUpFTDVCV0lUMVZLTCZlbmNyeXB0ZWRBZElkPUEwMDIxNDY2MTZMVUpWSEtPNFNHMSZ3aWRnZXROYW1lPXNwX2F0ZiZhY3Rpb249Y2xpY2tSZWRpcmVjdCZkb05vdExvZ0NsaWNrPXRydWU=) |
| Duct Tape | 1 | $8 | [Amazon US](https://www.amazon.com/Original-Strength-Duck-Silver-394475/dp/B0000DH4ME/ref=sr_1_3?crid=1BATBX7XS37IH&keywords=duct%2Btape&qid=1669693579&sprefix=duct%2Btape%2Caps%2C135&sr=8-3&th=1) |
| Cables Ties | 1 | $6 | [Amazon US](https://www.amazon.com/dp/B08TTPX4KB?ref_=cm_sw_r_cp_ud_dp_E8TXV87317XD0AXHRXZ8&th=1) |
| Power Strip | 1 | $22 | [Amazon US](https://www.amazon.com/Alestor-Protector-Outlets-Extension-Essentials/dp/B08P5LRY37/ref=mp_s_a_1_8?crid=3F3BGM9NP2U08&keywords=power%2Bstrip&qid=1674789403&sprefix=power%2Bst%2Caps%2C182&sr=8-8&th=1) |
| Extension Cord | 1 | $11 | [Amazon US](https://www.amazon.com/AmazonBasics-Extension-Cord-feet-Black/dp/B075BCD1LP/ref=mp_s_a_1_5?crid=13A81TEC0APYS&keywords=3%2Bprong%2Bextension%2Bcord&qid=1674789516&sprefix=3%2Bprong%2B%2Caps%2C376&sr=8-5&th=1) |
| Ethernet Switch | 1 | $22 | [Amazon US](https://www.amazon.com/NETGEAR-Gigabit-Ethernet-Unmanaged-1000Mbps/dp/B00KFD0SMC/ref=mp_s_a_1_9?crid=25PREBQW9BS0N&keywords=netgear+ethernet+splitter&qid=1674794661&sprefix=netgear+ethern%2Caps%2C136&sr=8-9) |




================================================
FILE: docs/software-setup/docker.md
================================================
---
layout: default
title: Running Application through Docker
has_children: false
nav_order: 1
parent: Software Setup
---


This guide commences with outlining the configuration of your Franka robot arm. Following this we detail the configuration of the Oculus Quest. We then outline the software setup for the NUC which interfaces directly with the Franka control unit and finally the laptop which runs the GUI application and client software.

<details open markdown="block">
  <summary>
    Table of contents
  </summary>
  {: .text-delta }
1. TOC
{:toc}
</details>

# Configuring the Franka robot

## Franka World Account

A Franka World account is used to manage Franka robot arms and associated firmware updates. You may login to Franka World or register and account at the following [portal](https://franka.world/). 

Thankfully Franka Robotics have provided a tutorial on registering and synchronizing software for your Franka robot on Franka World. Please follow this tutorial to set up your robot arm:

<iframe width="560" height="315" src="https://www.youtube.com/embed/lF6HwaFnGaQ?si=IfZgSJzYRVEyCJzO" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

Importantly, after the above tutorial, it is necessary to ensure that the Franka Control Interface (FCI) feature is installed on your robot. Further information on this interface which Franka Robotics expose is found in the guide below:

<iframe width="560" height="315" src="https://www.youtube.com/embed/91wFDNHVXI4?si=NzGwr9n7X5klrWHW" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

## Operating the Robot

Please refer to the Franka Robotics [Getting Started Guide](https://frankaemika.github.io/docs/getting_started.html) to perform the initial setup and operation of your robot arm. At this point it is worthwhile noting the details of how to activate and deactivate FCI for your robot, these details can be found [here](https://frankaemika.github.io/docs/getting_started.html#preparing-the-robot-for-fci-usage-in-desk). 


## Updating Inertia Parameters for Robotiq Gripper

In order to accurately represent the robot model with the Robotiq gripper we need to update the end-effector configuration. From the Desk UI you may access this option under Settings > End-Effector as outlined [here](https://facebookresearch.github.io/fairo/polymetis/troubleshooting.html#end-effector-configuration). 

Please add the following [configuration file](https://github.com/frankaemika/external_gripper_example/blob/master/panda_with_robotiq_gripper_example/config/endeffector-config.json) via the UI.

# Configuring the Oculus Quest

The Oculus will be used to run an application that tracks the pose of hand-held controllers which we use to teleoperate the robot.

The first step to setting up your Oculus Quest is to associate a Meta account with the device. You may create an account at the following [portal](https://secure.oculus.com/sign-up/). Once you've created a Meta account you also need to register as a Meta developer, a guide for accomplishing this can be found [here](https://developers.facebook.com/docs/development/register/).

Once you have a meta developer account, you are ready to begin enabling developer mode on your Oculus device. Developer mode is required to run a custom application for the Oculus Quest which we use as part of the DROID platform. For instructions on enabling developer mode on your Oculus device please see the following [guide](https://developer.oculus.com/documentation/native/android/mobile-device-setup/).

Later in this guide we will detail how to run the required application on the Oculus, for now the device setup is complete.

# Configuring the NUC

The NUC device is used to run the polymetis controller manager server. This server manages translating commands from the user client to the robot client. A high-level schematic outling the controller manager is given below, further details can also be found [here](https://facebookresearch.github.io/fairo/polymetis/overview.html):

<img src="../assets/software-setup/polymetis_controller_manager.png" alt="image" width="90%" height="auto">

In order to gain an understanding of the various components being installed it is worthwhile working through this section. If you are less concerned with understanding individual software dependencies and their function using the automated install script is more time efficient.

## Booting with Ubuntu 22.04

We will install Ubuntu 22.04 through flashing a USB with Ubuntu 22.04 and using this flased USB drive as the install medium.

You may use the following [application](https://etcher.balena.io/) to flash a USB drive with the Ubuntu OS image. Download this application on a device other than the laptop/workstation as we don't want to include any unnecessary applications on this device. Follow the official [Ubuntu Installation guide](https://ubuntu.com/tutorials/install-ubuntu-desktop#1-overview) to set up the USB flashdrive and install the Ubuntu OS (22.04).

When installing Ubuntu, set the computer password `robot` and enable the `Log in automatically` setting.

## Ubuntu Pro Token

In order to simplify the process of performing a RT patch of the Ubuntu Kernel we leverage Ubuntu Pro. Generate an Ubuntu Pro token through following the section 3 called "Get your free Ubuntu pro subscription" from the following [guide](https://ubuntu.com/pro/tutorial). We will use this token to activate Ubuntu pro and activate an RT patched kernel on our machine.

## Cloning Codebase

To clone the codebase including submodule dependencies run:

```bash
git clone git@github.com:droid-dataset/droid.git
cd droid 
git submodule update --init --recursive
```

## Configure Parameters

Complete the following parameters in `./droid/misc/parameters.py`. 

* Start by specifying static IP addresses for all the machines on the platform (NUC, laptop, control unit). 

* Add the sudo password for the machine you are using (only relevant for the NUC machine). 

* Set the type and serial number of your robot. The serial number for your robot can be found on the control box unit. 

* If this is your first time setting up the platform you can ignore the camera ids as you will need to load the GUI to generate these. 

* Set the Charuco board parameters to match the board you are using.

* Provide an Ubuntu Pro token (required to automate rt-patch of kernel on the NUC). 

Complete the following parameters in `./config/<robot_type>/franka_hardware.yaml`

* Specify the `robot_ip` parameter. 


## Run Setup Script

To complete the device setup execute the setup script with elevated privileges through running the following:

```bash
sudo ./scripts/setup/nuc_setup.sh
```

# Configuring the Laptop/Workstation 

The laptop/workstation device manages the execution of the Oculus Quest application, camera data and user client code including a GUI application. Recalling the Polymetis diagram from earlier, all the user client code is run on this device. 

<img src="../assets/software-setup/polymetis_controller_manager.png" alt="image" width="90%" height="auto">

## Booting with Ubuntu 22.04

We will install Ubuntu 22.04 through flashing a USB with Ubuntu 22.04 and using this flased USB drive as the install medium.

You may use the following [application](https://etcher.balena.io/) to flash a USB drive with the Ubuntu OS image. Download this application on a device other than the laptop/workstation as we don't want to include any unnecessary applications on this device. Follow the official [Ubuntu Installation guide](https://ubuntu.com/tutorials/install-ubuntu-desktop#1-overview) to set up the USB flashdrive and install the Ubuntu OS (22.04).

When installing Ubuntu, set the computer password `robot` and enable the `Log in automatically` setting.

## Cloning Codebase    

To clone the codebase including submodule dependencies run:

```bash
git clone git@github.com:droid-dataset/droid.git                
cd droid
git submodule update --init --recursive
```

## Configure Parameters

Complete the following parameters in `./droid/misc/parameters.py`.

* Specify static IP addresses for all the machines on the platform (NUC, laptop, control unit).

* Add the sudo password for the machine you are using (only relevant for the NUC machine).

* Set the type and serial number of your robot. The serial number for your robot can be found on the control box unit.

* If this is your first time setting up the platform you can ignore the camera ids as you will need to load the GUI to generate these.

* Set the Charuco board parameters to match the board you are using.

* Provide an Ubuntu Pro token (required to automate rt-patch of kernel on the NUC).

Complete the following parameters in `./config/<robot_type>/franka_hardware.yaml`

* Specify the `robot_ip` parameter.

## Run Setup Script

To complete the device setup execute the setup script with elevated privileges through running the following:

```bash
sudo ./scripts/setup/laptop_setup.sh
```

# Testing/Validating Entire Setup

In order to validate your device setup we will attempt to collect a trajectory. Start by running the NUC server machine through running the corresponding Docker container. This can be accomplished through running the setup script which spins the control server container. Alternatively you may run the container directly with the below command. 


```bash
cd .docker/nuc 
docker compose -f docker-compose-nuc.yaml up
```

**Note:** you want to ensure environment variables defined in the docker compose file are exported for them to be available to the container:

Next run the test script for collecting a trajectory on the laptop through running the test Docker container:

```bash
cd .docker/laptop
docker compose -f docker-compose-laptop.yaml run laptop_setup python scripts/test/collect_trajectory.py
```

**Note:** you want to ensure environment variables defined in the docker compose file are exported for them to be available to the container:



================================================
FILE: docs/software-setup/host-installation.md
================================================
---
layout: default
title: Running Application on Host
has_children: false
nav_order: 2
parent: Software Setup
---

This guide commences with outlining the configuration of your Franka robot arm. Following this we detail the configuration of the Oculus Quest. We then outline the software setup for the NUC which interfaces directly with the Franka control unit and finally the laptop which runs the GUI application and client software.

<details open markdown="block">
  <summary>
    Table of contents
  </summary>
  {: .text-delta }
1. TOC
{:toc}
</details>

# Configuring the Franka robot

## Franka World Account

A Franka World account is used to manage Franka robot arms and associated firmware updates. You may login to Franka World or register and account at the following [portal](https://franka.world/). 

Thankfully Franka Robotics have provided a tutorial on registering and synchronizing software for your Franka robot on Franka World. Please follow this tutorial to set up your robot arm:

<iframe width="560" height="315" src="https://www.youtube.com/embed/lF6HwaFnGaQ?si=IfZgSJzYRVEyCJzO" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

Importantly, after the above tutorial, it is necessary to ensure that the Franka Control Interface (FCI) feature is installed on your robot. Further information on this interface which Franka Robotics expose is found in the guide below:

<iframe width="560" height="315" src="https://www.youtube.com/embed/91wFDNHVXI4?si=NzGwr9n7X5klrWHW" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

## Operating the Robot

Please refer to the Franka Robotics [Getting Started Guide](https://frankaemika.github.io/docs/getting_started.html) to perform the initial setup and operation of your robot arm. At this point it is worthwhile noting the details of how to activate and deactivate FCI for your robot, these details can be found [here](https://frankaemika.github.io/docs/getting_started.html#preparing-the-robot-for-fci-usage-in-desk). 


## Updating Inertia Parameters for Robotiq Gripper

In order to accurately represent the robot model with the Robotiq gripper we need to update the end-effector configuration. From the Desk UI you may access this option under Settings > End-Effector as outlined [here](https://facebookresearch.github.io/fairo/polymetis/troubleshooting.html#end-effector-configuration). 

Please add the following [configuration file](https://github.com/frankaemika/external_gripper_example/blob/master/panda_with_robotiq_gripper_example/config/endeffector-config.json) via the UI.

# Configuring the Oculus Quest

The Oculus will be used to run an application that tracks the pose of hand-held controllers which we use to teleoperate the robot.

The first step to setting up your Oculus Quest is to associate a Meta account with the device. You may create an account at the following [portal](https://secure.oculus.com/sign-up/). Once you've created a Meta account you also need to register as a Meta developer, a guide for accomplishing this can be found [here](https://developers.facebook.com/docs/development/register/).

Once you have a meta developer account, you are ready to begin enabling developer mode on your Oculus device. Developer mode is required to run a custom application for the Oculus Quest which we use as part of the DROID platform. For instructions on enabling developer mode on your Oculus device please see the following [guide](https://developer.oculus.com/documentation/native/android/mobile-device-setup/).

Later in this guide we will detail how to run the required application on the Oculus, for now the device setup is complete.

# Configuring the NUC

The NUC device is used to run the polymetis controller manager server. This server manages translating commands from the user client to the robot client. A high-level schematic outling the controller manager is given below, further details can also be found [here](https://facebookresearch.github.io/fairo/polymetis/overview.html):

<img src="../assets/software-setup/polymetis_controller_manager.png" alt="image" width="90%" height="auto">

## Booting with Ubuntu 18.04

We will install Ubuntu 18.04 through flashing a USB with Ubuntu 18.04 and using this flased USB drive as the install medium.

You may use the following [application](https://etcher.balena.io/) to flash a USB drive with the Ubuntu OS image. Download this application on a device other than the NUC as we don't want to include any unnecessary applications on the NUC (e.g. personal laptop). Follow the official [Ubuntu Installation guide](https://ubuntu.com/tutorials/install-ubuntu-desktop#1-overview) to set up the USB flashdrive and install the Ubuntu OS (18.04) on the NUC.

When installing Ubuntu, set the computer password `robot` and enable the `Log in automatically` setting.

## Configuring Static IP Address

To set the static IP for the NUC, enter Settings > Network and click the add wired connection option (through clicking the + option under the Wired heading). Click the IPv4 tab; under IPv4 method select manual. Set the Address to 172.16.0.2 and the netmask to 255.255.255.0. Click apply to create this static IP address. 

## Performing RT Patch of Kernel

In order to ensure that real-time priorities set in the libfranka codebase are respected we need to perform a Real-Time (RT) patch of the default kernel. For this section we will be following the [official guide](https://frankaemika.github.io/docs/installation_linux.html#setting-up-the-real-time-kernel) provided by Franka Robotics. 

To ensure, you replicate the same setup as the original DROID platform implementation start by installing dependencies with:

```bash
sudo apt-get install build-essential bc curl ca-certificates gnupg2 libssl-dev lsb-release libelf-dev bison flex dwarves zstd libncurses-dev
```

Following the installation of dependencies, install a kernel patch that has been tested with the DROID platform:

```bash
curl -SLO [https://mirrors.edge.kernel.org/pub/linux/kernel/v5.x/linux-5.4.70.tar.xz](https://mirrors.edge.kernel.org/pub/linux/kernel/v5.x/linux-5.4.70.tar.xz)
curl -SLO [https://mirrors.edge.kernel.org/pub/linux/kernel/projects/rt/5.4/older/patch-5.4.70-rt40.patch.xz](https://mirrors.edge.kernel.org/pub/linux/kernel/projects/rt/5.4/older/patch-5.4.70-rt40.patch.xz)
xz -d *.xz
```

Follow the official Franka Robotics guide from the [Compiling the kernel](https://frankaemika.github.io/docs/installation_linux.html#compiling-the-kernel) section onwards, using the kernel patch that was installed above. 

## CPU Frequency Scaling

Most CPUs are by default configured to use a lower operating frequency, we wish to override this setting in order to increase the operating frequenecy and as a result reduce the latency when interfacing with libfranka. Follow this [section](https://frankaemika.github.io/docs/troubleshooting.html#disabling-cpu-frequency-scaling) of the official Franka Robotics guides to disable CPU frequency scaling.


## Configuring Python Virtual Environment (Conda)

We will use Conda as the package manager for our python virtual environment. You can install Conda from the following [link](https://docs.conda.io/projects/conda/en/latest/user-guide/install/linux.html). Once you've installed Conda, ensure that you have configured git on the NUC so you can pull software packages from GitHub. A guide on configuring git is provided [here](https://docs.github.com/en/get-started/getting-started-with-git/set-up-git#setting-up-git).

Next, we will commence with setting up the Polymetis library. The following [guide](https://facebookresearch.github.io/fairo/polymetis/installation.html#for-advanced-users-developers) details how to build Polymetis, we will be following this guide.

The first step of the linked guide requires you to clone the fairo repository. We have included a pinned version of the library with the DROID main repository as a git submodule. Clone the DROID repository and submodules with the following command:

```bash
git clone git@github.com:droid-dataset/droid.git
# sync and pull submodules
git submodule sync
git submodule update --init --recursive
```

You should now find the fairo repository at `droid/fairo`, from this point continue with steps 2 and 3 of the polymetis guide.

In step 4, when building libfranka from source, it is crucial to fix the version of libfranka you are using to one that is compatible with the robot you are using, see the below table for a list of robot hardware and libfranka version compatibility: 

| Robot | libfranka Version |
| --------- | -------- |
| Franka Emika Panda | 0.9.0 |
| Franka Research 3 | 0.10.0 |

In order to install a specific libfranka version (e.g. 0.10.0) using the linked guide, you can run the `build_libfranka` script as follows:

```bash
./scripts/build_libfranka.sh 0.10.0
```

When building the polymetis from source with CMAKE, be sure to set all setting to `ON`: 

```bash
cmake .. -DCMAKE_BUILD_TYPE=Release -DBUILD_FRANKA=ON -DBUILD_TESTS=ON -DBUILD_DOCS=ON
make -j
```

Now that Polymetis is built, we will add other dependencies to our conda virtual environment. 

Now that polymetis is built we will commence with installing the remaining dependencies for our python virtual environment. From the root directory of this repository, with the python virtual environment created during polymetis build activated run the following command to install remaining dependencies:

```bash
pip install -e .

# Done like this to avoid dependency issues
pip install dm-robotics-moma==0.5.0 --no-deps
pip install dm-robotics-transformations==0.5.0 --no-deps
pip install dm-robotics-agentflow==0.5.0 --no-deps
pip install dm-robotics-geometry==0.5.0 --no-deps
pip install dm-robotics-manipulation==0.5.0 --no-deps
pip install dm-robotics-controllers==0.5.0 --no-deps
```

The next step is to ensure the the project config files reference the robot you are using. First we will update the robot client config file and after this we will update the robot model config file. Please find links to the required config files below: 

| Robot | Config Files |
| --------- | -------- |
| Franka Emika Panda | [config files](https://drive.google.com/drive/folders/1wXTQQbFKjd9ed3yKxB4td9GzA_XrR7Xk) |
| Franka Research 3 | [config files](https://drive.google.com/drive/folders/178-MJTAVV0m5_RDs2ScUNcYameGDA0Eg) |

We will start by updating the robot client config file. To do so go into `fairo/polymetis/polymetis/conf/robot_client` and delete the existing `franka_hardware.yaml` file. Replace this file with `franka_hardware[robot_name].yaml` from the linked config files folder. Rename this file to `franka_hardware.yaml`. Finally, edit this `franka_hardware.yaml` file and replace `executable_cfg.robot_ip` to match you Franka control box's IP address. 

Next we will update the robot model config file. To do so go into `fairo/polymetis/polymetis/conf/robot_model` and delete the existing `franka_panda.yaml` file. Replace this file with `franka_panda[robot_name].yaml` from the linked config files folder. Rename this file to `franka_panda.yaml`. 

Update the IP parameters in `droid/misc/parameters.py`, in particular set `robot_ip` to match the IP address of your robot and `nuc_ip` to match the IP address of your NUC. Also set the `sudo_password` to match your machine's sudo password (sudo access is required to launch the robot). Finally update the `robot_type` parameter to `panda` or `fr3` depending on which robot you are using. 

If you choose to install miniconda instead of anaconda in previous steps of this guide you will need to make the following edits:

In droid/franka change the word anaconda to minicode in the scripts `launch_gripper.sh` and `launch_robot.sh`. Also change the paths to be absolute. Repeat for the `launch_server.sh` file in `scripts/server`

## Optional: Configure Headless Server

When you have validated your entire setup you may wish to make the NUC a headless server such that you don't need to manually run commands to start the control server. 

From the terminal enter `crontab -e` and add the ollowing line to the bottom of the file:

```
@reboot bash [YOUR PATH TO DROID]/scripts/server/launch_server.sh
```

You may remove any displays associated with your NUC once you have configured it to run as a headless server. 

# Configuring the Laptop/Workstation 

The laptop/workstation device manages the execution of the Oculus Quest application, camera data and user client code including a GUI application. Recalling the Polymetis diagram from earlier, all the user client code is run on this device. 
<img src="../assets/software-setup/polymetis_controller_manager.png" alt="image" width="90%" height="auto">

## Booting with Ubuntu 22.04

We will install Ubuntu 22.04 through flashing a USB with Ubuntu 22.04 and using this flased USB drive as the install medium.

You may use the following [application](https://etcher.balena.io/) to flash a USB drive with the Ubuntu OS image. Download this application on a device other than the laptop/workstation as we don't want to include any unnecessary applications on this device. Follow the official [Ubuntu Installation guide](https://ubuntu.com/tutorials/install-ubuntu-desktop#1-overview) to set up the USB flashdrive and install the Ubuntu OS (22.04).

When installing Ubuntu, set the computer password `robot` and enable the `Log in automatically` setting.

## Configuring Static IP Address

To set the static IP for the laptop, enter Settings > Network and click the add wired connection option (through clicking the + option under the Wired heading). Click the IPv4 tab; under IPv4 method select manual. Set the Address to 172.16.0.1 and the netmask to 255.255.255.0. Click apply to create this static IP address.

## Configuring Python Virtual Environment (Conda)

We will use Conda as the package manager for our python virtual environment. You can install Conda from the following [link](https://docs.conda.io/projects/conda/en/latest/user-guide/install/linux.html). Once you've installed Conda, ensure that you have configured git on the NUC so you can pull software packages from GitHub. A guide on configuring git is provided [here](https://docs.github.com/en/get-started/getting-started-with-git/set-up-git#setting-up-git).

Clone the DROID repository and submodules with the following command:

```bash
git clone git@github.com:droid-dataset/droid.git
git submodule sync
git submodule update --init --recursive
```

Next create and activate a conda environment called `robot` with python 3.7 through running the following commands:

```bash
conda create -n "robot" python=3.7
conda activate robot
```

Ensure that GCC is installed through running the following commands:

```bash
sudo apt update
sudo apt install build-essential
```

Verify your installation through running (you should observe a version returned):

```bash
gcc --version
```

Next we will install the ZED SDK and python API which is required to interface with ZED cameras. Follow the official guide outlined [here](https://www.stereolabs.com/docs/installation/linux), while adhering to the below instructions during your installation:

* Ensure you have the robot conda environment activated
* Enter yes when prompted to install CUDA
* Enter yes to everything else during the installation procedure with the exception of optimizing ZED models this is optiona

Once you have restarted your machine post installation, activate the robot conda environment and test that you can import pyzed through running:

```bash
python -c "import pyzed"
```

The software for the Oculus application is included under `droid/oculus_reader` as a git submodule. Start by installing the oculus reader dependencies into the current virtual environment through running:

```bash
pip install -e ./droid/oculus_reader
```

Next, install the android debug bridge software dependency required to interface with your Oculus Quest through running:

```bash
sudo apt install android-tools-adb
```

Finally from the root directory of the project repository, with the python virtual environment activated run the following command to install remaining dependencies:

```bash
pip install -e .

# Done like this to avoid dependency issues
pip install dm-robotics-moma==0.5.0 --no-deps
pip install dm-robotics-transformations==0.5.0 --no-deps
pip install dm-robotics-agentflow==0.5.0 --no-deps
pip install dm-robotics-geometry==0.5.0 --no-deps
pip install dm-robotics-manipulation==0.5.0 --no-deps
pip install dm-robotics-controllers==0.5.0 --no-deps
```

Update the IP parameters in `droid/misc/parameters.py`, in particular set `robot_ip` to match the IP address of your robot and `nuc_ip` to match the IP address of your NUC. In addition, set the `robot_serial_number` to match your robot's serial number (found on your franka website, under Settings -> Franka World -> Control S/N). For the `robot_type variable`, enter 'fr3' or 'panda' depending on which Franka robot you are using. Update the Charuco board parameters to match the board you are using. Finally you will need to set the camera ids in this parameters file further details on how to accomplish this are provided later in the guide. 

If you choose to install miniconda instead of anaconda in previous steps of this guide you will need to make the following edits:

In droid/franka change the word anaconda to minicode in the scripts `launch_gripper.sh` and `launch_robot.sh`. Also change the paths to be absolute. Repeat for the `launch_server.sh` file in `scripts/server`


# Testing/Validating Entire Setup

In order to validate your device setup we will attempt to collect a trajectory. Start by running the NUC server machine through running the following commands:

```bash
conda activate polymetis-local
python scripts/server/run_server.py
```

With the control server running on the NUC, run the script for testing trajectory collection with:

```bash
conda activate robot
python scripts/test/collect_trajectory.py
```



================================================
FILE: docs/software-setup/software-setup.md
================================================
---
layout: default
title: Software Setup
has_children: true
has_toc: false
nav_order: 3
permalink: /docs/software-setup
---

## üñ•Ô∏è  Welcome to the DROID software guides.

The goal of the software guides are to:

1. Configure the Franka robot for DROID application software
2. Configure the Oculus Quest 2 for DROID application software
3. Set up a Polymetis control server on the NUC 
4. Set up a user client and GUI on the laptop/workstation
5. Validate the entire platform setup

There are two methods of installation for the software used in DROID; one can run the application software directly on the host machine or through Docker. Below we provide a brief description of each, importantly it is only necessary to perform one of the two installations, please choose one based on the outlined considerations. We strongly recommend using the Docker installation as it requires less manual configuration and it decouples most of the DROID application config from your host machine configuration.

### Installation Method 1: Docker Installation
Running DROID software through Docker requires less installation steps and allows for machines to easily be repurposed for other sets of software as the application software is containerised. Running the application software through Docker does however require an understanding of Docker the management of Docker containers. 

### Installation Method 2: Host Installation
Running DROID software directly on the host machine requires more installation steps but is worthwhile in the case where machines are dedicated to the DROID setup as it forgoes the need to launch and manage Docker containers. 



================================================
FILE: droid/__init__.py
================================================
[Empty file]


================================================
FILE: droid/robot_env.py
================================================
from copy import deepcopy

import gym
import numpy as np

from droid.calibration.calibration_utils import load_calibration_info
from droid.camera_utils.info import camera_type_dict
from droid.camera_utils.wrappers.multi_camera_wrapper import MultiCameraWrapper
from droid.misc.parameters import hand_camera_id, nuc_ip
from droid.misc.server_interface import ServerInterface
from droid.misc.time import time_ms
from droid.misc.transformations import change_pose_frame


class RobotEnv(gym.Env):
    def __init__(self, action_space="cartesian_velocity", gripper_action_space=None, camera_kwargs={}, do_reset=True):
        # Initialize Gym Environment
        super().__init__()

        # Define Action Space #
        assert action_space in ["cartesian_position", "joint_position", "cartesian_velocity", "joint_velocity"]
        self.action_space = action_space
        self.gripper_action_space = gripper_action_space
        self.check_action_range = "velocity" in action_space

        # Robot Configuration
        self.reset_joints = np.array([0, -1 / 5 * np.pi, 0, -4 / 5 * np.pi, 0, 3 / 5 * np.pi, 0.0])
        self.randomize_low = np.array([-0.1, -0.2, -0.1, -0.3, -0.3, -0.3])
        self.randomize_high = np.array([0.1, 0.2, 0.1, 0.3, 0.3, 0.3])
        self.DoF = 7 if ("cartesian" in action_space) else 8
        self.control_hz = 15

        if nuc_ip is None:
            from franka.robot import FrankaRobot

            self._robot = FrankaRobot()
        else:
            self._robot = ServerInterface(ip_address=nuc_ip)

        # Create Cameras
        self.camera_reader = MultiCameraWrapper(camera_kwargs)
        self.calibration_dict = load_calibration_info()
        self.camera_type_dict = camera_type_dict

        # Reset Robot
        if do_reset:
            self.reset()

    def step(self, action):
        # Check Action
        assert len(action) == self.DoF
        if self.check_action_range:
            assert (action.max() <= 1) and (action.min() >= -1)

        # Update Robot
        action_info = self.update_robot(
            action,
            action_space=self.action_space,
            gripper_action_space=self.gripper_action_space,
        )

        # Return Action Info
        return action_info

    def reset(self, randomize=False):
        self._robot.update_gripper(0, velocity=False, blocking=True)

        if randomize:
            noise = np.random.uniform(low=self.randomize_low, high=self.randomize_high)
        else:
            noise = None

        self._robot.update_joints(self.reset_joints, velocity=False, blocking=True, cartesian_noise=noise)

    def update_robot(self, action, action_space="cartesian_velocity", gripper_action_space=None, blocking=False):
        action_info = self._robot.update_command(
            action,
            action_space=action_space,
            gripper_action_space=gripper_action_space,
            blocking=blocking
        )
        return action_info

    def create_action_dict(self, action):
        return self._robot.create_action_dict(action)

    def read_cameras(self):
        return self.camera_reader.read_cameras()

    def get_state(self):
        read_start = time_ms()
        state_dict, timestamp_dict = self._robot.get_robot_state()
        timestamp_dict["read_start"] = read_start
        timestamp_dict["read_end"] = time_ms()
        return state_dict, timestamp_dict

    def get_camera_extrinsics(self, state_dict):
        # Adjust gripper camere by current pose
        extrinsics = deepcopy(self.calibration_dict)
        for cam_id in self.calibration_dict:
            if hand_camera_id not in cam_id:
                continue
            gripper_pose = state_dict["cartesian_position"]
            extrinsics[cam_id + "_gripper_offset"] = extrinsics[cam_id]
            extrinsics[cam_id] = change_pose_frame(extrinsics[cam_id], gripper_pose)
        return extrinsics

    def get_observation(self):
        obs_dict = {"timestamp": {}}

        # Robot State #
        state_dict, timestamp_dict = self.get_state()
        obs_dict["robot_state"] = state_dict
        obs_dict["timestamp"]["robot_state"] = timestamp_dict

        # Camera Readings #
        camera_obs, camera_timestamp = self.read_cameras()
        obs_dict.update(camera_obs)
        obs_dict["timestamp"]["cameras"] = camera_timestamp

        # Camera Info #
        obs_dict["camera_type"] = deepcopy(self.camera_type_dict)
        extrinsics = self.get_camera_extrinsics(state_dict)
        obs_dict["camera_extrinsics"] = extrinsics

        intrinsics = {}
        for cam in self.camera_reader.camera_dict.values():
            cam_intr_info = cam.get_intrinsics()
            for (full_cam_id, info) in cam_intr_info.items():
                intrinsics[full_cam_id] = info["cameraMatrix"]
        obs_dict["camera_intrinsics"] = intrinsics

        return obs_dict



================================================
FILE: droid/calibration/__init__.py
================================================
[Empty file]


================================================
FILE: droid/calibration/calibration_info.json
================================================
{"26405488_left": {"pose": [0.1113325872574514, 0.8207481722451926, 0.04550905451430457, -1.64633974845365, -0.17857016318886476, -2.937663676632542], "timestamp": 1673597556.0704472}, "26405488_right": {"pose": [-0.009417838799177, 0.7972668750161092, 0.0693151223699266, -1.6473025257925253, -0.18917295438381165, -2.942218897492042], "timestamp": 1673597557.3687217}, "19824535_left": {"pose": [-0.07528310648512061, 0.02872136764193708, 0.023316565592628957, -0.3255773557130956, 0.006247432431090383, -1.562874530416053], "timestamp": 1680893804.7454631}, "19824535_right": {"pose": [-0.06961440752114842, -0.035396599227355253, 0.020410786906711484, -0.31886273624702377, 0.007822018427507738, -1.552143062181081], "timestamp": 1680893807.0068362}, "29838012_left": {"pose": [0.41548840480128696, 0.4775470950869088, 0.4114322255308952, -2.2160410202605263, -0.02257694787387754, -2.963723863071196], "timestamp": 1686263782.2506583}, "29838012_right": {"pose": [0.294022371612491, 0.45672117609819446, 0.4111009411005134, -2.2079948175639927, -0.01940119744356439, -2.959776211707233], "timestamp": 1686263784.2242827}, "23404442_left": {"pose": [0.25955495037528126, -0.5001018266363574, 0.436881899751338, -2.2773456296459424, 0.3361897716248827, -0.5369007045203406], "timestamp": 1686263149.2820735}, "23404442_right": {"pose": [0.3520388119867355, -0.5577297192334658, 0.4018245547798702, -2.279806362246719, 0.34639742666205775, -0.5475215836690435], "timestamp": 1686263151.7758524}}



================================================
FILE: droid/calibration/calibration_utils.py
================================================
import json
import os
import time
from collections import defaultdict

import cv2
import matplotlib.pyplot as plt
import numpy as np
from cv2 import aruco
from scipy.spatial.transform import Rotation as R

from droid.misc.parameters import *
from droid.misc.transformations import *

# Create Board #
CHARUCO_BOARD = aruco.CharucoBoard_create(
    squaresX=CHARUCOBOARD_COLCOUNT,
    squaresY=CHARUCOBOARD_ROWCOUNT,
    squareLength=CHARUCOBOARD_CHECKER_SIZE,
    markerLength=CHARUCOBOARD_MARKER_SIZE,
    dictionary=ARUCO_DICT,
)

# Detector Params
detector_params = cv2.aruco.DetectorParameters_create()
detector_params.cornerRefinementMethod = cv2.aruco.CORNER_REFINE_SUBPIX
calib_flags = cv2.CALIB_USE_INTRINSIC_GUESS + cv2.CALIB_FIX_PRINCIPAL_POINT + cv2.CALIB_FIX_FOCAL_LENGTH

# Prepare Calibration Info #
dir_path = os.path.dirname(os.path.realpath(__file__))
calib_info_filepath = os.path.join(dir_path, "calibration_info.json")


def load_calibration_info(keep_time=False):
    if not os.path.isfile(calib_info_filepath):
        return {}
    with open(calib_info_filepath, "r") as jsonFile:
        calibration_info = json.load(jsonFile)
    if not keep_time:
        calibration_info = {key: data["pose"] for key, data in calibration_info.items()}
    return calibration_info


def update_calibration_info(cam_id, transformation):
    calibration_info = load_calibration_info(keep_time=True)
    calibration_info[cam_id] = {"pose": list(transformation), "timestamp": time.time()}

    with open(calib_info_filepath, "w") as jsonFile:
        json.dump(calibration_info, jsonFile)


def check_calibration_info(required_ids, time_threshold=3600):
    calibration_info = load_calibration_info(keep_time=True)
    calibration_ids = list(calibration_info.keys())
    info_dict = {"missing": [], "old": []}

    for cam_id in required_ids:
        if cam_id not in calibration_ids:
            info_dict["missing"].append(cam_id)
            continue
        time_passed = time.time() - calibration_info[cam_id]["timestamp"]
        if time_passed > time_threshold:
            info_dict["old"].append(cam_id)

    return info_dict


def visualize_calibration(calibration_dict):
    shapes = [".", "o", "v", "^", "s", "x", "D", "h", "<", ">", "8", "1", "2", "3"]
    assert len(calibration_dict) < (len(shapes) - 1)
    plt.clf()

    axes = plt.subplot(111, projection="3d")
    axes.plot(0, 0, 0, "*", label="Robot Base")

    for view_id in calibration_dict:
        curr_shape = shapes.pop(0)
        pose = calibration_dict[view_id]
        angle = [int(d * 180 / np.pi) for d in pose[3:]]
        label = "{0}: {1}".format(view_id, angle)
        axes.plot(pose[0], pose[1], pose[2], curr_shape, label=label)

    plt.legend(loc="center right", bbox_to_anchor=(2, 0.5))
    plt.title("Calibration Visualization")
    plt.show()


def calibration_traj(t, pos_scale=0.1, angle_scale=0.2, hand_camera=False):
    x = -np.abs(np.sin(3 * t)) * pos_scale
    y = -0.8 * np.sin(2 * t) * pos_scale
    z = 0.5 * np.sin(4 * t) * pos_scale
    a = -np.sin(4 * t) * angle_scale
    b = np.sin(3 * t) * angle_scale
    c = np.sin(2 * t) * angle_scale
    if hand_camera:
        value = np.array([z, y, -x, c / 1.5, b / 1.5, -a / 1.5])
    else:
        value = np.array([x, y, z, a, b, c])
    return value


class CharucoDetector:
    def __init__(
        self,
        intrinsics_dict,
        inlier_error_threshold=3.0,
        reprojection_error_threshold=3.0,
        num_img_threshold=10,
        num_corner_threshold=10,
    ):
        # Set Parameters
        self.inlier_error_threshold = inlier_error_threshold
        self.reprojection_error_threshold = reprojection_error_threshold
        self.num_img_threshold = num_img_threshold
        self.num_corner_threshold = num_corner_threshold
        self.intrinsic_params = {}
        self._intrinsics_dict = intrinsics_dict
        self._readings_dict = defaultdict(list)
        self._pose_dict = defaultdict(list)
        self._curr_cam_id = None

    def process_image(self, image):
        if image.shape[2] == 4:
            gray = cv2.cvtColor(image, cv2.COLOR_BGRA2GRAY)
        elif image.shape[2] == 3:
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        else:
            raise ValueError
        img_size = image.shape[:2]

        # Find Aruco Markers In Image #
        corners, ids, rejected = aruco.detectMarkers(image=gray, dictionary=ARUCO_DICT, parameters=detector_params)

        corners, ids, _, _ = cv2.aruco.refineDetectedMarkers(
            gray,
            CHARUCO_BOARD,
            corners,
            ids,
            rejected,
            parameters=detector_params,
            **self._intrinsics_dict[self._curr_cam_id],
        )

        # Find Charuco Corners #
        if len(corners) == 0:
            return None

        num_corners_found, charuco_corners, charuco_ids = aruco.interpolateCornersCharuco(
            markerCorners=corners, markerIds=ids, image=gray, board=CHARUCO_BOARD, **self.intrinsic_params
        )

        if num_corners_found < self.num_corner_threshold:
            return None

        return corners, charuco_corners, charuco_ids, img_size

    def add_sample(self, cam_id, image, pose):
        readings = self.process_image(image)
        if readings is None:
            return
        self._readings_dict[cam_id].append(readings)
        self._pose_dict[cam_id].append(pose)

    def calculate_target_to_cam(self, readings, train=True):
        init_corners_all = []  # Corners discovered in all images processed
        init_ids_all = []  # Aruco ids corresponding to corners discovered
        fixed_image_size = readings[0][3]

        # Proccess Readings #
        init_successes = []
        for i in range(len(readings)):
            corners, charuco_corners, charuco_ids, img_size = readings[i]
            assert img_size == fixed_image_size
            init_corners_all.append(charuco_corners)
            init_ids_all.append(charuco_ids)
            init_successes.append(i)

        # First Pass: Find Outliers #
        threshold = self.num_img_threshold if train else 5
        if len(init_successes) < threshold:
            return None
        # print('Not enough points round 1')
        # print('Num Points: ', len(init_successes))
        # return None

        calibration_error, cameraMatrix, distCoeffs, rvecs, tvecs, stdIntrinsics, stdExtrinsics, perViewErrors = (
            aruco.calibrateCameraCharucoExtended(
                charucoCorners=init_corners_all,
                charucoIds=init_ids_all,
                board=CHARUCO_BOARD,
                imageSize=fixed_image_size,
                flags=calib_flags,
                **self._intrinsics_dict[self._curr_cam_id],
            )
        )

        # Remove Outliers #
        threshold = self.num_img_threshold if train else 5
        final_corners_all = [
            init_corners_all[i] for i in range(len(perViewErrors)) if perViewErrors[i] <= self.inlier_error_threshold
        ]
        final_ids_all = [
            init_ids_all[i] for i in range(len(perViewErrors)) if perViewErrors[i] <= self.inlier_error_threshold
        ]
        final_successes = [
            init_successes[i] for i in range(len(perViewErrors)) if perViewErrors[i] <= self.inlier_error_threshold
        ]
        if len(final_successes) < threshold:
            return None
        # print('Not enough points round 2')
        # print('Num Points: ', len(final_successes))
        # print('Error Mean: ', perViewErrors.mean())
        # print('Error Std: ', perViewErrors.std())
        # return None

        # Second Pass: Calculate Finalized Extrinsics #
        calibration_error, cameraMatrix, distCoeffs, rvecs, tvecs = aruco.calibrateCameraCharuco(
            charucoCorners=final_corners_all,
            charucoIds=final_ids_all,
            board=CHARUCO_BOARD,
            imageSize=fixed_image_size,
            flags=calib_flags,
            **self._intrinsics_dict[self._curr_cam_id],
        )

        # Return Transformation #
        if calibration_error > self.reprojection_error_threshold:
            return None
        # print('Failed Calibration Threshold')
        # print('Calibration Error: ', calibration_error)
        # return None

        rmats = [R.from_rotvec(rvec.flatten()).as_matrix() for rvec in rvecs]
        tvecs = [tvec.flatten() for tvec in tvecs]

        return rmats, tvecs, final_successes

    def augment_image(self, cam_id, image, visualize=False, visual_type=["markers", "axes"]):
        if type(visual_type) != list:
            visual_type = [visual_type]
        assert all([t in ["markers", "charuco", "axes"] for t in visual_type])
        if image.shape[2] == 4:
            image = cv2.cvtColor(image, cv2.COLOR_BGRA2BGR)
        self._curr_cam_id = cam_id

        image = np.copy(image)
        readings = self.process_image(image)

        if readings is None:
            if visualize:
                cv2.imshow("Charuco board: {0}".format(cam_id), image)
                cv2.waitKey(20)
            return image

        corners, charuco_corners, charuco_ids, image_size = readings

        # Outline the aruco markers found in our query image
        if "markers" in visual_type:
            image = aruco.drawDetectedMarkers(image=image, corners=corners)

        # Draw the Charuco board we've detected to show our calibrator the board was properly detected
        if "charuco" in visual_type:
            image = aruco.drawDetectedCornersCharuco(image=image, charucoCorners=charuco_corners, charucoIds=charuco_ids)

        if "axes" in visual_type:
            calibration_error, cameraMatrix, distCoeffs, rvecs, tvecs = aruco.calibrateCameraCharuco(
                charucoCorners=[charuco_corners],
                charucoIds=[charuco_ids],
                board=CHARUCO_BOARD,
                imageSize=image_size,
                flags=calib_flags,
                **self._intrinsics_dict[self._curr_cam_id],
            )
            cv2.drawFrameAxes(image, cameraMatrix, distCoeffs, rvecs[0], tvecs[0], 0.1)

        # Visualize
        if visualize:
            cv2.imshow("Charuco board: {0}".format(cam_id), image)
            cv2.waitKey(20)

        return image


class ThirdPersonCameraCalibrator(CharucoDetector):
    def __init__(
        self, intrinsics_dict, lin_error_threshold=1e-3, rot_error_threshold=1e-2, train_percentage=0.7, **kwargs
    ):
        self.lin_error_threshold = lin_error_threshold
        self.rot_error_threshold = rot_error_threshold
        self.train_percentage = train_percentage
        super().__init__(intrinsics_dict, **kwargs)

    def calibrate(self, cam_id):
        return self._calibrate_cam_to_base(cam_id=cam_id)

    def _calibrate_cam_to_base(self, cam_id=None, readings=None, gripper_poses=None, target2cam_results=None):
        # Get Calibration Data #
        if cam_id is not None:
            readings, gripper_poses = self._readings_dict[cam_id], self._pose_dict[cam_id]
            self._curr_cam_id = cam_id

        # Get Target2Cam Transformation #
        if target2cam_results is None:
            target2cam_results = self.calculate_target_to_cam(readings)
        if target2cam_results is None:
            return None

        R_target2cam, t_target2cam, successes = target2cam_results
        gripper_poses = np.array(gripper_poses)[successes]

        # Calculate Appropriate Transformations #
        t_base2gripper = [
            -R.from_euler("xyz", pose[3:6]).inv().as_matrix() @ np.array(pose[:3]) for pose in gripper_poses
        ]
        R_base2gripper = [R.from_euler("xyz", pose[3:6]).inv().as_matrix() for pose in gripper_poses]

        # Perform Calibration #
        rmat, pos = cv2.calibrateHandEye(
            R_gripper2base=R_base2gripper,
            t_gripper2base=t_base2gripper,
            R_target2cam=R_target2cam,
            t_target2cam=t_target2cam,
            method=4,
        )

        # Return Pose #
        pos = pos.flatten()
        angle = R.from_matrix(rmat).as_euler("xyz")
        pose = np.concatenate([pos, angle])

        return pose

    def _calibrate_gripper_to_target(self, cam_id=None, readings=None, gripper_poses=None, target2cam_results=None):
        # Get Calibration Data #
        if cam_id is not None:
            readings, gripper_poses = self._readings_dict[cam_id], self._pose_dict[cam_id]
            self._curr_cam_id = cam_id

        # Get Target2Cam Transformation #
        if target2cam_results is None:
            target2cam_results = self.calculate_target_to_cam(readings)
        if target2cam_results is None:
            return None

        R_target2cam, t_target2cam, successes = target2cam_results
        gripper_poses = np.array(gripper_poses)[successes]

        # Calculate Appropriate Transformations #
        t_base2gripper = [
            -R.from_euler("xyz", pose[3:6]).inv().as_matrix() @ np.array(pose[:3]) for pose in gripper_poses
        ]
        R_base2gripper = [R.from_euler("xyz", pose[3:6]).inv().as_matrix() for pose in gripper_poses]

        # Perform Calibration #
        rmat, pos = cv2.calibrateHandEye(
            R_gripper2base=R_target2cam,
            t_gripper2base=t_target2cam,
            R_target2cam=R_base2gripper,
            t_target2cam=t_base2gripper,
            method=4,
        )

        # Return Pose #
        pos = pos.flatten()
        angle = R.from_matrix(rmat).as_euler("xyz")
        pose = np.concatenate([pos, angle])

        return pose

    def _calculate_gripper_to_base(self, train_readings, train_gripper_poses, eval_readings=None):
        if eval_readings is None:
            eval_readings = train_readings

        # Get Eval Target2Cam Transformations #
        eval_results = self.calculate_target_to_cam(eval_readings, train=False)
        if eval_results is None:
            return None
        eval_R_target2cam, eval_t_target2cam, eval_successes = eval_results
        rmats, tvecs = [], []

        # Get Train Target2Cam Transformations #
        train_results = self.calculate_target_to_cam(train_readings)
        if train_results is None:
            return None

        # Use Training Data For Calibrations #
        gripper2target = self._calibrate_gripper_to_target(
            gripper_poses=train_gripper_poses, target2cam_results=train_results
        )
        R_gripper2target = R.from_euler("xyz", gripper2target[3:]).as_matrix()
        t_gripper2target = np.array(gripper2target[:3])

        cam2base = self._calibrate_cam_to_base(gripper_poses=train_gripper_poses, target2cam_results=train_results)
        R_cam2base = R.from_euler("xyz", cam2base[3:]).as_matrix()
        t_cam2base = np.array(cam2base[:3])

        # Calculate Gripper2Base #
        for i in range(len(eval_R_target2cam)):
            R_gripper2cam = eval_R_target2cam[i] @ R_gripper2target
            t_gripper2cam = eval_R_target2cam[i] @ t_gripper2target + eval_t_target2cam[i]

            R_gripper2base = R_cam2base @ R_gripper2cam
            t_gripper2base = R_cam2base @ t_gripper2cam + t_cam2base

            rmats.append(R_gripper2base)
            tvecs.append(t_gripper2base)

        # Return Poses #
        eulers = np.array([R.from_matrix(rmat).as_euler("xyz") for rmat in rmats])
        eval_poses = np.concatenate([np.array(tvecs), eulers], axis=1)

        return eval_poses, eval_successes

    def is_calibration_accurate(self, cam_id):
        # Set Camera #
        self._curr_cam_id = cam_id

        # Split Into Train / Test #
        readings = self._readings_dict[cam_id]
        if len(readings) == 0:
            return False
        poses = np.array(self._pose_dict[cam_id])
        ind = np.random.choice(len(readings), size=len(readings), replace=False)
        num_train = int(len(readings) * self.train_percentage)

        train_ind, test_ind = ind[:num_train], ind[num_train:]
        train_poses, test_poses = poses[train_ind], poses[test_ind]
        train_readings = [readings[i] for i in train_ind]
        test_readings = [readings[i] for i in test_ind]

        # Calculate Approximate Gripper2Base Transformations #
        results = self._calculate_gripper_to_base(train_readings, train_poses, eval_readings=test_readings)
        if results is None:
            return False
        approx_poses, successes = results
        test_poses = np.array(test_poses)[successes]

        # Calculate Per Dimension Error #
        pose_error = np.array([pose_diff(pose, approx_pose) for pose, approx_pose in zip(test_poses, approx_poses)])
        lin_error = np.linalg.norm(pose_error[:, :3], axis=0) ** 2 / pose_error.shape[0]
        rot_error = np.linalg.norm(pose_error[:, 3:6], axis=0) ** 2 / pose_error.shape[0]

        # Check Calibration Error #
        lin_success = np.all(lin_error < self.lin_error_threshold)
        rot_success = np.all(rot_error < self.rot_error_threshold)

        # print('Pose Std: ', poses.std(axis=0))
        # print('Lin Error: ', lin_error)
        # print('Rot Error: ', rot_error)

        return lin_success and rot_success


class HandCameraCalibrator(CharucoDetector):
    def __init__(self, camera, lin_error_threshold=1e-3, rot_error_threshold=1e-2, train_percentage=0.7, **kwargs):
        self.lin_error_threshold = lin_error_threshold
        self.rot_error_threshold = rot_error_threshold
        self.train_percentage = train_percentage
        super().__init__(camera, **kwargs)

    def calibrate(self, cam_id):
        return self._calibrate_cam_to_gripper(cam_id=cam_id)

    def _calibrate_cam_to_gripper(self, cam_id=None, readings=None, gripper_poses=None, target2cam_results=None):
        # Get Calibration Data #
        if cam_id is not None:
            readings, gripper_poses = self._readings_dict[cam_id], self._pose_dict[cam_id]
            self._curr_cam_id = cam_id

        # Get Target2Cam Transformation #
        if target2cam_results is None:
            target2cam_results = self.calculate_target_to_cam(readings)
        if target2cam_results is None:
            return None

        R_target2cam, t_target2cam, successes = target2cam_results
        gripper_poses = np.array(gripper_poses)[successes]

        # Calculate Appropriate Transformations #
        t_gripper2base = [np.array(pose[:3]) for pose in gripper_poses]
        R_gripper2base = [R.from_euler("xyz", pose[3:6]).as_matrix() for pose in gripper_poses]

        # Perform Calibration #
        rmat, pos = cv2.calibrateHandEye(
            R_gripper2base=R_gripper2base,
            t_gripper2base=t_gripper2base,
            R_target2cam=R_target2cam,
            t_target2cam=t_target2cam,
            method=4,
        )

        # Return Pose #
        pos = pos.flatten()
        angle = R.from_matrix(rmat).as_euler("xyz")
        pose = np.concatenate([pos, angle])

        return pose

    def _calibrate_base_to_target(self, cam_id=None, readings=None, gripper_poses=None, target2cam_results=None):
        # Get Calibration Data #
        if cam_id is not None:
            readings, gripper_poses = self._readings_dict[cam_id], self._pose_dict[cam_id]
            self._curr_cam_id = cam_id

        # Get Target2Cam Transformation #
        if target2cam_results is None:
            target2cam_results = self.calculate_target_to_cam(readings)
        if target2cam_results is None:
            return None

        R_target2cam, t_target2cam, successes = target2cam_results
        gripper_poses = np.array(gripper_poses)[successes]

        # Calculate Appropriate Transformations #
        t_gripper2base = [np.array(pose[:3]) for pose in gripper_poses]
        R_gripper2base = [R.from_euler("xyz", pose[3:6]).as_matrix() for pose in gripper_poses]

        # Perform Calibration #
        rmat, pos = cv2.calibrateHandEye(
            R_gripper2base=R_target2cam,
            t_gripper2base=t_target2cam,
            R_target2cam=R_gripper2base,
            t_target2cam=t_gripper2base,
            method=4,
        )

        # Return Pose #
        pos = pos.flatten()
        angle = R.from_matrix(rmat).as_euler("xyz")
        pose = np.concatenate([pos, angle])

        return pose

    def _calculate_gripper_to_base(self, train_readings, train_gripper_poses, eval_readings=None):
        if eval_readings is None:
            eval_readings = train_readings

        # Get Eval Target2Cam Transformations #
        eval_results = self.calculate_target_to_cam(eval_readings, train=False)
        if eval_results is None:
            return None
        eval_R_target2cam, eval_t_target2cam, eval_successes = eval_results
        rmats, tvecs = [], []

        # Get Train Target2Cam Transformations #
        train_results = self.calculate_target_to_cam(train_readings)
        if train_results is None:
            return None

        # Use Training Data For Calibrations #
        base2target = self._calibrate_base_to_target(gripper_poses=train_gripper_poses, target2cam_results=train_results)
        R_base2target = R.from_euler("xyz", base2target[3:]).as_matrix()
        t_base2target = np.array(base2target[:3])

        cam2gripper = self._calibrate_cam_to_gripper(gripper_poses=train_gripper_poses, target2cam_results=train_results)
        R_cam2gripper = R.from_euler("xyz", cam2gripper[3:]).as_matrix()
        t_cam2gripper = np.array(cam2gripper[:3])

        # Calculate Gripper2Base #
        for i in range(len(eval_R_target2cam)):
            R_base2cam = eval_R_target2cam[i] @ R_base2target
            t_base2cam = eval_R_target2cam[i] @ t_base2target + eval_t_target2cam[i]

            R_base2gripper = R_cam2gripper @ R_base2cam
            t_base2gripper = R_cam2gripper @ t_base2cam + t_cam2gripper

            R_gripper2base = R.from_matrix(R_base2gripper).inv().as_matrix()
            t_gripper2base = -R_gripper2base @ t_base2gripper

            rmats.append(R_gripper2base)
            tvecs.append(t_gripper2base)

        # Return Poses #
        eulers = np.array([R.from_matrix(rmat).as_euler("xyz") for rmat in rmats])
        eval_poses = np.concatenate([np.array(tvecs), eulers], axis=1)

        return eval_poses, eval_successes

    def is_calibration_accurate(self, cam_id):
        # Set Camera #
        self._curr_cam_id = cam_id

        # Split Into Train / Test #
        readings = self._readings_dict[cam_id]
        if len(readings) == 0:
            return False
        poses = np.array(self._pose_dict[cam_id])
        ind = np.random.choice(len(readings), size=len(readings), replace=False)
        num_train = int(len(readings) * self.train_percentage)

        train_ind, test_ind = ind[:num_train], ind[num_train:]
        train_poses, test_poses = poses[train_ind], poses[test_ind]
        train_readings = [readings[i] for i in train_ind]
        test_readings = [readings[i] for i in test_ind]

        # Calculate Approximate Gripper2Base Transformations #
        results = self._calculate_gripper_to_base(train_readings, train_poses, eval_readings=test_readings)
        if results is None:
            return False
        approx_poses, successes = results
        test_poses = np.array(test_poses)[successes]

        # Calculate Per Dimension Error #
        pose_error = np.array([pose_diff(pose, approx_pose) for pose, approx_pose in zip(test_poses, approx_poses)])
        lin_error = np.linalg.norm(pose_error[:, :3], axis=0) ** 2 / pose_error.shape[0]
        rot_error = np.linalg.norm(pose_error[:, 3:6], axis=0) ** 2 / pose_error.shape[0]

        # Check Calibration Error #
        lin_success = np.all(lin_error < self.lin_error_threshold)
        rot_success = np.all(rot_error < self.rot_error_threshold)

        # print('Pose Std: ', poses.std(axis=0))
        # print('Lin Error: ', lin_error)
        # print('Rot Error: ', rot_error)

        return lin_success and rot_success



================================================
FILE: droid/camera_utils/__init__.py
================================================
[Empty file]


================================================
FILE: droid/camera_utils/info.py
================================================
from droid.misc.parameters import *

camera_type_dict = {
    hand_camera_id: 0,
    varied_camera_1_id: 1,
    varied_camera_2_id: 1,
}

camera_type_to_string_dict = {
    0: "hand_camera",
    1: "varied_camera",
    2: "fixed_camera",
}

camera_name_dict = {
    hand_camera_id: "Hand Camera",
    varied_camera_1_id: "Varied Camera #1",
    varied_camera_2_id: "Varied Camera #2",
}


def get_camera_name(cam_id):
    if cam_id in camera_name_dict:
        return camera_name_dict[cam_id]
    return cam_id


def get_camera_type(cam_id):
    if cam_id not in camera_type_dict:
        return None
    type_int = camera_type_dict[cam_id]
    type_str = camera_type_to_string_dict[type_int]
    return type_str



================================================
FILE: droid/camera_utils/camera_readers/__init__.py
================================================
[Empty file]


================================================
FILE: droid/camera_utils/camera_readers/zed_camera.py
================================================
from copy import deepcopy

import cv2
import numpy as np

from droid.misc.parameters import hand_camera_id
from droid.misc.time import time_ms

try:
    import pyzed.sl as sl
except ModuleNotFoundError:
    print("WARNING: You have not setup the ZED cameras, and currently cannot use them")


def gather_zed_cameras():
    all_zed_cameras = []
    try:
        cameras = sl.Camera.get_device_list()
    except NameError:
        return []

    for cam in cameras:
        cam = ZedCamera(cam)
        all_zed_cameras.append(cam)

    return all_zed_cameras


resize_func_map = {"cv2": cv2.resize, None: None}

standard_params = dict(
    depth_minimum_distance=0.1, camera_resolution=sl.RESOLUTION.HD720, depth_stabilization=False, camera_fps=60, camera_image_flip=sl.FLIP_MODE.OFF
)

advanced_params = dict(
    depth_minimum_distance=0.1, camera_resolution=sl.RESOLUTION.HD2K, depth_stabilization=False, camera_fps=15, camera_image_flip=sl.FLIP_MODE.OFF
)


class ZedCamera:
    def __init__(self, camera):
        # Save Parameters #
        self.serial_number = str(camera.serial_number)
        self.is_hand_camera = self.serial_number == hand_camera_id
        self.high_res_calibration = False
        self.current_mode = None
        self._current_params = None
        self._extriniscs = {}

        # Open Camera #
        print("Opening Zed: ", self.serial_number)

    def enable_advanced_calibration(self):
        self.high_res_calibration = True

    def disable_advanced_calibration(self):
        self.high_res_calibration = False

    def set_reading_parameters(
        self,
        image=True,
        depth=False,
        pointcloud=False,
        concatenate_images=False,
        resolution=(0, 0),
        resize_func=None,
    ):
        # Non-Permenant Values #
        self.traj_image = image
        self.traj_concatenate_images = concatenate_images
        self.traj_resolution = resolution

        # Permenant Values #
        self.depth = depth
        self.pointcloud = pointcloud
        self.resize_func = resize_func_map[resize_func]

    ### Camera Modes ###
    def set_calibration_mode(self):
        # Set Parameters #
        self.image = True
        self.concatenate_images = False
        self.skip_reading = False
        self.zed_resolution = sl.Resolution(0, 0)
        self.resizer_resolution = (0, 0)

        # Set Mode #
        change_settings_1 = (self.high_res_calibration) and (self._current_params != advanced_params)
        change_settings_2 = (not self.high_res_calibration) and (self._current_params != standard_params)
        if change_settings_1:
            self._configure_camera(advanced_params)
        if change_settings_2:
            self._configure_camera(standard_params)
        self.current_mode = "calibration"

    def set_trajectory_mode(self):
        # Set Parameters #
        self.image = self.traj_image
        self.concatenate_images = self.traj_concatenate_images
        self.skip_reading = not any([self.image, self.depth, self.pointcloud])

        if self.resize_func is None:
            self.zed_resolution = sl.Resolution(*self.traj_resolution)
            self.resizer_resolution = (0, 0)
        else:
            self.zed_resolution = sl.Resolution(0, 0)
            self.resizer_resolution = self.traj_resolution

        # Set Mode #
        change_settings = self._current_params != standard_params
        if change_settings:
            self._configure_camera(standard_params)
        self.current_mode = "trajectory"

    def _configure_camera(self, init_params):
        # Close Existing Camera #
        self.disable_camera()

        # Initialize Readers #
        self._cam = sl.Camera()
        self._sbs_img = sl.Mat()
        self._left_img = sl.Mat()
        self._right_img = sl.Mat()
        self._left_depth = sl.Mat()
        self._right_depth = sl.Mat()
        self._left_pointcloud = sl.Mat()
        self._right_pointcloud = sl.Mat()
        self._runtime = sl.RuntimeParameters()

        # Open Camera #
        self._current_params = init_params
        sl_params = sl.InitParameters(**init_params)
        sl_params.set_from_serial_number(int(self.serial_number))
        sl_params.camera_image_flip = sl.FLIP_MODE.OFF
        status = self._cam.open(sl_params)
        if status != sl.ERROR_CODE.SUCCESS:
            raise RuntimeError("Camera Failed To Open")

        # Save Intrinsics #
        self.latency = int(2.5 * (1e3 / sl_params.camera_fps))
        calib_params = self._cam.get_camera_information().camera_configuration.calibration_parameters
        self._intrinsics = {
            self.serial_number + "_left": self._process_intrinsics(calib_params.left_cam),
            self.serial_number + "_right": self._process_intrinsics(calib_params.right_cam),
        }

    ### Calibration Utilities ###
    def _process_intrinsics(self, params):
        intrinsics = {}
        intrinsics["cameraMatrix"] = np.array([[params.fx, 0, params.cx], [0, params.fy, params.cy], [0, 0, 1]])
        intrinsics["distCoeffs"] = np.array(list(params.disto))
        return intrinsics

    def get_intrinsics(self):
        return deepcopy(self._intrinsics)

    ### Recording Utilities ###
    def start_recording(self, filename):
        assert filename.endswith(".svo")
        recording_param = sl.RecordingParameters(filename, sl.SVO_COMPRESSION_MODE.H265)
        err = self._cam.enable_recording(recording_param)
        assert err == sl.ERROR_CODE.SUCCESS

    def stop_recording(self):
        self._cam.disable_recording()

    ### Basic Camera Utilities ###
    def _process_frame(self, frame):
        frame = deepcopy(frame.get_data())
        if self.resizer_resolution == (0, 0):
            return frame
        return self.resize_func(frame, self.resizer_resolution)

    def read_camera(self):
        # Skip if Read Unnecesary #
        if self.skip_reading:
            return {}, {}

        # Read Camera #
        timestamp_dict = {self.serial_number + "_read_start": time_ms()}
        err = self._cam.grab(self._runtime)
        if err != sl.ERROR_CODE.SUCCESS:
            return None
        timestamp_dict[self.serial_number + "_read_end"] = time_ms()

        # Benchmark Latency #
        received_time = self._cam.get_timestamp(sl.TIME_REFERENCE.IMAGE).get_milliseconds()
        timestamp_dict[self.serial_number + "_frame_received"] = received_time
        timestamp_dict[self.serial_number + "_estimated_capture"] = received_time - self.latency

        # Return Data #
        data_dict = {}

        if self.image:
            if self.concatenate_images:
                self._cam.retrieve_image(self._sbs_img, sl.VIEW.SIDE_BY_SIDE, resolution=self.zed_resolution)
                data_dict["image"] = {self.serial_number: self._process_frame(self._sbs_img)}
            else:
                self._cam.retrieve_image(self._left_img, sl.VIEW.LEFT, resolution=self.zed_resolution)
                self._cam.retrieve_image(self._right_img, sl.VIEW.RIGHT, resolution=self.zed_resolution)
                data_dict["image"] = {
                    self.serial_number + "_left": self._process_frame(self._left_img),
                    self.serial_number + "_right": self._process_frame(self._right_img),
                }
        # if self.depth:
        # 	self._cam.retrieve_measure(self._left_depth, sl.MEASURE.DEPTH, resolution=self.resolution)
        # 	self._cam.retrieve_measure(self._right_depth, sl.MEASURE.DEPTH_RIGHT, resolution=self.resolution)
        # 	data_dict['depth'] = {
        # 		self.serial_number + '_left': self._left_depth.get_data().copy(),
        # 		self.serial_number + '_right': self._right_depth.get_data().copy()}
        # if self.pointcloud:
        # 	self._cam.retrieve_measure(self._left_pointcloud, sl.MEASURE.XYZRGBA, resolution=self.resolution)
        # 	self._cam.retrieve_measure(self._right_pointcloud, sl.MEASURE.XYZRGBA_RIGHT, resolution=self.resolution)
        # 	data_dict['pointcloud'] = {
        # 		self.serial_number + '_left': self._left_pointcloud.get_data().copy(),
        # 		self.serial_number + '_right': self._right_pointcloud.get_data().copy()}

        return data_dict, timestamp_dict

    def disable_camera(self):
        if self.current_mode == "disabled":
            return
        if hasattr(self, "_cam"):
            self._current_params = None
            self._cam.close()
        self.current_mode = "disabled"

    def is_running(self):
        return self.current_mode != "disabled"



================================================
FILE: droid/camera_utils/recording_readers/mp4_reader.py
================================================
import json
import os
from copy import deepcopy

import cv2

resize_func_map = {"cv2": cv2.resize, None: None}


class MP4Reader:
    def __init__(self, filepath, serial_number):
        # Save Parameters #
        self.serial_number = serial_number
        self._index = 0

        # Open Video Reader #
        self._mp4_reader = cv2.VideoCapture(filepath)
        if not self._mp4_reader.isOpened():
            print(filepath)
            raise RuntimeError("Corrupted MP4 File")

        # Load Recording Timestamps #
        timestamp_filepath = filepath[:-4] + "_timestamps.json"
        if not os.path.isfile(timestamp_filepath):
            self._recording_timestamps = []
        with open(timestamp_filepath, "r") as jsonFile:
            self._recording_timestamps = json.load(jsonFile)

    def set_reading_parameters(
        self,
        image=True,
        concatenate_images=False,
        resolution=(0, 0),
        resize_func=None,
    ):
        # Save Parameters #
        self.image = image
        self.concatenate_images = concatenate_images
        self.resolution = resolution
        self.resize_func = resize_func_map[resize_func]
        self.skip_reading = not image
        if self.skip_reading:
            return

    def get_frame_resolution(self):
        width = self._mp4_reader.get(cv2.cv.CV_CAP_PROP_FRAME_WIDTH)
        height = self._mp4_reader.get(cv2.cv.CV_CAP_PROP_FRAME_HEIGHT)
        return (width, height)

    def get_frame_count(self):
        if self.skip_reading:
            return 0
        frame_count = int(self._mp4_reader.get(cv2.cv.CV_CAP_PROP_FRAME_COUNT))
        return frame_count

    def set_frame_index(self, index):
        if self.skip_reading:
            return

        if index < self._index:
            self._mp4_reader.set(cv2.CAP_PROP_POS_FRAMES, index - 1)
            self._index = index

        while self._index < index:
            self.read_camera(ignore_data=True)

    def _process_frame(self, frame):
        frame = deepcopy(frame)
        if self.resolution == (0, 0):
            return frame
        return self.resize_func(frame, self.resolution)
        # return cv2.resize(frame, self.resolution)#, interpolation=cv2.INTER_AREA)

    def read_camera(self, ignore_data=False, correct_timestamp=None, return_timestamp=False):
        # Skip if Read Unnecesary #
        if self.skip_reading:
            return {}

        # Read Camera #
        success, frame = self._mp4_reader.read()
        try:
            received_time = self._recording_timestamps[self._index]
        except IndexError:
            received_time = None

        self._index += 1
        if not success:
            return None
        if ignore_data:
            return None

        # Check Image Timestamp #
        timestamps_given = (received_time is not None) and (correct_timestamp is not None)
        if timestamps_given and (correct_timestamp != received_time):
            print("Timestamps did not match...")
            return None

        # Return Data #
        data_dict = {}

        if self.concatenate_images:
            data_dict["image"] = {self.serial_number: self._process_frame(frame)}
        else:
            single_width = frame.shape[1] // 2
            data_dict["image"] = {
                self.serial_number + "_left": self._process_frame(frame[:, :single_width, :]),
                self.serial_number + "_right": self._process_frame(frame[:, single_width:, :]),
            }

        if return_timestamp:
            return data_dict, received_time
        return data_dict

    def disable_camera(self):
        if hasattr(self, "_mp4_reader"):
            self._mp4_reader.release()



================================================
FILE: droid/camera_utils/recording_readers/svo_reader.py
================================================
from copy import deepcopy

import cv2

try:
    import pyzed.sl as sl
except ModuleNotFoundError:
    print("WARNING: You have not setup the ZED cameras, and currently cannot use them")

resize_func_map = {"cv2": cv2.resize, None: None}


class SVOReader:
    def __init__(self, filepath, serial_number):
        # Save Parameters #
        self.serial_number = serial_number
        self._index = 0

        # Initialize Readers #
        self._sbs_img = sl.Mat()
        self._left_img = sl.Mat()
        self._right_img = sl.Mat()
        self._left_depth = sl.Mat()
        self._right_depth = sl.Mat()
        self._left_pointcloud = sl.Mat()
        self._right_pointcloud = sl.Mat()

        # Set SVO path for playback
        init_parameters = sl.InitParameters()
        init_parameters.camera_image_flip = sl.FLIP_MODE.OFF
        init_parameters.set_from_svo_file(filepath)

        # Open the ZED
        self._cam = sl.Camera()
        status = self._cam.open(init_parameters)
        if status != sl.ERROR_CODE.SUCCESS:
            print("Zed Error: " + repr(status))

    def set_reading_parameters(
        self,
        image=True,
        depth=False,
        pointcloud=False,
        concatenate_images=False,
        resolution=(0, 0),
        resize_func=None,
    ):
        # Save Parameters #
        self.image = image
        self.depth = depth
        self.pointcloud = pointcloud
        self.concatenate_images = concatenate_images
        self.resize_func = resize_func_map[resize_func]

        if self.resize_func is None:
            self.zed_resolution = sl.Resolution(*resolution)
            self.resizer_resolution = (0, 0)
        else:
            self.zed_resolution = sl.Resolution(0, 0)
            self.resizer_resolution = resolution

        self.skip_reading = not any([image, depth, pointcloud])
        if self.skip_reading:
            return

    def get_frame_resolution(self):
        camera_info = self._cam.get_camera_information().camera_configuration
        width = camera_info.resolution.width
        height = camera_info.resolution.height
        return (width, height)

    def get_frame_count(self):
        if self.skip_reading:
            return 0
        return self._cam.get_svo_number_of_frames()

    def set_frame_index(self, index):
        if self.skip_reading:
            return

        if index < self._index:
            self._cam.set_svo_position(index)
            self._index = index

        while self._index < index:
            self.read_camera(ignore_data=True)

    def _process_frame(self, frame):
        frame = deepcopy(frame.get_data())
        if self.resizer_resolution == (0, 0):
            return frame
        return self.resize_func(frame, self.resizer_resolution)

    def read_camera(self, ignore_data=False, correct_timestamp=None, return_timestamp=False):
        # Skip if Read Unnecesary #
        if self.skip_reading:
            return {}

        # Read Camera #
        self._index += 1
        err = self._cam.grab()
        if err != sl.ERROR_CODE.SUCCESS:
            return None
        if ignore_data:
            return None

        # Check Image Timestamp #
        received_time = self._cam.get_timestamp(sl.TIME_REFERENCE.IMAGE).get_milliseconds()
        timestamp_error = (correct_timestamp is not None) and (correct_timestamp != received_time)

        if timestamp_error:
            print("Timestamps did not match...")
            return None

        # Return Data #
        data_dict = {}

        if self.image:
            if self.concatenate_images:
                self._cam.retrieve_image(self._sbs_img, sl.VIEW.SIDE_BY_SIDE, resolution=self.zed_resolution)
                data_dict["image"] = {self.serial_number: self._process_frame(self._sbs_img)}
            else:
                self._cam.retrieve_image(self._left_img, sl.VIEW.LEFT, resolution=self.zed_resolution)
                self._cam.retrieve_image(self._right_img, sl.VIEW.RIGHT, resolution=self.zed_resolution)
                data_dict["image"] = {
                    self.serial_number + "_left": self._process_frame(self._left_img),
                    self.serial_number + "_right": self._process_frame(self._right_img),
                }
        # if self.depth:
        # 	self._cam.retrieve_measure(self._left_depth, sl.MEASURE.DEPTH, resolution=self.resolution)
        # 	self._cam.retrieve_measure(self._right_depth, sl.MEASURE.DEPTH_RIGHT, resolution=self.resolution)
        # 	data_dict['depth'] = {
        # 		self.serial_number + '_left': self._left_depth.get_data().copy(),
        # 		self.serial_number + '_right': self._right_depth.get_data().copy()}
        # if self.pointcloud:
        # 	self._cam.retrieve_measure(self._left_pointcloud, sl.MEASURE.XYZRGBA, resolution=self.resolution)
        # 	self._cam.retrieve_measure(self._right_pointcloud, sl.MEASURE.XYZRGBA_RIGHT, resolution=self.resolution)
        # 	data_dict['pointcloud'] = {
        # 		self.serial_number + '_left': self._left_pointcloud.get_data().copy(),
        # 		self.serial_number + '_right': self._right_pointcloud.get_data().copy()}

        if return_timestamp:
            return data_dict, received_time
        return data_dict

    def disable_camera(self):
        if hasattr(self, "_cam"):
            self._cam.close()



================================================
FILE: droid/camera_utils/wrappers/__init__.py
================================================
[Empty file]


================================================
FILE: droid/camera_utils/wrappers/multi_camera_wrapper.py
================================================
import os
import random
from collections import defaultdict

from droid.camera_utils.camera_readers.zed_camera import gather_zed_cameras
from droid.camera_utils.info import get_camera_type


class MultiCameraWrapper:
    def __init__(self, camera_kwargs={}):
        # Open Cameras #
        zed_cameras = gather_zed_cameras()
        self.camera_dict = {cam.serial_number: cam for cam in zed_cameras}

        # Set Correct Parameters #
        for cam_id in self.camera_dict.keys():
            cam_type = get_camera_type(cam_id)
            curr_cam_kwargs = camera_kwargs.get(cam_type, {})
            self.camera_dict[cam_id].set_reading_parameters(**curr_cam_kwargs)

        # Launch Camera #
        self.set_trajectory_mode()

    ### Calibration Functions ###
    def get_camera(self, camera_id):
        return self.camera_dict[camera_id]

    def enable_advanced_calibration(self):
        for cam in self.camera_dict.values():
            cam.enable_advanced_calibration()

    def disable_advanced_calibration(self):
        for cam in self.camera_dict.values():
            cam.disable_advanced_calibration()

    def set_calibration_mode(self, cam_id):
        # If High Res Calibration, Only One Can Run #
        close_all = any([cam.high_res_calibration for cam in self.camera_dict.values()])

        if close_all:
            for curr_cam_id in self.camera_dict:
                if curr_cam_id != cam_id:
                    self.camera_dict[curr_cam_id].disable_camera()

        self.camera_dict[cam_id].set_calibration_mode()

    def set_trajectory_mode(self):
        # If High Res Calibration, Close All #
        close_all = any(
            [cam.high_res_calibration and cam.current_mode == "calibration" for cam in self.camera_dict.values()]
        )

        if close_all:
            for cam in self.camera_dict.values():
                cam.disable_camera()

        # Put All Cameras In Trajectory Mode #
        for cam in self.camera_dict.values():
            cam.set_trajectory_mode()

    ### Data Storing Functions ###
    def start_recording(self, recording_folderpath):
        subdir = os.path.join(recording_folderpath, "SVO")
        if not os.path.isdir(subdir):
            os.makedirs(subdir)
        for cam in self.camera_dict.values():
            filepath = os.path.join(subdir, cam.serial_number + ".svo")
            cam.start_recording(filepath)

    def stop_recording(self):
        for cam in self.camera_dict.values():
            cam.stop_recording()

    ### Basic Camera Functions ###
    def read_cameras(self):
        full_obs_dict = defaultdict(dict)
        full_timestamp_dict = {}

        # Read Cameras In Randomized Order #
        all_cam_ids = list(self.camera_dict.keys())
        random.shuffle(all_cam_ids)

        for cam_id in all_cam_ids:
            if not self.camera_dict[cam_id].is_running():
                continue
            data_dict, timestamp_dict = self.camera_dict[cam_id].read_camera()

            for key in data_dict:
                full_obs_dict[key].update(data_dict[key])
            full_timestamp_dict.update(timestamp_dict)

        return full_obs_dict, full_timestamp_dict

    def disable_cameras(self):
        for camera in self.camera_dict.values():
            camera.disable_camera()



================================================
FILE: droid/camera_utils/wrappers/recorded_multi_camera_wrapper.py
================================================
import glob
import random
from collections import defaultdict

from droid.camera_utils.info import get_camera_type
from droid.camera_utils.recording_readers.mp4_reader import MP4Reader
from droid.camera_utils.recording_readers.svo_reader import SVOReader


class RecordedMultiCameraWrapper:
    def __init__(self, recording_folderpath, camera_kwargs={}):
        # Save Camera Info #
        self.camera_kwargs = camera_kwargs

        # Open Camera Readers #
        svo_filepaths = glob.glob(recording_folderpath + "/*.svo")
        mp4_filepaths = glob.glob(recording_folderpath + "/*.mp4")
        all_filepaths = svo_filepaths + mp4_filepaths

        self.camera_dict = {}
        for f in all_filepaths:
            serial_number = f.split("/")[-1][:-4]
            cam_type = get_camera_type(serial_number)
            camera_kwargs.get(cam_type, {})

            if f.endswith(".svo"):
                Reader = SVOReader
            elif f.endswith(".mp4"):
                Reader = MP4Reader
            else:
                raise ValueError

            self.camera_dict[serial_number] = Reader(f, serial_number)

    def read_cameras(self, index=None, camera_type_dict={}, timestamp_dict={}):
        full_obs_dict = defaultdict(dict)

        # Read Cameras In Randomized Order #
        all_cam_ids = list(self.camera_dict.keys())
        random.shuffle(all_cam_ids)

        for cam_id in all_cam_ids:
            cam_type = camera_type_dict[cam_id]
            curr_cam_kwargs = self.camera_kwargs.get(cam_type, {})
            self.camera_dict[cam_id].set_reading_parameters(**curr_cam_kwargs)

            timestamp = timestamp_dict.get(cam_id + "_frame_received", None)
            if index is not None:
                self.camera_dict[cam_id].set_frame_index(index)

            data_dict = self.camera_dict[cam_id].read_camera(correct_timestamp=timestamp)

            # Process Returned Data #
            if data_dict is None:
                return None
            for key in data_dict:
                full_obs_dict[key].update(data_dict[key])

        return full_obs_dict

    def disable_cameras(self):
        for camera in self.camera_dict.values():
            camera.disable_camera()



================================================
FILE: droid/controllers/oculus_controller.py
================================================
import time

import numpy as np
from oculus_reader.reader import OculusReader

from droid.misc.subprocess_utils import run_threaded_command
from droid.misc.transformations import add_angles, euler_to_quat, quat_diff, quat_to_euler, rmat_to_quat


def vec_to_reorder_mat(vec):
    X = np.zeros((len(vec), len(vec)))
    for i in range(X.shape[0]):
        ind = int(abs(vec[i])) - 1
        X[i, ind] = np.sign(vec[i])
    return X


class VRPolicy:
    def __init__(
        self,
        right_controller: bool = True,
        max_lin_vel: float = 1,
        max_rot_vel: float = 1,
        max_gripper_vel: float = 1,
        spatial_coeff: float = 1,
        pos_action_gain: float = 5,
        rot_action_gain: float = 2,
        gripper_action_gain: float = 3,
        rmat_reorder: list = [-2, -1, -3, 4],
    ):
        self.oculus_reader = OculusReader()
        self.vr_to_global_mat = np.eye(4)
        self.max_lin_vel = max_lin_vel
        self.max_rot_vel = max_rot_vel
        self.max_gripper_vel = max_gripper_vel
        self.spatial_coeff = spatial_coeff
        self.pos_action_gain = pos_action_gain
        self.rot_action_gain = rot_action_gain
        self.gripper_action_gain = gripper_action_gain
        self.global_to_env_mat = vec_to_reorder_mat(rmat_reorder)
        self.controller_id = "r" if right_controller else "l"
        self.reset_orientation = True
        self.reset_state()

        # Start State Listening Thread #
        run_threaded_command(self._update_internal_state)

    def reset_state(self):
        self._state = {
            "poses": {},
            "buttons": {"A": False, "B": False, "X": False, "Y": False},
            "movement_enabled": False,
            "controller_on": True,
        }
        self.update_sensor = True
        self.reset_origin = True
        self.robot_origin = None
        self.vr_origin = None
        self.vr_state = None

    def _update_internal_state(self, num_wait_sec=5, hz=50):
        last_read_time = time.time()
        while True:
            # Regulate Read Frequency #
            time.sleep(1 / hz)

            # Read Controller
            time_since_read = time.time() - last_read_time
            poses, buttons = self.oculus_reader.get_transformations_and_buttons()
            self._state["controller_on"] = time_since_read < num_wait_sec
            if poses == {}:
                continue

            # Determine Control Pipeline #
            toggled = self._state["movement_enabled"] != buttons[self.controller_id.upper() + "G"]
            self.update_sensor = self.update_sensor or buttons[self.controller_id.upper() + "G"]
            self.reset_orientation = self.reset_orientation or buttons[self.controller_id.upper() + "J"]
            self.reset_origin = self.reset_origin or toggled

            # Save Info #
            self._state["poses"] = poses
            self._state["buttons"] = buttons
            self._state["movement_enabled"] = buttons[self.controller_id.upper() + "G"]
            self._state["controller_on"] = True
            last_read_time = time.time()

            # Update Definition Of "Forward" #
            stop_updating = self._state["buttons"][self.controller_id.upper() + "J"] or self._state["movement_enabled"]
            if self.reset_orientation:
                rot_mat = np.asarray(self._state["poses"][self.controller_id])
                if stop_updating:
                    self.reset_orientation = False
                # try to invert the rotation matrix, if not possible, then just use the identity matrix
                try:
                    rot_mat = np.linalg.inv(rot_mat)
                except:
                    print(f"exception for rot mat: {rot_mat}")
                    rot_mat = np.eye(4)
                    self.reset_orientation = True
                self.vr_to_global_mat = rot_mat

    def _process_reading(self):
        rot_mat = np.asarray(self._state["poses"][self.controller_id])
        rot_mat = self.global_to_env_mat @ self.vr_to_global_mat @ rot_mat
        vr_pos = self.spatial_coeff * rot_mat[:3, 3]
        vr_quat = rmat_to_quat(rot_mat[:3, :3])
        vr_gripper = self._state["buttons"]["rightTrig" if self.controller_id == "r" else "leftTrig"][0]

        self.vr_state = {"pos": vr_pos, "quat": vr_quat, "gripper": vr_gripper}

    def _limit_velocity(self, lin_vel, rot_vel, gripper_vel):
        """Scales down the linear and angular magnitudes of the action"""
        lin_vel_norm = np.linalg.norm(lin_vel)
        rot_vel_norm = np.linalg.norm(rot_vel)
        gripper_vel_norm = np.linalg.norm(gripper_vel)
        if lin_vel_norm > self.max_lin_vel:
            lin_vel = lin_vel * self.max_lin_vel / lin_vel_norm
        if rot_vel_norm > self.max_rot_vel:
            rot_vel = rot_vel * self.max_rot_vel / rot_vel_norm
        if gripper_vel_norm > self.max_gripper_vel:
            gripper_vel = gripper_vel * self.max_gripper_vel / gripper_vel_norm
        return lin_vel, rot_vel, gripper_vel

    def _calculate_action(self, state_dict, include_info=False):
        # Read Sensor #
        if self.update_sensor:
            self._process_reading()
            self.update_sensor = False

        # Read Observation
        robot_pos = np.array(state_dict["cartesian_position"][:3])
        robot_euler = state_dict["cartesian_position"][3:]
        robot_quat = euler_to_quat(robot_euler)
        robot_gripper = state_dict["gripper_position"]

        # Reset Origin On Release #
        if self.reset_origin:
            self.robot_origin = {"pos": robot_pos, "quat": robot_quat}
            self.vr_origin = {"pos": self.vr_state["pos"], "quat": self.vr_state["quat"]}
            self.reset_origin = False

        # Calculate Positional Action #
        robot_pos_offset = robot_pos - self.robot_origin["pos"]
        target_pos_offset = self.vr_state["pos"] - self.vr_origin["pos"]
        pos_action = target_pos_offset - robot_pos_offset

        # Calculate Euler Action #
        robot_quat_offset = quat_diff(robot_quat, self.robot_origin["quat"])
        target_quat_offset = quat_diff(self.vr_state["quat"], self.vr_origin["quat"])
        quat_action = quat_diff(target_quat_offset, robot_quat_offset)
        euler_action = quat_to_euler(quat_action)

        # Calculate Gripper Action #
        gripper_action = (self.vr_state["gripper"] * 1.5) - robot_gripper

        # Calculate Desired Pose #
        target_pos = pos_action + robot_pos
        target_euler = add_angles(euler_action, robot_euler)
        target_cartesian = np.concatenate([target_pos, target_euler])
        target_gripper = self.vr_state["gripper"]

        # Scale Appropriately #
        pos_action *= self.pos_action_gain
        euler_action *= self.rot_action_gain
        gripper_action *= self.gripper_action_gain
        lin_vel, rot_vel, gripper_vel = self._limit_velocity(pos_action, euler_action, gripper_action)

        # Prepare Return Values #
        info_dict = {"target_cartesian_position": target_cartesian, "target_gripper_position": target_gripper}
        action = np.concatenate([lin_vel, rot_vel, [gripper_vel]])
        action = action.clip(-1, 1)

        # Return #
        if include_info:
            return action, info_dict
        else:
            return action

    def get_info(self):
        return {
            "success": self._state["buttons"]["A"] if self.controller_id == 'r' else self._state["buttons"]["X"],
            "failure": self._state["buttons"]["B"] if self.controller_id == 'r' else self._state["buttons"]["Y"],
            "movement_enabled": self._state["movement_enabled"],
            "controller_on": self._state["controller_on"],
        }

    def forward(self, obs_dict, include_info=False):
        if self._state["poses"] == {}:
            action = np.zeros(7)
            if include_info:
                return action, {}
            else:
                return action
        return self._calculate_action(obs_dict["robot_state"], include_info=include_info)



================================================
FILE: droid/data_loading/data_loader.py
================================================
from torch.utils.data import DataLoader
from torch.utils.data.datapipes.iter import Shuffler

from droid.data_loading.dataset import TrajectoryDataset
from droid.data_loading.trajectory_sampler import *


def create_dataloader(
    data_folderpaths,
    recording_prefix="MP4",
    batch_size=32,
    num_workers=6,
    buffer_size=1000,
    prefetch_factor=2,
    traj_loading_kwargs={},
    timestep_filtering_kwargs={},
    camera_kwargs={},
    image_transform_kwargs={},
):
    traj_sampler = TrajectorySampler(
        data_folderpaths,
        recording_prefix=recording_prefix,
        traj_loading_kwargs=traj_loading_kwargs,
        timestep_filtering_kwargs=timestep_filtering_kwargs,
        image_transform_kwargs=image_transform_kwargs,
        camera_kwargs=camera_kwargs,
    )
    dataset = TrajectoryDataset(traj_sampler)
    shuffled_dataset = Shuffler(dataset, buffer_size=buffer_size)
    dataloader = DataLoader(
        shuffled_dataset, batch_size=batch_size, num_workers=num_workers, prefetch_factor=prefetch_factor
    )

    return dataloader


def create_train_test_data_loader(data_loader_kwargs={}, data_processing_kwargs={}, camera_kwargs={}):
    # Generate Train / Test Split #
    data_filtering_kwargs = data_loader_kwargs.pop("data_filtering_kwargs", {})
    train_folderpaths, test_folderpaths = generate_train_test_split(**data_filtering_kwargs)

    # Create Train / Test Dataloaders #
    train_dataloader = create_dataloader(
        train_folderpaths, **data_loader_kwargs, **data_processing_kwargs, camera_kwargs=camera_kwargs
    )
    test_dataloader = create_dataloader(
        test_folderpaths, **data_loader_kwargs, **data_processing_kwargs, camera_kwargs=camera_kwargs
    )

    return train_dataloader, test_dataloader


# This is nice bc we only have one buffer at a time, but isn't working
class TrainTestDataloader:
    def __init__(self, data_loader_kwargs={}, data_processing_kwargs={}, camera_kwargs={}):
        # Generate Train / Test Split #
        data_filtering_kwargs = data_loader_kwargs.pop("data_filtering_kwargs", {})
        self.train_folderpaths, self.test_folderpaths = generate_train_test_split(**data_filtering_kwargs)

        # Create Dataloader Generator #
        self.generate_dataloader = lambda folderpaths: create_dataloader(
            folderpaths, **data_loader_kwargs, **data_processing_kwargs, camera_kwargs=camera_kwargs
        )

    def set_train_mode(self):
        self.dataloader = self.generate_dataloader(self.train_folderpaths)

    def set_test_mode(self):
        self.dataloader = self.generate_dataloader(self.test_folderpaths)

    def __iter__(self):
        return iter(self.dataloader)



================================================
FILE: droid/data_loading/dataset.py
================================================
import torch
from torch.utils.data import IterableDataset


class TrajectoryDataset(IterableDataset):
    def __init__(self, trajectory_sampler):
        self._trajectory_sampler = trajectory_sampler

    def _refresh_generator(self):
        worker_info = torch.utils.data.get_worker_info()
        new_samples = self._trajectory_sampler.fetch_samples(worker_info=worker_info)
        self._sample_generator = iter(new_samples)

    def __iter__(self):
        self._refresh_generator()
        while True:
            try:
                yield next(self._sample_generator)
            except StopIteration:
                self._refresh_generator()



================================================
FILE: droid/data_loading/tf_data_loader.py
================================================
from functools import partial
from typing import Dict

import tensorflow as tf


def get_type_spec(path: str) -> Dict[str, tf.TensorSpec]:
    """Get a type spec from a tfrecord file.

    Args:
        path (str): Path to a single tfrecord file.

    Returns:
        dict: A dictionary mapping feature names to tf.TensorSpecs.
    """
    data = next(iter(tf.data.TFRecordDataset(path))).numpy()
    example = tf.train.Example()
    example.ParseFromString(data)

    out = {}
    for key, value in example.features.feature.items():
        data = value.bytes_list.value[0]
        tensor_proto = tf.make_tensor_proto([])
        tensor_proto.ParseFromString(data)
        dtype = tf.dtypes.as_dtype(tensor_proto.dtype)
        shape = [d.size for d in tensor_proto.tensor_shape.dim]
        shape[0] = None  # first dimension is trajectory length, which is variable
        out[key] = tf.TensorSpec(shape=shape, dtype=dtype)

    return out


def get_tf_dataloader(
    path: str,
    *,
    batch_size: int,
    shuffle_buffer_size: int = 25000,
    cache: bool = False,
) -> tf.data.Dataset:
    # get the tfrecord files
    paths = tf.io.gfile.glob(tf.io.gfile.join(path, "*.tfrecord"))

    # extract the type spec
    type_spec = get_type_spec(paths[0])

    # read the tfrecords (yields raw serialized examples)
    dataset = tf.data.TFRecordDataset(paths, num_parallel_reads=tf.data.AUTOTUNE)

    # decode the examples (yields trajectories)
    dataset = dataset.map(partial(_decode_example, type_spec=type_spec), num_parallel_calls=tf.data.AUTOTUNE)

    # cache all the dataloading (uses a lot of memory)
    if cache:
        dataset = dataset.cache()

    # do any trajectory-level transforms here (e.g. filtering, goal relabeling)

    # unbatch to get individual transitions
    dataset = dataset.unbatch()

    # process each transition
    dataset = dataset.map(_process_transition, num_parallel_calls=tf.data.AUTOTUNE)

    # do any transition-level transformations here (e.g. augmentations)

    # shuffle the dataset
    dataset = dataset.shuffle(shuffle_buffer_size)

    dataset = dataset.repeat()

    # batch the dataset
    dataset = dataset.batch(batch_size, num_parallel_calls=tf.data.AUTOTUNE)

    # always prefetch last
    dataset = dataset.prefetch(tf.data.AUTOTUNE)

    return dataset


def _decode_example(example_proto: tf.Tensor, type_spec: Dict[str, tf.TensorSpec]) -> Dict[str, tf.Tensor]:
    features = {key: tf.io.FixedLenFeature([], tf.string) for key in type_spec.keys()}
    parsed_features = tf.io.parse_single_example(example_proto, features)
    parsed_tensors = {key: tf.io.parse_tensor(parsed_features[key], spec.dtype) for key, spec in type_spec.items()}

    for key in parsed_tensors:
        parsed_tensors[key] = tf.ensure_shape(parsed_tensors[key], type_spec[key].shape)

    return parsed_tensors


def _process_transition(transition: Dict[str, tf.Tensor]) -> Dict[str, tf.Tensor]:
    for key in transition:
        if "image" in key:
            transition[key] = tf.io.decode_jpeg(transition[key])

            # convert to float and normalize to [-1, 1]
            transition[key] = tf.cast(transition[key], tf.float32) / 127.5 - 1.0
    return transition


if __name__ == "__main__":
    ### EXAMPLE USAGE ###
    import tqdm

    dataset = get_tf_dataloader(
        "/tmp/franka_tfrecord_test/train",
        batch_size=8,
        shuffle_buffer_size=1,
    )
    with tqdm.tqdm() as pbar:
        for batch in dataset.as_numpy_iterator():
            for key, value in batch.items():
                pbar.write(f"{key}: {value.shape}")
            images = batch["observation/image/20521388_right"]
            pbar.update(len(images))
            # for image in images:
            #     plt.imshow(image / 2 + 0.5)
            #     plt.savefig("test.png")



================================================
FILE: droid/data_loading/trajectory_sampler.py
================================================
import os

import h5py
import numpy as np

from droid.data_processing.timestep_processing import TimestepProcesser
from droid.trajectory_utils.misc import load_trajectory


def crawler(dirname, filter_func=None):
    subfolders = [f.path for f in os.scandir(dirname) if f.is_dir()]
    traj_files = [f.path for f in os.scandir(dirname) if (f.is_file() and "trajectory.h5" in f.path)]

    if len(traj_files):
        # Only Save Desired Data #
        if filter_func is None:
            use_data = True
        else:
            hdf5_file = h5py.File(traj_files[0], "r")
            use_data = filter_func(hdf5_file.attrs)
            hdf5_file.close()

        if use_data:
            return [dirname]

    all_folderpaths = []
    for child_dirname in subfolders:
        child_paths = crawler(child_dirname, filter_func=filter_func)
        all_folderpaths.extend(child_paths)

    return all_folderpaths


def generate_train_test_split(filter_func=None, remove_failures=True, train_p=0.9):
    # Collect And Split #
    all_folderpaths = collect_data_folderpaths(filter_func=filter_func, remove_failures=remove_failures)

    # Split Into Train / Test #
    num_train_traj = int(train_p * len(all_folderpaths))
    all_ind = np.random.permutation(len(all_folderpaths))
    training_ind, test_ind = all_ind[:num_train_traj], all_ind[num_train_traj:]
    train_folderpaths, test_folderpaths = [], []

    for i in range(len(all_folderpaths)):
        folderpath = all_folderpaths[i]
        if i in training_ind:
            train_folderpaths.append(folderpath)
        if i in test_ind:
            test_folderpaths.append(folderpath)

    return train_folderpaths, test_folderpaths


def collect_data_folderpaths(filter_func=None, remove_failures=True):
    # Prepare Data Folder #
    dir_path = os.path.dirname(os.path.realpath(__file__))
    data_dir = os.path.join(dir_path, "../../data")
    if remove_failures:
        data_dir = os.path.join(data_dir, "success")

    # Collect #
    all_folderpaths = crawler(data_dir, filter_func=filter_func)

    # Return Paths #
    return all_folderpaths


class TrajectorySampler:
    def __init__(
        self,
        all_folderpaths,
        recording_prefix="",
        traj_loading_kwargs={},
        timestep_filtering_kwargs={},
        image_transform_kwargs={},
        camera_kwargs={},
    ):
        self._all_folderpaths = all_folderpaths
        self.recording_prefix = recording_prefix
        self.traj_loading_kwargs = traj_loading_kwargs
        self.timestep_processer = TimestepProcesser(
            **timestep_filtering_kwargs, image_transform_kwargs=image_transform_kwargs
        )
        self.camera_kwargs = camera_kwargs

    def fetch_samples(self, worker_info=None):
        if worker_info is None:
            range_low, range_high = 0, len(self._all_folderpaths)
        else:
            slice_size = len(self._all_folderpaths) // worker_info.num_workers
            range_low = slice_size * worker_info.id
            range_high = slice_size * (worker_info.id + 1)

        traj_ind = np.random.randint(low=range_low, high=range_high)
        folderpath = self._all_folderpaths[traj_ind]

        filepath = os.path.join(folderpath, "trajectory.h5")
        recording_folderpath = os.path.join(folderpath, "recordings", self.recording_prefix)
        if not os.path.exists(recording_folderpath):
            recording_folderpath = None

        traj_samples = load_trajectory(
            filepath,
            recording_folderpath=recording_folderpath,
            camera_kwargs=self.camera_kwargs,
            **self.traj_loading_kwargs,
        )

        processed_traj_samples = [self.timestep_processer.forward(t) for t in traj_samples]

        return processed_traj_samples



================================================
FILE: droid/data_processing/data_transforms.py
================================================
import cv2
from torchvision import transforms as T


class ImageTransformer:
    def __init__(
        self, remove_alpha=False, bgr_to_rgb=False, augment=False, to_tensor=False, image_path="observation/camera/image"
    ):
        self.image_path = image_path.split("/")
        self.apply_transforms = any([remove_alpha, bgr_to_rgb, augment, to_tensor])

        # Build Composed Transform #
        transforms = []

        if remove_alpha:
            new_transform = T.Lambda(lambda data: data[:, :, :3])
            transforms.append(new_transform)

        if bgr_to_rgb:

            def helper(data):
                if data.shape[-1] == 4:
                    return cv2.cvtColor(data, cv2.COLOR_BGRA2RGBA)
                return cv2.cvtColor(data, cv2.COLOR_BGR2RGB)

            new_transform = T.Lambda(lambda data: helper(data))
            transforms.append(new_transform)

        if augment:
            transforms.append(T.ToPILImage())
            transforms.append(T.AugMix())

        if to_tensor:
            transforms.append(T.ToTensor())

        self.composed_transforms = T.Compose(transforms)

    def forward(self, timestep):
        # Skip If Unnecesary #
        if not self.apply_transforms:
            return timestep

        # Isolate Image Data #
        obs = timestep
        for key in self.image_path:
            obs = obs[key]

        # Apply Transforms #
        for cam_type in obs:
            for i in range(len(obs[cam_type])):
                data = self.composed_transforms(obs[cam_type][i])
                obs[cam_type][i] = data



================================================
FILE: droid/data_processing/timestep_processing.py
================================================
from collections import defaultdict
from copy import deepcopy
from itertools import chain

import numpy as np

from droid.camera_utils.info import camera_type_to_string_dict
from droid.data_processing.data_transforms import ImageTransformer


class TimestepProcesser:
    def __init__(
        self,
        ignore_action=False,
        action_space="cartesian_velocity",
        gripper_action_space=None,
        robot_state_keys=["cartesian_position", "gripper_position", "joint_positions", "joint_velocities"],
        camera_extrinsics=["hand_camera", "varied_camera", "fixed_camera"],
        state_dtype=np.float32,
        action_dtype=np.float32,
        image_transform_kwargs={},
    ):
        assert action_space in ["cartesian_position", "joint_position", "cartesian_velocity", "joint_velocity"]

        self.action_space = action_space
        self.gripper_key = "gripper_velocity" if "velocity" in gripper_action_space else "gripper_position"
        self.ignore_action = ignore_action

        self.robot_state_keys = robot_state_keys
        self.camera_extrinsics = camera_extrinsics

        self.state_dtype = state_dtype
        self.action_dtype = action_dtype

        self.image_transformer = ImageTransformer(**image_transform_kwargs)

    def forward(self, timestep):
        # Make Deep Copy #
        timestep = deepcopy(timestep)

        # Get Relevant Camera Info #
        camera_type_dict = {k: camera_type_to_string_dict[v] for k, v in timestep["observation"]["camera_type"].items()}
        sorted_camera_ids = sorted(camera_type_dict.keys())

        ### Get Robot State Info ###
        sorted_state_keys = sorted(self.robot_state_keys)
        full_robot_state = timestep["observation"]["robot_state"]
        robot_state = [np.array(full_robot_state[key]).flatten() for key in sorted_state_keys]
        if len(robot_state):
            robot_state = np.concatenate(robot_state)

        ### Get Extrinsics ###
        calibration_dict = timestep["observation"]["camera_extrinsics"]
        sorted_calibrated_ids = sorted(calibration_dict.keys())
        extrinsics_dict = defaultdict(list)

        for serial_number in sorted_camera_ids:
            cam_type = camera_type_dict[serial_number]
            if cam_type not in self.camera_extrinsics:
                continue

            for full_cam_id in sorted_calibrated_ids:
                if serial_number in full_cam_id:
                    cam2base = calibration_dict[full_cam_id]
                    extrinsics_dict[cam_type].append(cam2base)

        sorted_extrinsics_keys = sorted(extrinsics_dict.keys())
        extrinsics_state = list(chain(*[extrinsics_dict[cam_type] for cam_type in sorted_extrinsics_keys]))
        if len(extrinsics_state):
            extrinsics_state = np.concatenate(extrinsics_state)

        ### Get Intrinsics ###
        cam_intrinsics_obs = timestep["observation"]["camera_intrinsics"]
        sorted_calibrated_ids = sorted(calibration_dict.keys())
        intrinsics_dict = defaultdict(list)

        for serial_number in sorted_camera_ids:
            cam_type = camera_type_dict[serial_number]
            if cam_type not in self.camera_extrinsics:
                continue

            full_cam_ids = sorted(cam_intrinsics_obs.keys())
            for full_cam_id in full_cam_ids:
                if serial_number in full_cam_id:
                    intr = cam_intrinsics_obs[full_cam_id]
                    intrinsics_dict[cam_type].append(intr)

        sorted_intrinsics_keys = sorted(intrinsics_dict.keys())
        intrinsics_state = list([np.array(intrinsics_dict[cam_type]).flatten() for cam_type in sorted_intrinsics_keys])
        if len(intrinsics_state):
            intrinsics_state = np.concatenate(intrinsics_state)

        ### Get High Dimensional State Info ###
        high_dim_state_dict = defaultdict(lambda: defaultdict(list))

        for obs_type in ["image", "depth", "pointcloud"]:
            obs_type_dict = timestep["observation"].get(obs_type, {})
            sorted_obs_ids = sorted(obs_type_dict.keys())

            for serial_number in sorted_camera_ids:
                cam_type = camera_type_dict[serial_number]

                for full_obs_id in sorted_obs_ids:
                    if serial_number in full_obs_id:
                        data = obs_type_dict[full_obs_id]
                        high_dim_state_dict[obs_type][cam_type].append(data)

        ### Finish Observation Portion ###
        low_level_state = np.concatenate([robot_state, extrinsics_state, intrinsics_state], dtype=self.state_dtype)
        processed_timestep = {"observation": {"state": low_level_state, "camera": high_dim_state_dict}}
        self.image_transformer.forward(processed_timestep)

        ### Add Proper Action ###
        if not self.ignore_action:
            arm_action = timestep["action"][self.action_space]
            gripper_action = timestep["action"][self.gripper_key]
            action = np.concatenate([arm_action, [gripper_action]], dtype=self.action_dtype)
            processed_timestep["action"] = action

        # return raw information + meta data
        processed_timestep["extrinsics_dict"] = extrinsics_dict
        processed_timestep["intrinsics_dict"] = intrinsics_dict

        return processed_timestep



================================================
FILE: droid/evaluation/eval_launcher.py
================================================
import json
import os

import numpy as np
import torch

from droid.controllers.oculus_controller import VRPolicy
from droid.evaluation.policy_wrapper import PolicyWrapper
from droid.robot_env import RobotEnv
from droid.user_interface.data_collector import DataCollecter
from droid.user_interface.gui import RobotGUI


def eval_launcher(variant, run_id, exp_id):
    # Get Directory #
    dir_path = os.path.dirname(os.path.realpath(__file__))

    # Prepare Log Directory #
    variant["exp_name"] = os.path.join(variant["exp_name"], "run{0}/id{1}/".format(run_id, exp_id))
    log_dir = os.path.join(dir_path, "../../evaluation_logs", variant["exp_name"])

    # Set Random Seeds #
    torch.manual_seed(variant["seed"])
    np.random.seed(variant["seed"])

    # Set Compute Mode #
    use_gpu = variant.get("use_gpu", False)
    torch.device("cuda:0" if use_gpu else "cpu")

    # Load Model + Variant #
    policy_logdir = os.path.join(dir_path, "../../training_logs", variant["policy_logdir"])
    policy_filepath = os.path.join(policy_logdir, "models", "{0}.pt".format(variant["model_id"]))
    policy = torch.load(policy_filepath)

    variant_filepath = os.path.join(policy_logdir, "variant.json")
    with open(variant_filepath, "r") as jsonFile:
        policy_variant = json.load(jsonFile)

    # Prepare Policy Wrapper #
    data_processing_kwargs = variant.get("data_processing_kwargs", {})
    timestep_filtering_kwargs = data_processing_kwargs.get("timestep_filtering_kwargs", {})
    image_transform_kwargs = data_processing_kwargs.get("image_transform_kwargs", {})

    policy_data_processing_kwargs = policy_variant.get("data_processing_kwargs", {})
    policy_timestep_filtering_kwargs = policy_data_processing_kwargs.get("timestep_filtering_kwargs", {})
    policy_image_transform_kwargs = policy_data_processing_kwargs.get("image_transform_kwargs", {})

    policy_timestep_filtering_kwargs.update(timestep_filtering_kwargs)
    policy_image_transform_kwargs.update(image_transform_kwargs)

    wrapped_policy = PolicyWrapper(
        policy=policy,
        timestep_filtering_kwargs=policy_timestep_filtering_kwargs,
        image_transform_kwargs=policy_image_transform_kwargs,
        eval_mode=True,
    )

    # Prepare Environment #
    policy_action_space = policy_timestep_filtering_kwargs["action_space"]

    camera_kwargs = variant.get("camera_kwargs", {})
    policy_camera_kwargs = policy_variant.get("camera_kwargs", {})
    policy_camera_kwargs.update(camera_kwargs)

    env = RobotEnv(action_space=policy_action_space, camera_kwargs=policy_camera_kwargs)
    controller = VRPolicy()

    # Launch GUI #
    data_collector = DataCollecter(
        env=env,
        controller=controller,
        policy=wrapped_policy,
        save_traj_dir=log_dir,
        save_data=variant.get("save_data", True),
    )
    RobotGUI(robot=data_collector)



================================================
FILE: droid/evaluation/eval_launcher_robomimic.py
================================================
import json
import os
import numpy as np
import torch
from collections import OrderedDict
from copy import deepcopy

from droid.controllers.oculus_controller import VRPolicy
from droid.evaluation.policy_wrapper import PolicyWrapperRobomimic
from droid.robot_env import RobotEnv
from droid.user_interface.data_collector import DataCollecter
from droid.user_interface.gui import RobotGUI

import robomimic.utils.file_utils as FileUtils
import robomimic.utils.torch_utils as TorchUtils
import robomimic.utils.tensor_utils as TensorUtils

import cv2

def eval_launcher(variant, run_id, exp_id):
    # Get Directory #
    dir_path = os.path.dirname(os.path.realpath(__file__))

    # Prepare Log Directory #
    variant["exp_name"] = os.path.join(variant["exp_name"], "run{0}/id{1}/".format(run_id, exp_id))
    log_dir = os.path.join(dir_path, "../../evaluation_logs", variant["exp_name"])

    # Set Random Seeds #
    torch.manual_seed(variant["seed"])
    np.random.seed(variant["seed"])

    # Set Compute Mode #
    use_gpu = variant.get("use_gpu", False)
    torch.device("cuda:0" if use_gpu else "cpu")

    ckpt_path = variant["ckpt_path"]

    device = TorchUtils.get_torch_device(try_to_use_cuda=True)
    ckpt_dict = FileUtils.maybe_dict_from_checkpoint(ckpt_path=ckpt_path)
    config = json.loads(ckpt_dict["config"])

    ### infer image size ###
    for obs_key in ckpt_dict["shape_metadata"]["all_shapes"].keys():
        if 'camera/image' in obs_key:
            imsize = max(ckpt_dict["shape_metadata"]["all_shapes"][obs_key])
            break

    ckpt_dict["config"] = json.dumps(config)
    policy, _ = FileUtils.policy_from_checkpoint(ckpt_dict=ckpt_dict, device=device, verbose=True)
    policy.goal_mode = config["train"]["goal_mode"]
    policy.eval_mode = True

    # determine the action space (relative or absolute)
    action_keys = config["train"]["action_keys"]
    if "action/rel_pos" in action_keys:
        action_space = "cartesian_velocity"
        for k in action_keys:
            assert not k.startswith("action/abs_")
    elif "action/abs_pos" in action_keys:
        action_space = "cartesian_position"
        for k in action_keys:
            assert not k.startswith("action/rel_")
    else:
        raise ValueError

    # determine the action space for the gripper
    if "action/gripper_velocity" in action_keys:
        gripper_action_space = "velocity"
    elif "action/gripper_position" in action_keys:
        gripper_action_space = "position"
    else:
        raise ValueError

    # determine the action space (relative or absolute)
    action_keys = config["train"]["action_keys"]
    if "action/rel_pos" in action_keys:
        action_space = "cartesian_velocity"
        for k in action_keys:
            assert not k.startswith("action/abs_")
    elif "action/abs_pos" in action_keys:
        action_space = "cartesian_position"
        for k in action_keys:
            assert not k.startswith("action/rel_")
    else:
        raise ValueError

    # determine the action space for the gripper
    if "action/gripper_velocity" in action_keys:
        gripper_action_space = "velocity"
    elif "action/gripper_position" in action_keys:
        gripper_action_space = "position"
    else:
        raise ValueError

    # Prepare Policy Wrapper #
    data_processing_kwargs = dict(
        timestep_filtering_kwargs=dict(
            action_space=action_space,
            gripper_action_space=gripper_action_space,
            robot_state_keys=["cartesian_position", "gripper_position", "joint_positions"],
            # camera_extrinsics=[],
        ),
        image_transform_kwargs=dict(
            remove_alpha=True,
            bgr_to_rgb=True,
            to_tensor=True,
            augment=False,
        ),
    )
    timestep_filtering_kwargs = data_processing_kwargs.get("timestep_filtering_kwargs", {})
    image_transform_kwargs = data_processing_kwargs.get("image_transform_kwargs", {})

    policy_data_processing_kwargs = {}
    policy_timestep_filtering_kwargs = policy_data_processing_kwargs.get("timestep_filtering_kwargs", {})
    policy_image_transform_kwargs = policy_data_processing_kwargs.get("image_transform_kwargs", {})

    policy_timestep_filtering_kwargs.update(timestep_filtering_kwargs)
    policy_image_transform_kwargs.update(image_transform_kwargs)

    fs = config["train"]["frame_stack"]

    wrapped_policy = PolicyWrapperRobomimic(
        policy=policy,
        timestep_filtering_kwargs=policy_timestep_filtering_kwargs,
        image_transform_kwargs=policy_image_transform_kwargs,
        frame_stack=fs,
        eval_mode=True,
    )

    camera_kwargs = dict(
        hand_camera=dict(image=True, concatenate_images=False, resolution=(imsize, imsize), resize_func="cv2"),
        varied_camera=dict(image=True, concatenate_images=False, resolution=(imsize, imsize), resize_func="cv2"),
    )
    
    policy_camera_kwargs = {}
    policy_camera_kwargs.update(camera_kwargs)

    env = RobotEnv(
        action_space=policy_timestep_filtering_kwargs["action_space"],
        gripper_action_space=policy_timestep_filtering_kwargs["gripper_action_space"],
        camera_kwargs=policy_camera_kwargs
    )
    controller = VRPolicy()

    # Launch GUI #
    data_collector = DataCollecter(
        env=env,
        controller=controller,
        policy=wrapped_policy,
        save_traj_dir=log_dir,
        save_data=variant.get("save_data", True),
    )
    RobotGUI(robot=data_collector)


def get_goal_im(variant, run_id, exp_id):
    # Get Directory #
    dir_path = os.path.dirname(os.path.realpath(__file__))

    # Prepare Log Directory #
    variant["exp_name"] = os.path.join(variant["exp_name"], "run{0}/id{1}/".format(run_id, exp_id))
    log_dir = os.path.join(dir_path, "../../evaluation_logs", variant["exp_name"])

    # Set Random Seeds #
    torch.manual_seed(variant["seed"])
    np.random.seed(variant["seed"])

    # Set Compute Mode #
    use_gpu = variant.get("use_gpu", False)
    torch.device("cuda:0" if use_gpu else "cpu")

    ckpt_path = variant["ckpt_path"]

    device = TorchUtils.get_torch_device(try_to_use_cuda=True)
    ckpt_dict = FileUtils.maybe_dict_from_checkpoint(ckpt_path=ckpt_path)
    config = json.loads(ckpt_dict["config"])

    ### infer image size ###
    imsize = max(ckpt_dict["shape_metadata"]["all_shapes"]["camera/image/hand_camera_left_image"])

    ckpt_dict["config"] = json.dumps(config)
    policy, _ = FileUtils.policy_from_checkpoint(ckpt_dict=ckpt_dict, device=device, verbose=True)

    # determine the action space (relative or absolute)
    action_keys = config["train"]["action_keys"]
    if "action/rel_pos" in action_keys:
        action_space = "cartesian_velocity"
        for k in action_keys:
            assert not k.startswith("action/abs_")
    elif "action/abs_pos" in action_keys:
        action_space = "cartesian_position"
        for k in action_keys:
            assert not k.startswith("action/rel_")
    else:
        raise ValueError

    # determine the action space for the gripper
    if "action/gripper_velocity" in action_keys:
        gripper_action_space = "velocity"
    elif "action/gripper_position" in action_keys:
        gripper_action_space = "position"
    else:
        raise ValueError

    # Prepare Policy Wrapper #
    data_processing_kwargs = dict(
        timestep_filtering_kwargs=dict(
            action_space=action_space,
            gripper_action_space=gripper_action_space,
            robot_state_keys=["cartesian_position", "gripper_position", "joint_positions"],
            # camera_extrinsics=[],
        ),
        image_transform_kwargs=dict(
            remove_alpha=True,
            bgr_to_rgb=True,
            to_tensor=True,
            augment=False,
        ),
    )
    timestep_filtering_kwargs = data_processing_kwargs.get("timestep_filtering_kwargs", {})
    image_transform_kwargs = data_processing_kwargs.get("image_transform_kwargs", {})

    policy_data_processing_kwargs = {}
    policy_timestep_filtering_kwargs = policy_data_processing_kwargs.get("timestep_filtering_kwargs", {})
    policy_image_transform_kwargs = policy_data_processing_kwargs.get("image_transform_kwargs", {})

    policy_timestep_filtering_kwargs.update(timestep_filtering_kwargs)
    policy_image_transform_kwargs.update(image_transform_kwargs)

    wrapped_policy = PolicyWrapperRobomimic(
        policy=policy,
        timestep_filtering_kwargs=policy_timestep_filtering_kwargs,
        image_transform_kwargs=policy_image_transform_kwargs,
        frame_stack=config["train"]["frame_stack"],
        eval_mode=True,
    )

    camera_kwargs = dict(
        hand_camera=dict(image=True, concatenate_images=False, resolution=(imsize, imsize), resize_func="cv2"),
        varied_camera=dict(image=True, concatenate_images=False, resolution=(imsize, imsize), resize_func="cv2"),
    )
    
    policy_camera_kwargs = {}
    policy_camera_kwargs.update(camera_kwargs)

    env = RobotEnv(
        action_space=policy_timestep_filtering_kwargs["action_space"],
        gripper_action_space=policy_timestep_filtering_kwargs["gripper_action_space"],
        camera_kwargs=policy_camera_kwargs,
        do_reset=False
    )

    ims = env.read_cameras()[0]["image"]
    if not os.path.exists('eval_params'):
        os.makedirs('eval_params')
    for k in ims.keys():
        image = ims[k]
        cv2.imwrite(f'eval_params/{k}.png', image[:, :, :3])
    return ims



================================================
FILE: droid/evaluation/policy_wrapper.py
================================================
import numpy as np
import torch
from collections import deque

from droid.data_processing.timestep_processing import TimestepProcesser
import robomimic.utils.torch_utils as TorchUtils
import robomimic.utils.tensor_utils as TensorUtils


def converter_helper(data, batchify=True):
    if torch.is_tensor(data):
        pass
    elif isinstance(data, np.ndarray):
        data = torch.from_numpy(data)
    else:
        raise ValueError

    if batchify:
        data = data.unsqueeze(0)
    return data


def np_dict_to_torch_dict(np_dict, batchify=True):
    torch_dict = {}

    for key in np_dict:
        curr_data = np_dict[key]
        if isinstance(curr_data, dict):
            torch_dict[key] = np_dict_to_torch_dict(curr_data)
        elif isinstance(curr_data, np.ndarray) or torch.is_tensor(curr_data):
            torch_dict[key] = converter_helper(curr_data, batchify=batchify)
        elif isinstance(curr_data, list):
            torch_dict[key] = [converter_helper(d, batchify=batchify) for d in curr_data]
        else:
            raise ValueError

    return torch_dict


class PolicyWrapper:
    def __init__(self, policy, timestep_filtering_kwargs, image_transform_kwargs, eval_mode=True):
        self.policy = policy

        if eval_mode:
            self.policy.eval()
        else:
            self.policy.train()

        self.timestep_processor = TimestepProcesser(
            ignore_action=True, **timestep_filtering_kwargs, image_transform_kwargs=image_transform_kwargs
        )

    def forward(self, observation):
        timestep = {"observation": observation}
        processed_timestep = self.timestep_processor.forward(timestep)
        torch_timestep = np_dict_to_torch_dict(processed_timestep)
        action = self.policy(torch_timestep)[0]
        np_action = action.detach().numpy()

        # a_star = np.cumsum(processed_timestep['observation']['state']) / 7
        # print('Policy Action: ', np_action)
        # print('Expert Action: ', a_star)
        # print('Error: ', np.abs(a_star - np_action).mean())

        # import pdb; pdb.set_trace()
        return np_action


class PolicyWrapperRobomimic:
    def __init__(self, policy, timestep_filtering_kwargs, image_transform_kwargs, frame_stack, eval_mode=True):
        self.policy = policy

        assert eval_mode is True

        self.fs_wrapper = FrameStackWrapper(num_frames=frame_stack)
        self.fs_wrapper.reset()
        self.policy.start_episode()

        self.timestep_processor = TimestepProcesser(
            ignore_action=True, **timestep_filtering_kwargs, image_transform_kwargs=image_transform_kwargs
        )

    def convert_raw_extrinsics_to_Twc(self, raw_data):
        """
        helper function that convert raw extrinsics (6d pose) to transformation matrix (Twc)
        """
        raw_data = torch.from_numpy(np.array(raw_data))
        pos = raw_data[0:3]
        rot_mat = TorchUtils.euler_angles_to_matrix(raw_data[3:6], convention="XYZ")
        extrinsics = np.zeros((4, 4))
        extrinsics[:3,:3] = TensorUtils.to_numpy(rot_mat)
        extrinsics[:3,3] = TensorUtils.to_numpy(pos)
        extrinsics[3,3] = 1.0
        # invert the matrix to represent standard definition of extrinsics: from world to cam
        extrinsics = np.linalg.inv(extrinsics)
        return extrinsics

    def forward(self, observation):
        timestep = {"observation": observation}
        processed_timestep = self.timestep_processor.forward(timestep)

        extrinsics_dict = processed_timestep["extrinsics_dict"]
        intrinsics_dict = processed_timestep["intrinsics_dict"]
        # import pdb; pdb.set_trace()

        obs = {
            "robot_state/cartesian_position": observation["robot_state"]["cartesian_position"],
            "robot_state/gripper_position": [observation["robot_state"]["gripper_position"]], # wrap as array, raw data is single float
            "camera/image/hand_camera_left_image": processed_timestep["observation"]["camera"]["image"]["hand_camera"][0],
            "camera/image/hand_camera_right_image": processed_timestep["observation"]["camera"]["image"]["hand_camera"][1],
            "camera/image/varied_camera_1_left_image": processed_timestep["observation"]["camera"]["image"]["varied_camera"][0],
            "camera/image/varied_camera_1_right_image": processed_timestep["observation"]["camera"]["image"]["varied_camera"][1],
            "camera/image/varied_camera_2_left_image": processed_timestep["observation"]["camera"]["image"]["varied_camera"][2],
            "camera/image/varied_camera_2_right_image": processed_timestep["observation"]["camera"]["image"]["varied_camera"][3],

            "camera/extrinsics/hand_camera_left": self.convert_raw_extrinsics_to_Twc(extrinsics_dict["hand_camera"][0]),
            "camera/extrinsics/hand_camera_right": self.convert_raw_extrinsics_to_Twc(extrinsics_dict["hand_camera"][2]),
            "camera/extrinsics/varied_camera_1_left": self.convert_raw_extrinsics_to_Twc(extrinsics_dict["varied_camera"][0]),
            "camera/extrinsics/varied_camera_1_right": self.convert_raw_extrinsics_to_Twc(extrinsics_dict["varied_camera"][1]),
            "camera/extrinsics/varied_camera_2_left": self.convert_raw_extrinsics_to_Twc(extrinsics_dict["varied_camera"][2]),
            "camera/extrinsics/varied_camera_2_right": self.convert_raw_extrinsics_to_Twc(extrinsics_dict["varied_camera"][3]),

            "camera/intrinsics/hand_camera_left": intrinsics_dict["hand_camera"][0],
            "camera/intrinsics/hand_camera_right": intrinsics_dict["hand_camera"][1],
            "camera/intrinsics/varied_camera_1_left": intrinsics_dict["varied_camera"][0],
            "camera/intrinsics/varied_camera_1_right": intrinsics_dict["varied_camera"][1],
            "camera/intrinsics/varied_camera_2_left": intrinsics_dict["varied_camera"][2],
            "camera/intrinsics/varied_camera_2_right": intrinsics_dict["varied_camera"][3],
        }

        # set item of obs as np.array
        for k in obs:
            obs[k] = np.array(obs[k])
        
        self.fs_wrapper.add_obs(obs)
        obs_history = self.fs_wrapper.get_obs_history()
        action = self.policy(obs_history)

        return action

    def reset(self):
        self.fs_wrapper.reset()
        self.policy.start_episode()
    

class FrameStackWrapper:
    """
    Wrapper for frame stacking observations during rollouts. The agent
    receives a sequence of past observations instead of a single observation
    when it calls @env.reset, @env.reset_to, or @env.step in the rollout loop.
    """
    def __init__(self, num_frames):
        """
        Args:
            env (EnvBase instance): The environment to wrap.
            num_frames (int): number of past observations (including current observation)
                to stack together. Must be greater than 1 (otherwise this wrapper would
                be a no-op).
        """
        self.num_frames = num_frames

        ### TODO: add action padding option + adding action to obs to include action history in obs ###

        # keep track of last @num_frames observations for each obs key
        self.obs_history = None

    def _set_initial_obs_history(self, init_obs):
        """
        Helper method to get observation history from the initial observation, by
        repeating it.

        Returns:
            obs_history (dict): a deque for each observation key, with an extra
                leading dimension of 1 for each key (for easy concatenation later)
        """
        self.obs_history = {}
        for k in init_obs:
            self.obs_history[k] = deque(
                [init_obs[k][None] for _ in range(self.num_frames)], 
                maxlen=self.num_frames,
            )

    def reset(self):
        self.obs_history = None

    def get_obs_history(self):
        """
        Helper method to convert internal variable @self.obs_history to a 
        stacked observation where each key is a numpy array with leading dimension
        @self.num_frames.
        """
        # concatenate all frames per key so we return a numpy array per key
        if self.num_frames == 1:
            return { k : np.concatenate(self.obs_history[k], axis=0)[0] for k in self.obs_history }
        else:
            return { k : np.concatenate(self.obs_history[k], axis=0) for k in self.obs_history }

    def add_obs(self, obs):
        if self.obs_history is None:
            self._set_initial_obs_history(obs)

        # update frame history
        for k in obs:
            # make sure to have leading dim of 1 for easy concatenation
            self.obs_history[k].append(obs[k][None])



================================================
FILE: droid/evaluation/rt1_wrapper.py
================================================
from pathlib import Path
from PIL import Image
import tensorflow as tf
import numpy as np
from tf_agents.policies import py_tf_eager_policy
import tf_agents
from tf_agents.trajectories import time_step as ts
import tensorflow_hub as hub

from droid.user_interface.eval_gui import GoalCondPolicy, DEFAULT_LANG_TEXT


def resize(image):
    image = tf.image.resize_with_pad(image, target_width=320, target_height=256)
    image = tf.cast(image, tf.uint8)
    return image


class RT1Policy(GoalCondPolicy):
    def __init__(self, checkpoint_path, goal_images=None, language_instruction=None, camera_obs_keys=[]):
        """goal_images is a tuple of two goal images."""
        self._policy = py_tf_eager_policy.SavedModelPyTFEagerPolicy(
            model_path=checkpoint_path, load_specs_from_pbtxt=True, use_tf_function=True
        )
        self.obs = self._run_dummy_inference()
        self._goal_images = goal_images
        self._policy_state = self._policy.get_initial_state(batch_size=1)
        self._camera_obs_keys = camera_obs_keys
        if language_instruction is not None:
            self._language_embedding = self.compute_embedding(language_instruction)
        else:
            self._language_embedding = None

    def _run_dummy_inference(self):
        observation = tf_agents.specs.zero_spec_nest(tf_agents.specs.from_spec(self._policy.time_step_spec.observation))
        tfa_time_step = ts.transition(observation, reward=np.zeros((), dtype=np.float32))
        policy_state = self._policy.get_initial_state(batch_size=1)
        action = self._policy.action(tfa_time_step, policy_state)
        return observation

    def _normalize_task_name(self, task_name):

        replaced = (
            task_name.replace("_", " ")
            .replace("1f", " ")
            .replace("4f", " ")
            .replace("-", " ")
            .replace("50", " ")
            .replace("55", " ")
            .replace("56", " ")
        )
        return replaced.lstrip(" ").rstrip(" ")

    def compute_embedding(self, task_name):
        print("Computing embedding ...")
        # Load language model and embed the task string
        embed = hub.load("https://tfhub.dev/google/universal-sentence-encoder-large/5")
        print("Finished loading language model.")
        return embed([self._normalize_task_name(task_name)])[0]

    def forward(self, observation):
        # construct observation
        if not self._goal_images:
            # throw exception saying no goal images were provided
            raise Exception("No goal images were provided")

        for i, key in enumerate(self._camera_obs_keys):
            if i == 0:
                self.obs["image"] = resize(tf.convert_to_tensor(observation["image"][key][:, :, :3].copy()[..., ::-1]))
                self.obs["goal_image"] = self._goal_images[0]
            elif i >= 1:
                self.obs[f"image{i}"] = resize(
                    tf.convert_to_tensor(observation["image"][key][:, :, :3].copy()[..., ::-1])
                )
                self.obs[f"goal_image{i}"] = self._goal_images[1]

        if self._language_embedding is not None:
            self.obs["natural_language_embedding"] = self._language_embedding

        tfa_time_step = ts.transition(self.obs, reward=np.zeros((), dtype=np.float32))

        policy_step = self._policy.action(tfa_time_step, self._policy_state)
        self._policy_state = policy_step.state
        action = np.concatenate(
            [
                policy_step.action["world_vector"],
                policy_step.action["rotation_delta"],
                policy_step.action["gripper_closedness_action"],
            ]
        )
        print(f"returning action: {action}")
        return action

    def load_goal_imgs(self, img_dict):
        goal_images = []
        for key in self._camera_obs_keys:
            goal_images.append(resize(tf.convert_to_tensor(img_dict[key])))
        self._goal_images = goal_images

    def load_lang(self, text):
        if text != DEFAULT_LANG_TEXT:
            self._language_embedding = self.compute_embedding(text)



================================================
FILE: droid/franka/__init__.py
================================================
[Empty file]


================================================
FILE: droid/franka/launch_gripper.sh
================================================
source ~/anaconda3/etc/profile.d/conda.sh
conda activate polymetis-local
pkill -9 gripper
chmod a+rw /dev/ttyUSB0
launch_gripper.py gripper=robotiq_2f gripper.comport=/dev/ttyUSB0



================================================
FILE: droid/franka/launch_robot.sh
================================================
source ~/anaconda3/etc/profile.d/conda.sh
conda activate polymetis-local
pkill -9 run_server
pkill -9 franka_panda_cl
launch_robot.py robot_client=franka_hardware



================================================
FILE: droid/franka/robot.py
================================================
# ROBOT SPECIFIC IMPORTS
import os
import time

import grpc
import numpy as np
import torch
from polymetis import GripperInterface, RobotInterface

from droid.misc.parameters import sudo_password
from droid.misc.subprocess_utils import run_terminal_command, run_threaded_command

# UTILITY SPECIFIC IMPORTS
from droid.misc.transformations import add_poses, euler_to_quat, pose_diff, quat_to_euler
from droid.robot_ik.robot_ik_solver import RobotIKSolver


class FrankaRobot:
    def launch_controller(self):
        try:
            self.kill_controller()
        except:
            pass

        dir_path = os.path.dirname(os.path.realpath(__file__))
        self._robot_process = run_terminal_command(
            "echo " + sudo_password + " | sudo -S " + "bash " + dir_path + "/launch_robot.sh"
        )
        self._gripper_process = run_terminal_command(
            "echo " + sudo_password + " | sudo -S " + "bash " + dir_path + "/launch_gripper.sh"
        )
        self._server_launched = True
        time.sleep(5)

    def launch_robot(self):
        self._robot = RobotInterface(ip_address="localhost")
        self._gripper = GripperInterface(ip_address="localhost")
        self._max_gripper_width = self._gripper.metadata.max_width
        self._ik_solver = RobotIKSolver()
        self._controller_not_loaded = False

    def kill_controller(self):
        self._robot_process.kill()
        self._gripper_process.kill()

    def update_command(self, command, action_space="cartesian_velocity", gripper_action_space=None, blocking=False):
        action_dict = self.create_action_dict(command, action_space=action_space, gripper_action_space=gripper_action_space)

        self.update_joints(action_dict["joint_position"], velocity=False, blocking=blocking)
        self.update_gripper(action_dict["gripper_position"], velocity=False, blocking=blocking)

        return action_dict

    def update_pose(self, command, velocity=False, blocking=False):
        if blocking:
            if velocity:
                curr_pose = self.get_ee_pose()
                cartesian_delta = self._ik_solver.cartesian_velocity_to_delta(command)
                command = add_poses(cartesian_delta, curr_pose)

            pos = torch.Tensor(command[:3])
            quat = torch.Tensor(euler_to_quat(command[3:6]))
            curr_joints = self._robot.get_joint_positions()
            desired_joints = self._robot.solve_inverse_kinematics(pos, quat, curr_joints)
            self.update_joints(desired_joints, velocity=False, blocking=True)
        else:
            if not velocity:
                curr_pose = self.get_ee_pose()
                cartesian_delta = pose_diff(command, curr_pose)
                command = self._ik_solver.cartesian_delta_to_velocity(cartesian_delta)

            robot_state = self.get_robot_state()[0]
            joint_velocity = self._ik_solver.cartesian_velocity_to_joint_velocity(command, robot_state=robot_state)

            self.update_joints(joint_velocity, velocity=True, blocking=False)

    def update_joints(self, command, velocity=False, blocking=False, cartesian_noise=None):
        if cartesian_noise is not None:
            command = self.add_noise_to_joints(command, cartesian_noise)
        command = torch.Tensor(command)

        if velocity:
            joint_delta = self._ik_solver.joint_velocity_to_delta(command)
            command = joint_delta + self._robot.get_joint_positions()

        def helper_non_blocking():
            if not self._robot.is_running_policy():
                self._controller_not_loaded = True
                self._robot.start_cartesian_impedance()
                timeout = time.time() + 5
                while not self._robot.is_running_policy():
                    time.sleep(0.01)
                    if time.time() > timeout:
                        self._robot.start_cartesian_impedance()
                        timeout = time.time() + 5

                self._controller_not_loaded = False
            try:
                self._robot.update_desired_joint_positions(command)
            except grpc.RpcError:
                pass

        if blocking:
            if self._robot.is_running_policy():
                self._robot.terminate_current_policy()
            try:
                time_to_go = self.adaptive_time_to_go(command)
                self._robot.move_to_joint_positions(command, time_to_go=time_to_go)
            except grpc.RpcError:
                pass

            self._robot.start_cartesian_impedance()
        else:
            if not self._controller_not_loaded:
                run_threaded_command(helper_non_blocking)

    def update_gripper(self, command, velocity=True, blocking=False):
        if velocity:
            gripper_delta = self._ik_solver.gripper_velocity_to_delta(command)
            command = gripper_delta + self.get_gripper_position()

        command = float(np.clip(command, 0, 1))
        self._gripper.goto(width=self._max_gripper_width * (1 - command), speed=0.05, force=0.1, blocking=blocking)

    def add_noise_to_joints(self, original_joints, cartesian_noise):
        original_joints = torch.Tensor(original_joints)

        pos, quat = self._robot.robot_model.forward_kinematics(original_joints)
        curr_pose = pos.tolist() + quat_to_euler(quat).tolist()
        new_pose = add_poses(cartesian_noise, curr_pose)

        new_pos = torch.Tensor(new_pose[:3])
        new_quat = torch.Tensor(euler_to_quat(new_pose[3:]))

        noisy_joints, success = self._robot.solve_inverse_kinematics(new_pos, new_quat, original_joints)

        if success:
            desired_joints = noisy_joints
        else:
            desired_joints = original_joints

        return desired_joints.tolist()

    def get_joint_positions(self):
        return self._robot.get_joint_positions().tolist()

    def get_joint_velocities(self):
        return self._robot.get_joint_velocities().tolist()

    def get_gripper_position(self):
        return 1 - (self._gripper.get_state().width / self._max_gripper_width)

    def get_ee_pose(self):
        pos, quat = self._robot.get_ee_pose()
        angle = quat_to_euler(quat.numpy())
        return np.concatenate([pos, angle]).tolist()

    def get_robot_state(self):
        robot_state = self._robot.get_robot_state()
        gripper_position = self.get_gripper_position()
        pos, quat = self._robot.robot_model.forward_kinematics(torch.Tensor(robot_state.joint_positions))
        cartesian_position = pos.tolist() + quat_to_euler(quat.numpy()).tolist()

        state_dict = {
            "cartesian_position": cartesian_position,
            "gripper_position": gripper_position,
            "joint_positions": list(robot_state.joint_positions),
            "joint_velocities": list(robot_state.joint_velocities),
            "joint_torques_computed": list(robot_state.joint_torques_computed),
            "prev_joint_torques_computed": list(robot_state.prev_joint_torques_computed),
            "prev_joint_torques_computed_safened": list(robot_state.prev_joint_torques_computed_safened),
            "motor_torques_measured": list(robot_state.motor_torques_measured),
            "prev_controller_latency_ms": robot_state.prev_controller_latency_ms,
            "prev_command_successful": robot_state.prev_command_successful,
        }

        timestamp_dict = {
            "robot_timestamp_seconds": robot_state.timestamp.seconds,
            "robot_timestamp_nanos": robot_state.timestamp.nanos,
        }

        return state_dict, timestamp_dict

    def adaptive_time_to_go(self, desired_joint_position, t_min=0, t_max=4):
        curr_joint_position = self._robot.get_joint_positions()
        displacement = desired_joint_position - curr_joint_position
        time_to_go = self._robot._adaptive_time_to_go(displacement)
        clamped_time_to_go = min(t_max, max(time_to_go, t_min))
        return clamped_time_to_go

    def create_action_dict(self, action, action_space, gripper_action_space=None, robot_state=None):
        assert action_space in ["cartesian_position", "joint_position", "cartesian_velocity", "joint_velocity"]
        if robot_state is None:
            robot_state = self.get_robot_state()[0]
        action_dict = {"robot_state": robot_state}
        velocity = "velocity" in action_space

        if gripper_action_space is None:
            gripper_action_space = "velocity" if velocity else "position"
        assert gripper_action_space in ["velocity", "position"]
            

        if gripper_action_space == "velocity":
            action_dict["gripper_velocity"] = action[-1]
            gripper_delta = self._ik_solver.gripper_velocity_to_delta(action[-1])
            gripper_position = robot_state["gripper_position"] + gripper_delta
            action_dict["gripper_position"] = float(np.clip(gripper_position, 0, 1))
        else:
            action_dict["gripper_position"] = float(np.clip(action[-1], 0, 1))
            gripper_delta = action_dict["gripper_position"] - robot_state["gripper_position"]
            gripper_velocity = self._ik_solver.gripper_delta_to_velocity(gripper_delta)
            action_dict["gripper_delta"] = gripper_velocity

        if "cartesian" in action_space:
            if velocity:
                action_dict["cartesian_velocity"] = action[:-1]
                cartesian_delta = self._ik_solver.cartesian_velocity_to_delta(action[:-1])
                action_dict["cartesian_position"] = add_poses(
                    cartesian_delta, robot_state["cartesian_position"]
                ).tolist()
            else:
                action_dict["cartesian_position"] = action[:-1]
                cartesian_delta = pose_diff(action[:-1], robot_state["cartesian_position"])
                cartesian_velocity = self._ik_solver.cartesian_delta_to_velocity(cartesian_delta)
                action_dict["cartesian_velocity"] = cartesian_velocity.tolist()

            action_dict["joint_velocity"] = self._ik_solver.cartesian_velocity_to_joint_velocity(
                action_dict["cartesian_velocity"], robot_state=robot_state
            ).tolist()
            joint_delta = self._ik_solver.joint_velocity_to_delta(action_dict["joint_velocity"])
            action_dict["joint_position"] = (joint_delta + np.array(robot_state["joint_positions"])).tolist()

        if "joint" in action_space:
            # NOTE: Joint to Cartesian has undefined dynamics due to IK
            if velocity:
                action_dict["joint_velocity"] = action[:-1]
                joint_delta = self._ik_solver.joint_velocity_to_delta(action[:-1])
                action_dict["joint_position"] = (joint_delta + np.array(robot_state["joint_positions"])).tolist()
            else:
                action_dict["joint_position"] = action[:-1]
                joint_delta = np.array(action[:-1]) - np.array(robot_state["joint_positions"])
                joint_velocity = self._ik_solver.joint_delta_to_velocity(joint_delta)
                action_dict["joint_velocity"] = joint_velocity.tolist()

        return action_dict



================================================
FILE: droid/misc/parameters.py
================================================
import os
from cv2 import aruco

# Robot Params #
nuc_ip = ""
robot_ip = ""
laptop_ip = ""
sudo_password = ""
robot_type = ""  # 'panda' or 'fr3'
robot_serial_number = ""

# Camera ID's #
hand_camera_id = ""
varied_camera_1_id = ""
varied_camera_2_id = ""

# Charuco Board Params #
CHARUCOBOARD_ROWCOUNT = 9
CHARUCOBOARD_COLCOUNT = 14
CHARUCOBOARD_CHECKER_SIZE = 0.020
CHARUCOBOARD_MARKER_SIZE = 0.016
ARUCO_DICT = aruco.Dictionary_get(aruco.DICT_5X5_100)

# Ubuntu Pro Token (RT PATCH) #
ubuntu_pro_token = ""

# Code Version [DONT CHANGE] #
droid_version = "1.3"




================================================
FILE: droid/misc/pointcloud_utils.py
================================================
import cv2
import numpy as np
import open3d as o3d
from scipy.spatial.transform import Rotation as R

# VOXEL_SIZE = 0.02
VOXEL_SIZE = 0.001
MAX_DISTANCE_COARSE = VOXEL_SIZE * 15
MAX_DISTANCE_FINE = VOXEL_SIZE * 1.5


def numpy_to_o3d(numpy_pcd):
    xyz = numpy_pcd[:, :, :3].reshape(-1, 3)

    rgba_size = (*numpy_pcd.shape[:2], 4)
    rgba_float = np.ascontiguousarray(numpy_pcd[:, :, 3])
    rgba = rgba_float.view(np.uint8).reshape(rgba_size)
    rgb = rgba[:, :, :3].reshape(-1, 3)

    o3d_pcd = o3d.geometry.PointCloud()
    o3d_pcd.points = o3d.utility.Vector3dVector(xyz)
    o3d_pcd.colors = o3d.utility.Vector3dVector(rgb)
    o3d_pcd = o3d_pcd.voxel_down_sample(voxel_size=VOXEL_SIZE)
    return o3d_pcd


def o3d_to_numpy(o3d_pcd):
    points = np.asarray(o3d_pcd.points)
    colors = np.asarray(o3d_pcd.colors)
    numpy_pcd = np.concatenate([points, colors], axis=1)
    return numpy_pcd


def rgbd_to_pcd(color, depth, camera_matrix):
    color = cv2.cvtColor(color, cv2.COLOR_BGRA2RGB)
    o3d_color = o3d.geometry.Image(color)
    o3d_depth = o3d.geometry.Image(depth)
    rgbd = o3d.geometry.RGBDImage.create_from_color_and_depth(o3d_color, o3d_depth, convert_rgb_to_intensity=False)
    intrinsics = o3d.camera.PinholeCameraIntrinsic(
        width=color.shape[1],
        height=color.shape[0],
        fx=camera_matrix[0, 0],
        cx=camera_matrix[0, 2],
        fy=camera_matrix[1, 1],
        cy=camera_matrix[1, 2],
    )

    o3d_pcd = o3d.geometry.PointCloud.create_from_rgbd_image(rgbd, intrinsics)
    # o3d_pcd = o3d.geometry.PointCloud.create_from_depth_image(o3d_depth, intrinsics)
    o3d_pcd = o3d_pcd.voxel_down_sample(voxel_size=VOXEL_SIZE)
    o3d_pcd.estimate_normals()

    return o3d_pcd


def visualize_pointcloud(pcd):
    if type(pcd) == np.ndarray:
        pcd = numpy_to_o3d(pcd)
    o3d.visualization.draw_geometries([pcd])


def transform_pointcloud(pcd, cam2base):
    rotation = R.from_euler("xyz", cam2base[3:]).as_matrix()
    translation = cam2base[:3]

    pcd.rotate(rotation)
    pcd.translate(translation)


def pairwise_registration(source, target):
    print("Apply point-to-plane ICP")
    icp_coarse = o3d.pipelines.registration.registration_icp(
        source,
        target,
        MAX_DISTANCE_COARSE,
        np.identity(4),
        o3d.pipelines.registration.TransformationEstimationPointToPlane(),
    )
    icp_fine = o3d.pipelines.registration.registration_icp(
        source,
        target,
        MAX_DISTANCE_FINE,
        icp_coarse.transformation,
        o3d.pipelines.registration.TransformationEstimationPointToPlane(),
    )
    transformation_icp = icp_fine.transformation
    information_icp = o3d.pipelines.registration.get_information_matrix_from_point_clouds(
        source, target, MAX_DISTANCE_FINE, icp_fine.transformation
    )
    return transformation_icp, information_icp


def full_registration(pcds):
    pose_graph = o3d.pipelines.registration.PoseGraph()
    odometry = np.identity(4)
    pose_graph.nodes.append(o3d.pipelines.registration.PoseGraphNode(odometry))
    n_pcds = len(pcds)
    for source_id in range(n_pcds):
        for target_id in range(source_id + 1, n_pcds):
            transformation_icp, information_icp = pairwise_registration(pcds[source_id], pcds[target_id])
            if target_id == source_id + 1:  # odometry case
                odometry = np.dot(transformation_icp, odometry)
                pose_graph.nodes.append(o3d.pipelines.registration.PoseGraphNode(np.linalg.inv(odometry)))
                pose_graph.edges.append(
                    o3d.pipelines.registration.PoseGraphEdge(
                        source_id, target_id, transformation_icp, information_icp, uncertain=False
                    )
                )
            else:  # loop closure case
                pose_graph.edges.append(
                    o3d.pipelines.registration.PoseGraphEdge(
                        source_id, target_id, transformation_icp, information_icp, uncertain=True
                    )
                )
    return pose_graph


def combine_pointclouds(o3d_pcd_dict, cam2base_dict=None, reference_key=None):
    # Create O3D Pointcloud Objects + Align Them With Robot Base #
    # o3d_pcd_dict = {pcd_id: numpy_to_o3d(pcd) for pcd_id, pcd in pointcloud_dict.items()}
    if cam2base_dict is not None:
        [transform_pointcloud(o3d_pcd_dict[key], cam2base_dict[key]) for key in cam2base_dict]
    pcd_list = [o3d_pcd_dict[key] for key in o3d_pcd_dict]

    # Set Reference Frame For Merged Pointcloud #
    if reference_key:
        pcd_list.remove(o3d_pcd_dict[reference_key])
        pcd_list.insert(0, o3d_pcd_dict[reference_key])

    pose_graph = full_registration(pcd_list)
    print("Optimizing...")
    option = o3d.pipelines.registration.GlobalOptimizationOption(
        max_correspondence_distance=MAX_DISTANCE_FINE, edge_prune_threshold=0.25, reference_node=0
    )

    o3d.pipelines.registration.global_optimization(
        pose_graph,
        o3d.pipelines.registration.GlobalOptimizationLevenbergMarquardt(),
        o3d.pipelines.registration.GlobalOptimizationConvergenceCriteria(),
        option,
    )

    print("Downsizing...")
    pcd_combined = o3d.geometry.PointCloud()
    for point_id in range(len(pcd_list)):
        pcd_list[point_id].transform(pose_graph.nodes[point_id].pose)
        pcd_combined += pcd_list[point_id]
    pcd_combined_down = pcd_combined.voxel_down_sample(voxel_size=VOXEL_SIZE)

    print("Visualizing...")
    visualize_pointcloud(pcd_combined_down)

    # numpy_pcd = o3d_to_numpy(pcd_combined_down)

    return pcd_combined_down



================================================
FILE: droid/misc/server_interface.py
================================================
import time

import numpy as np
import zerorpc


def attempt_n_times(function_list, max_attempts, sleep_time=0.1):
    if type(function_list) is not list:
        function_list = list(function_list)

    for i in range(max_attempts):
        try:
            [f() for f in function_list]
            return
        except zerorpc.exceptions.RemoteError as err:
            last_attempt = i == (max_attempts - 1)
            if last_attempt:
                raise err
            else:
                time.sleep(sleep_time)


class ServerInterface:
    def __init__(self, ip_address="127.0.0.1", launch=True):
        self.ip_address = ip_address
        self.establish_connection()

        if launch:
            func_list = [self.launch_controller, self.launch_robot]
            attempt_n_times(func_list, max_attempts=2)

    def establish_connection(self):
        self.server = zerorpc.Client(heartbeat=20)
        self.server.connect("tcp://" + self.ip_address + ":4242")

    def launch_controller(self):
        self.server.launch_controller()

    def launch_robot(self):
        self.server.launch_robot()

    def kill_controller(self):
        self.server.kill_controller()

    def update_command(self, command, action_space="cartesian_velocity", gripper_action_space="velocity", blocking=False):
        action_dict = self.server.update_command(command.tolist(), action_space, gripper_action_space, blocking)
        return action_dict

    def create_action_dict(self, command, action_space="cartesian_velocity"):
        action_dict = self.server.create_action_dict(command.tolist(), action_space)
        return action_dict

    def update_pose(self, command, velocity=True, blocking=False):
        self.server.update_pose(command.tolist(), velocity, blocking)

    def update_joints(self, command, velocity=True, blocking=False, cartesian_noise=None):
        if cartesian_noise is not None:
            cartesian_noise = cartesian_noise.tolist()
        self.server.update_joints(command.tolist(), velocity, blocking, cartesian_noise)

    def update_gripper(self, command, velocity=True, blocking=False):
        self.server.update_gripper(command, velocity, blocking)

    def get_ee_pose(self):
        return np.array(self.server.get_ee_pose())

    def get_joint_positions(self):
        return np.array(self.server.get_joint_positions())

    def get_joint_velocities(self):
        return np.array(self.server.get_joint_velocities())

    def get_gripper_state(self):
        return self.server.get_gripper_state()

    def get_robot_state(self):
        return self.server.get_robot_state()



================================================
FILE: droid/misc/subprocess_utils.py
================================================
import multiprocessing
import subprocess
import threading


def run_terminal_command(command):
    process = subprocess.Popen(
        command, stdout=subprocess.PIPE, stdin=subprocess.PIPE, shell=True, executable="/bin/bash", encoding="utf8"
    )

    return process


def run_threaded_command(command, args=(), daemon=True):
    thread = threading.Thread(target=command, args=args, daemon=daemon)
    thread.start()

    return thread


def run_multiprocessed_command(command, args=()):
    process = multiprocessing.Process(target=command, args=args)
    process.start()

    return process



================================================
FILE: droid/misc/time.py
================================================
import time


def time_ms():
    return time.time_ns() // 1_000_000



================================================
FILE: droid/misc/transformations.py
================================================
import numpy as np
from scipy.spatial.transform import Rotation as R


### Conversions ###
def quat_to_euler(quat, degrees=False):
    euler = R.from_quat(quat).as_euler("xyz", degrees=degrees)
    return euler


def euler_to_quat(euler, degrees=False):
    return R.from_euler("xyz", euler, degrees=degrees).as_quat()


def rmat_to_euler(rot_mat, degrees=False):
    euler = R.from_matrix(rot_mat).as_euler("xyz", degrees=degrees)
    return euler


def euler_to_rmat(euler, degrees=False):
    return R.from_euler("xyz", euler, degrees=degrees).as_matrix()


def rmat_to_quat(rot_mat, degrees=False):
    quat = R.from_matrix(rot_mat).as_quat()
    return quat


def quat_to_rmat(quat, degrees=False):
    return R.from_quat(quat, degrees=degrees).as_matrix()


### Subtractions ###
def quat_diff(target, source):
    result = R.from_quat(target) * R.from_quat(source).inv()
    return result.as_quat()


def angle_diff(target, source, degrees=False):
    target_rot = R.from_euler("xyz", target, degrees=degrees)
    source_rot = R.from_euler("xyz", source, degrees=degrees)
    result = target_rot * source_rot.inv()
    return result.as_euler("xyz")


def pose_diff(target, source, degrees=False):
    lin_diff = np.array(target[:3]) - np.array(source[:3])
    rot_diff = angle_diff(target[3:6], source[3:6], degrees=degrees)
    result = np.concatenate([lin_diff, rot_diff])
    return result


### Additions ###
def add_quats(delta, source):
    result = R.from_quat(delta) * R.from_quat(source)
    return result.as_quat()


def add_angles(delta, source, degrees=False):
    delta_rot = R.from_euler("xyz", delta, degrees=degrees)
    source_rot = R.from_euler("xyz", source, degrees=degrees)
    new_rot = delta_rot * source_rot
    return new_rot.as_euler("xyz", degrees=degrees)


def add_poses(delta, source, degrees=False):
    lin_sum = np.array(delta[:3]) + np.array(source[:3])
    rot_sum = add_angles(delta[3:6], source[3:6], degrees=degrees)
    result = np.concatenate([lin_sum, rot_sum])
    return result


### MISC ###
def change_pose_frame(pose, frame, degrees=False):
    R_frame = euler_to_rmat(frame[3:6], degrees=degrees)
    R_pose = euler_to_rmat(pose[3:6], degrees=degrees)
    t_frame, t_pose = frame[:3], pose[:3]
    euler_new = rmat_to_euler(R_frame @ R_pose, degrees=degrees)
    t_new = R_frame @ t_pose + t_frame
    result = np.concatenate([t_new, euler_new])
    return result



================================================
FILE: droid/misc/version_control/1_0.json
================================================
{"DROID Version": "1.0",
"oculus_params": {
	"saving_target_info": false,
	"pos_action_gain": 3,
	"rot_action_gain": 3
	},
"polymetis_params": {
	"default_Kx": [750, 750, 750, 15, 15, 15]
	},
"ik_params": {
	"relative_max_joint_delta": [0.2075, 0.2075, 0.2075, 0.2075, 0.251, 0.251, 0.251],
	"max_lin_delta": 0.125,
	"max_rot_delta": 0.15
	},
"camera_params": {
	"hand_camera_resolution": "VGA"
	}
}



================================================
FILE: droid/misc/version_control/1_1.json
================================================
{"DROID Version": "1.1",
"oculus_params": {
	"saving_target_info": true,
	"pos_action_gain": 5,
	"rot_action_gain": 2
},
"polymetis_params": {
	"default_Kx": [400, 400, 400, 15, 15, 15]
},
"ik_params": {
	"relative_max_joint_delta": [0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2],
	"max_lin_delta": 0.075,
	"max_rot_delta": 0.15
},
"camera_params": {
	"hand_camera_resolution": "HD720"
	}
}



================================================
FILE: droid/misc/version_control/loader.py
================================================
import json
import os

# Prepare Filepath #
dir_path = os.path.dirname(os.path.realpath(__file__))


def load_version_info(version_number):
    version_number = version_number.replace(".", "_")
    version_string = "{0}.json".format(version_number)
    version_info_filepath = os.path.join(dir_path, version_string)
    if not os.path.isfile(version_info_filepath):
        return {}
    with open(version_info_filepath, "r") as jsonFile:
        version_info = json.load(jsonFile)
    return version_info



================================================
FILE: droid/plotting/__init__.py
================================================
[Empty file]


================================================
FILE: droid/plotting/analysis_func.py
================================================
import os
from collections import defaultdict

import h5py
import numpy as np

from droid.plotting.misc import *
from droid.plotting.text import *

# Create Empty Objects #
user_progress_dict = defaultdict(lambda: 0)
weekly_progress_dict = defaultdict(lambda: 0)
traj_progress_dict = defaultdict(lambda: np.array([0 for i in range(NUM_DAYS)]))
task_distribution_dict = defaultdict(lambda: 0)
scene_progress_dict = defaultdict(lambda: np.array([0 for i in range(NUM_DAYS)]))
all_camera_poses = []
all_traj_lengths = []
all_traj_ids = set()
all_scene_ids = set()


# Define Analysis Function #
def analysis_func(hdf5_filepath, hdf5_file=None):
    if hdf5_file is None:
        hdf5_file = h5py.File(hdf5_filepath, "r")

    traj_horizon = hdf5_file["action"]["joint_position"].shape[0]
    file_timestamp = os.path.getmtime(hdf5_filepath)
    day_index = get_bucket_index(file_timestamp)
    orig_user = hdf5_file.attrs["user"]
    user = clean_user.get(orig_user, orig_user)
    scene_id = hdf5_file.attrs.get("scene_id", 0)
    is_new_scene = hdf5_file.attrs.get("scene_id", 0) not in all_scene_ids
    traj_id = user + hdf5_file.attrs["time"]
    is_old_traj = traj_id in all_traj_ids
    curr_task = hdf5_file.attrs["current_task"]
    camera_poses = grab_3rd_person_extrinsics(
        hdf5_file["observation"]["camera_extrinsics"], hdf5_file["observation"]["camera_type"]
    )

    if is_old_traj:
        return
    else:
        all_traj_ids.add(traj_id)
    if user not in user_to_lab:
        print("Relevant Filepath: " + hdf5_filepath)
        print("WARNING: {0} not assigned to lab! To fix this permenantly, update the user dictionary.".format(user))
        user_to_lab[user] = input("Enter the lab for {0}:\n".format(user))

    # Update Total Progress #
    lab = user_to_lab.get(user, user)
    traj_progress_dict[lab][day_index] += 1

    # Update User Progress #
    user_progress_dict[user] += traj_horizon

    # Update Scene Progress #
    if is_new_scene:
        scene_progress_dict[lab][day_index] += 1
        all_scene_ids.add(scene_id)

    # Update Weekly Progress #
    if (file_timestamp >= WEEK_START) and (file_timestamp <= WEEK_END):
        weekly_progress_dict[lab] += traj_horizon

    # Update Task Distribution #
    task_label = task_mapper(curr_task)
    task_distribution_dict[task_label] += 1

    # Update Trajectory Length Distribution #
    all_traj_lengths.append(traj_horizon)

    # Update Camera Distribution #
    all_camera_poses.extend(camera_poses)



================================================
FILE: droid/plotting/misc.py
================================================
import datetime
import os
import time

import h5py
import numpy as np
from dateutil.relativedelta import relativedelta
from scipy import stats

from droid.plotting.misc import *
from droid.plotting.text import *

# Define Data Crawler #
num_demos = 0


def data_crawler(dirname, func_list=None, ignore_failure=True):
    global num_demos
    subfolders = [f.path for f in os.scandir(dirname) if f.is_dir()]
    traj_files = [f.path for f in os.scandir(dirname) if (f.is_file() and "trajectory.h5" in f.path)]
    h5_file_exists = len(traj_files) == 1

    # Obey Success / Failure Requirements #
    if ignore_failure and "failure" in dirname:
        return

    # Process Data #
    if h5_file_exists:
        num_demos += 1
        print("Num Demos:", num_demos)

        for func in func_list:
            hdf5_file = h5py.File(traj_files[0], "r")
            func(traj_files[0], hdf5_file=hdf5_file)

    for child_dirname in subfolders:
        data_crawler(child_dirname, func_list=func_list, ignore_failure=ignore_failure)


def task_mapper(task_description):
    for curr_task in all_tasks:
        if curr_task in task_description:
            return curr_task
    return "Arbitrary user defined task"


def grab_3rd_person_extrinsics(camera_extrinsics, camera_type_dict):
    varied_extrinsics = []
    for cam_id in camera_type_dict:
        # Ignore Hand Camera #
        if camera_type_dict[cam_id][0] == 0:
            continue

        # Gather Relevant Poses #
        for full_id in camera_extrinsics:
            if cam_id in full_id:
                cam_pose = camera_extrinsics[full_id][0]
                varied_extrinsics.append(cam_pose)

    return varied_extrinsics


def estimate_pos_angle_density(pose_list):
    cleaned_poses = {tuple(pose) for pose in pose_list}
    stacked_values = np.vstack(list(cleaned_poses))
    pos_values = stacked_values[:, :3].T
    ang_values = stacked_values[:, 3:].T
    pos_density_func = stats.gaussian_kde(pos_values)
    ang_density_func = stats.gaussian_kde(ang_values)
    pos_density = pos_density_func(pos_values)
    ang_density = ang_density_func(ang_values)
    return pos_values, pos_density, ang_values, ang_density


# Useful Values #
START_TIME = 1677500000
min_dt = datetime.datetime.min.time()

# Discretize Time #
start_date = datetime.datetime.fromtimestamp(START_TIME)
end_date = datetime.datetime.fromtimestamp(time.time())
NUM_DAYS = (end_date - start_date).days + 1
DAY_TIMESTAMPS = [
    datetime.datetime.timestamp(
        datetime.datetime.combine(start_date + datetime.timedelta(days=i), datetime.datetime.min.time())
    )
    for i in range(NUM_DAYS)
]


def get_bucket_index(timestamp):
    date = datetime.datetime.fromtimestamp(timestamp)
    index = (date - start_date).days
    return index


# Get Week Bracket #
today = datetime.date.today()
week_start_date = today - datetime.timedelta(days=today.weekday(), weeks=1)
week_end_date = week_start_date + datetime.timedelta(weeks=1)
week_start_dt = datetime.datetime.combine(week_start_date, min_dt)
week_end_dt = datetime.datetime.combine(week_end_date, min_dt)
WEEK_START = datetime.datetime.timestamp(week_start_dt)
WEEK_END = datetime.datetime.timestamp(week_end_dt)

# Get Per Month Info #
all_month_names, all_month_timestamps = [], []
curr_dt = start_date.replace(day=1)
while curr_dt <= end_date:
    # Get Info #
    month_name = curr_dt.strftime("%b")
    month_dt = datetime.datetime.combine(curr_dt, min_dt)
    month_ts = datetime.datetime.timestamp(month_dt)
    all_month_names.append(month_name)
    all_month_timestamps.append(month_ts)

    # Increment #
    curr_dt += relativedelta(months=+1)



================================================
FILE: droid/plotting/text.py
================================================
user_to_lab = {
    "Alexander Khazatsky": "IRIS",
    "Sasha Khazatsky": "IRIS",
    "Ethan Foster": "IRIS",
    "Emma Klemperer": "IRIS",
    "Suneel Belkhale": "ILIAD",
    "Joey Hejna": "ILIAD",
    "Yilin Wu": "ILIAD",
    "Marion Lepert": "IPRL",
    "Jimmy Wu": "IPRL",
    "Daniel Morton": "IPRL",
    "Homer Walke": "RAIL",
    "Caroline Johnson": "RAIL",
    "Samantha Huang": "RAIL",
    "Christian Avina": "RAIL",
    "Emi Tran": "RAIL",
    "Mohan Kumar": "GuptaLab",
    "Glen Berseth": "REAL",
    "Kirsty Ellis": "REAL",
    "Kirsty ellis": "REAL",
    "Paul Crouther": "REAL",
    "Albert Zhan": "REAL",
    "Cassandre Hamel": "REAL",
    "Samy Rasmy": "REAL",
    "Minho Heo": "CLVR",
    "Sungjae Park": "CLVR",
    "Vaidehi Som": "PennPAL",
    # Added by Sidd...
    "Heng Wei": "REAL",
    "Yunshuang Li": "PennPAL",
    "Jason Ma": "PennPAL",
    "Rishi Bedi": "IPRL",
    "Amine Obeid": "REAL",
    "Kaylee Burns": "IRIS",
    # Added by Lawrence...
    "Lawrence Chen": "AUTOLab",
    "Roy Lin": "AUTOLab",
    "Zehan Ma": "AUTOLab",
}

clean_user = {
    "Alexander Khazatsky": "Sasha Khazatsky",
    "Kirsty ellis": "Kirsty Ellis",
    # Added by Sidd...
    "heng wei": "Heng Wei",
}

all_tasks = [
    "Press button",
    "Open or close hinged object",
    "Open or close slidable objects",
    "Turn twistable object",
    "Move object into or out of container",
    "Move lid on or off of container",
    "Move object to a new position and orientation",
    "Use cup to pour something granular",
    "Use object to pick up something",
    "Use cloth to clean something",
    "Use object to stir something",
    "Open or close curtain",
    "Hang or unhang object",
    "Fold, spread out, or clump object",
    "Do anything you like that takes multiple steps to complete",
    "Do any task, and then reset the scene",
    "Do any two tasks consecutively",
    "Do any three tasks consecutively",
]



================================================
FILE: droid/postprocessing/__init__.py
================================================
[Empty file]


================================================
FILE: droid/postprocessing/parse.py
================================================
"""
parse.py

Core parsing logic -- takes a path to a raw demonstration directory (comprised of `trajectory.h5` and the SVO
recordings), parses out the relevant structured information following the schema in `droid.postprocessing.schema`,
returning a JSON-serializable data record.
"""
from datetime import datetime
from pathlib import Path
from typing import Dict, Optional, Tuple
import os

import h5py
import json

from droid.postprocessing.schema import TRAJECTORY_SCHEMA


def parse_datetime(date_str: str, mode="day") -> datetime:
    if mode == "day":
        return datetime.strptime(date_str, "%Y-%m-%d")
    else:
        raise ValueError(f"Function `parse_datetime` mode `{mode}` not supported!")


def parse_user(
    trajectory_dir: Path, aliases: Dict[str, Tuple[str, str]], members: Dict[str, Dict[str, str]]
) -> Tuple[Optional[str], Optional[str]]:
    try:
        with h5py.File(trajectory_dir / "trajectory.h5", "r") as h5:
            user_alias = h5.attrs["user"].title()
            lab, user = aliases.get(user_alias, (None, None))

        assert user_alias in aliases, f"User alias `{user_alias}` not in REGISTERED_LAB_MEMBERS or REGISTERED_ALIASES!"
        assert lab in members, f"Lab `{lab}` not in REGISTERED_LAB_MEMBERS!"
        assert user in members[lab], f"Canonical user `{user}` not in REGISTERED_LAB_MEMBERS['{lab}']"

        return user, members[lab][user]

    except AssertionError as e:
        raise e

    except (KeyError, OSError, RuntimeError):
        # Invalid/Incomplete HDF5 File --> return invalid :: (None, None)
        return None, None

def parse_existing_metadata(trajectory_dir: str):
    dir_path = os.path.abspath(trajectory_dir)
    all_filepaths = [entry.path for entry in os.scandir(dir_path) if entry.is_file()]
    for filepath in all_filepaths:
        if 'metadata' in filepath:
            with open(filepath) as json_file:
                metadata = json.load(json_file)
            return metadata
    return None

def parse_data_directory(data_dir: str, lab_agnostic: bool = False, process_failures: bool = False):
    if lab_agnostic:
        data_dirs = [p for p in data_dir.iterdir() if p.is_dir()]
    else:
        data_dirs = [data_dir]

    paths_to_index = []
    for curr_dir in data_dirs:
        paths_to_index += [(p, p.name) for p in [curr_dir / "success"]]
        if process_failures:
            paths_to_index += [(p, p.name) for p in [curr_dir / "failure"]]

    return paths_to_index


def parse_timestamp(trajectory_dir: Path) -> str:
    for pattern in [
        "%a_%b__%d_%H:%M:%S_%Y",
        "%a_%b_%d_%H:%M:%S_%Y",  # Colon
        "%a_%b__%d_%H_%M_%S_%Y",
        "%a_%b_%d_%H_%M_%S_%Y",  # Underscore
        "%a_%b__%d_%H:%M:%S_%Y",
        "%a_%b_%d_%H:%M:%S_%Y",  # Slash
    ]:
        try:
            return datetime.strptime(trajectory_dir.name, pattern).strftime("%Y-%m-%d-%Hh-%Mm-%Ss")
        except ValueError as e:
            assert ("time data" in str(e)) and ("does not match format" in str(e))
            continue

    # Invalid Trajectory Directory Path --> wonky timestamp! Check for common failure cases, then error.
    try:
        _ = datetime.strptime(trajectory_dir.name, "%Y-%m-%d")
        raise AssertionError(f"Unexpected Directory `{trajectory_dir}` -- did you accidentally nest directories?")
    except ValueError as e:
        raise AssertionError(f"Invalid Directory `{trajectory_dir}` -- check timestamp format!") from e


def parse_trajectory(
    data_dir: Path, trajectory_dir: Path, uuid: str, lab: str, user: str, user_id: str, timestamp: str
) -> Tuple[bool, Optional[Dict]]:
    """Attempt to parse `<trajectory>/trajectory.h5` and extract relevant elements into a JSON-valid record."""
    try:
        with h5py.File(trajectory_dir / "trajectory.h5", "r") as h5:
            assert "action" in h5.keys(), "Incomplete HDF5 file; no actual trajectory data logged!"
            trajectory_record, attrs, trajectory_length = {}, h5.attrs, int(h5["action"]["joint_position"].shape[0])
            exts = ["ext1", "ext2"]

            # Extract Camera Information
            camera_types, camera_extrinsics = h5["observation"]["camera_type"], h5["observation"]["camera_extrinsics"]
            ctype2extrinsics = {
                "wrist" if camera_types[serial][0] == 0 else exts.pop(0): {
                    "serial": serial,
                    "extrinsics": camera_extrinsics[f"{serial}_left"][0],
                }
                for serial in sorted(camera_types.keys())
            }

            # Compute Relative Path to `trajectory.h5`
            hdf5_path = str(trajectory_dir.relative_to(data_dir) / "trajectory.h5")

            # Populate Record
            for cname, etl_fn in TRAJECTORY_SCHEMA.items():
                trajectory_record[cname] = etl_fn(
                    uuid=uuid,
                    lab=lab,
                    user=user,
                    user_id=user_id,
                    timestamp=timestamp,
                    hdf5_path=hdf5_path,
                    attrs=attrs,
                    trajectory_length=trajectory_length,
                    ctype2extrinsics=ctype2extrinsics,
                )

            return True, trajectory_record

    except (AssertionError, KeyError, OSError, RuntimeError):
        # Invalid/Incomplete HDF5 File --> return invalid!
        return False, None



================================================
FILE: droid/postprocessing/schema.py
================================================
"""
schema.py

Schema & processing functions for extracting and formatting metadata from an individual DROID trajectory as a
JSON-serializable record. Converting to JSON provides a more friendly human-readable format, and facilitates downstream
tools for standing up a database/query engine.
"""
from typing import Callable, Dict, List, Optional


# === ETL Function Definitions ===
def get_uuid(*, uuid: str, **_) -> str:
    return uuid


def get_lab(*, lab: str, **_) -> str:
    return lab


def get_user(*, user: str, **_) -> str:
    return user


def get_user_id(*, user_id: str, **_) -> str:
    return user_id


def get_date(*, timestamp: str, **_) -> str:
    return timestamp[:10]  # YYYY-MM-DD


def get_timestamp(*, timestamp: str, **_) -> str:
    return timestamp


def get_hdf5_path(*, hdf5_path: str, **_) -> str:
    return hdf5_path


def get_building(*, attrs: Dict, **_) -> str:
    return attrs.get("building", "N/A")


def get_scene_id(*, attrs: Dict, **_) -> str:
    return int(attrs.get("scene_id", -1))


def get_success(*, attrs: Dict, **_) -> Optional[bool]:
    return bool(attrs["success"]) if "success" in attrs else None


def get_robot_serial(*, attrs: Dict, **_) -> str:
    return attrs.get("robot_serial_number", "unknown")


def get_droid_version(*, attrs: Dict, **_) -> str:
    return str(attrs.get("version_number", "-1.0"))


def get_current_task(*, attrs: Dict, **_) -> str:
    return attrs["current_task"]


def get_trajectory_length(*, trajectory_length: int, **_) -> int:
    return trajectory_length


def get_wrist_cam_serial(*, ctype2extrinsics: Dict, **_) -> str:
    return ctype2extrinsics["wrist"]["serial"]


def get_ext1_cam_serial(*, ctype2extrinsics: Dict, **_) -> str:
    return ctype2extrinsics["ext1"]["serial"]


def get_ext2_cam_serial(*, ctype2extrinsics: Dict, **_) -> str:
    return ctype2extrinsics["ext2"]["serial"]


def get_wrist_cam_extrinsics(*, ctype2extrinsics: Dict, **_) -> List[float]:
    return ctype2extrinsics["wrist"]["extrinsics"].tolist()


def get_ext1_cam_extrinsics(*, ctype2extrinsics: Dict, **_) -> List[float]:
    return ctype2extrinsics["ext1"]["extrinsics"].tolist()


def get_ext2_cam_extrinsics(*, ctype2extrinsics: Dict, **_) -> List[float]:
    return ctype2extrinsics["ext2"]["extrinsics"].tolist()


def get_path_placeholder(**_) -> None:
    return None


# fmt: off
TRAJECTORY_SCHEMA: Dict[str, Callable] = {
    "uuid": get_uuid,
    "lab": get_lab,
    "user": get_user,
    "user_id": get_user_id,
    "date": get_date,
    "timestamp": get_timestamp,

    # === HDF5 Parameters ===
    "hdf5_path": get_hdf5_path,
    "building": get_building,
    "scene_id": get_scene_id,
    "success": get_success,
    "robot_serial": get_robot_serial,
    "droid_version": get_droid_version,

    # === Task Parameters ===
    "current_task": get_current_task,

    # === Trajectory Parameters ===
    "trajectory_length": get_trajectory_length,

    # === ZED Camera Parameters ===
    "wrist_cam_serial": get_wrist_cam_serial,
    "ext1_cam_serial": get_ext1_cam_serial,
    "ext2_cam_serial": get_ext2_cam_serial,

    # Camera Extrinsics for Third-Person Cameras (always assumes *left* stereo camera, ext1/ext2 sorted by serial #)
    #   => Extrinsics are saved as a 6-dim vector of [pos; rot] where:
    #       - `pos` is (x, y, z) offset --> moving left of robot is +y, moving right is -y
    #       - `rot` is rotation offset as Euler (`R.from_matrix(rmat).as_euler("xyz")`)
    "wrist_cam_extrinsics": get_wrist_cam_extrinsics,
    "ext1_cam_extrinsics": get_ext1_cam_extrinsics,
    "ext2_cam_extrinsics": get_ext2_cam_extrinsics,

    # Save SVO and MP4 Paths --> Paths are always "relative" to <LAB> directory!
    "wrist_svo_path": get_path_placeholder,
    "wrist_mp4_path": get_path_placeholder,
    "ext1_svo_path": get_path_placeholder,
    "ext1_mp4_path": get_path_placeholder,
    "ext2_svo_path": get_path_placeholder,
    "ext2_mp4_path": get_path_placeholder,

    # Mapping to "left" and "right" external MP4s --> computed heuristically...
    "left_mp4_path": get_path_placeholder,
    "right_mp4_path": get_path_placeholder,
}
# fmt: off



================================================
FILE: droid/postprocessing/stages.py
================================================
"""
stages.py

Functions capturing logic for the various postprocessing stages:
    - Stage 1 :: "Indexing"  -->  Quickly iterate through all data, identifying formatting errors & naively counting
                                  total number of demonstrations to process/convert/upload.

                                  Note :: Raises hard exceptions on any unexpected directory/file formatting!

    - Stage 2 :: "Processing" --> Walk through data, extract & validate metadata (writing a JSON record for each unique
                                  demonstration). Additionally, runs conversion from SVO --> MP4.

                                  Note :: Logs corrupt HDF5/SVO files & raises warning at end of stage.

    - Stage 3 :: "Uploading" -->  Iterates through individual processed demonstration directories, and uploads them
                                  sequentially to the AWS S3 Bucket (via `boto`).

The outputs/failures of each stage are logged to a special cache data structure that prevents redundant work where
possible. Note that to emphasize readability, some of the following code is intentionally redundant.
"""
import json
from datetime import datetime
from pathlib import Path
from typing import Dict, Tuple

import boto3
from tqdm import tqdm

from droid.postprocessing.parse import parse_datetime, parse_timestamp, parse_trajectory, parse_user, parse_existing_metadata, parse_data_directory
from droid.postprocessing.util.svo2mp4 import convert_mp4s
from droid.postprocessing.util.svo2depth import convert_depths
from droid.postprocessing.util.validate import validate_day_dir, validate_metadata_record, validate_svo_existence


# === Stage 1 :: Indexing ===
def run_indexing(
    data_dir: Path,
    lab: str,
    start_datetime: datetime,
    aliases: Dict[str, Tuple[str, str]],
    members: Dict[str, Dict[str, str]],
    totals: Dict[str, Dict[str, int]],
    scanned_paths: Dict[str, Dict[str, str]],
    indexed_uuids: Dict[str, Dict[str, str]],
    errored_paths: Dict[str, Dict[str, str]],
    search_existing_metadata: bool = False,
    lab_agnostic: bool = False,
    process_failures: bool = True,
) -> None:
    """Index data by iterating through each "success/ | failure/" --> <DAY>/ --> <TIMESTAMP>/ (specified trajectory)."""
    progress = tqdm(desc="[*] Stage 1 =>> Indexing")

    paths_to_index = parse_data_directory(data_dir, lab_agnostic=lab_agnostic, process_failures=process_failures)

    for outcome_dir, outcome in paths_to_index:
        if outcome == "failure" and not outcome_dir.exists():
            # Note: Some labs don't have failure trajectories...
            continue

        day_dirs = sorted([p for p in outcome_dir.iterdir() if p.is_dir() and validate_day_dir(p)])
        for day_dir, day in [(p, p.name) for p in day_dirs]:
            if parse_datetime(day) < start_datetime:
                continue

            for trajectory_dir in [p for p in day_dir.iterdir() if p.is_dir()]:
                rel_trajectory_dir = str(trajectory_dir.relative_to(data_dir))

                # Extract Timestamp (from `trajectory_dir`) and User, User ID (from `trajectory.h5`)
                existing_metadata_found = False
                if search_existing_metadata:
                    metadata = parse_existing_metadata(trajectory_dir)
                    if metadata is not None:
                        timestamp = metadata['timestamp']
                        user = metadata['user']
                        user_id = metadata['user_id']
                        uuid = metadata['uuid']
                        existing_metadata_found = True
                if not (search_existing_metadata and existing_metadata_found):
                    timestamp = parse_timestamp(trajectory_dir)
                    user, user_id = parse_user(trajectory_dir, aliases, members)

                    # Create Trajectory UUID --> <LAB>+<USER_ID>+YYYY-MM-DD-{24 Hour}h-{Min}m-{Sec}s
                    uuid = f"{lab}+{user_id}+{timestamp}"

                if user is None or user_id is None:
                    scanned_paths[outcome][rel_trajectory_dir] = True
                    errored_paths[outcome][rel_trajectory_dir] = (
                        "[Indexing Error] Missing/Invalid HDF5! "
                        "If the HDF5 is missing/corrupt, you can delete this trajectory!"
                    )
                    totals["scanned"][outcome] = len(scanned_paths[outcome])
                    totals["errored"][outcome] = len(errored_paths[outcome])
                    progress.update()
                    continue

                # Verify SVO Files
                if not validate_svo_existence(trajectory_dir):
                    scanned_paths[outcome][rel_trajectory_dir] = True
                    errored_paths[outcome][rel_trajectory_dir] = (
                        "[Indexing Error] Missing SVO Files! "
                        "Ensure all 3 SVO files are in `<timestamp>/recordings/SVO/<serial>.svo!"
                    )
                    totals["scanned"][outcome] = len(scanned_paths[outcome])
                    totals["errored"][outcome] = len(errored_paths[outcome])
                    progress.update()
                    continue

                # Otherwise -- we're good for indexing!
                indexed_uuids[outcome][uuid] = rel_trajectory_dir
                scanned_paths[outcome][rel_trajectory_dir] = True
                errored_paths[outcome].pop(rel_trajectory_dir, None)
                totals["scanned"][outcome] = len(scanned_paths[outcome])
                totals["indexed"][outcome] = len(indexed_uuids[outcome])
                totals["errored"][outcome] = len(errored_paths[outcome])
                progress.update()


# === Stage 2 :: Processing ===
def run_processing(
    data_dir: Path,
    lab: str,
    aliases: Dict[str, Tuple[str, str]],
    members: Dict[str, Dict[str, str]],
    totals: Dict[str, Dict[str, int]],
    indexed_uuids: Dict[str, Dict[str, str]],
    processed_uuids: Dict[str, Dict[str, str]],
    errored_paths: Dict[str, Dict[str, str]],
    process_batch_limit: int = 250,
    search_existing_metadata: bool = False,
    extract_MP4_data: bool = True,
    extract_depth_data: bool = False,
    depth_resolution: tuple = (0,0),
    depth_frequency: int = 1,
) -> None:
    """Iterate through each trajectory in `indexed_uuids` and 1) extract JSON metadata and 2) convert SVO -> MP4."""
    for outcome in indexed_uuids:
        uuid2trajectory_generator, counter = indexed_uuids[outcome].items(), 0
        for uuid, rel_trajectory_dir in tqdm(uuid2trajectory_generator, desc=f"[*] Stage 2 =>> `{outcome}` Processing"):
            if uuid in processed_uuids[outcome]:
                continue

            trajectory_dir = data_dir / rel_trajectory_dir
            
            existing_metadata_found = False
            if search_existing_metadata:
                metadata = parse_existing_metadata(trajectory_dir)
                if metadata is not None:
                    metadata_record = metadata
                    timestamp = metadata['timestamp']
                    user = metadata['user']
                    user_id = metadata['user_id']
                    uuid = metadata['uuid']
                    existing_metadata_found = True
                    valid_parse = True
            if not (search_existing_metadata and existing_metadata_found):
                timestamp = parse_timestamp(trajectory_dir)
                user, user_id = parse_user(trajectory_dir, aliases, members)

                # Run Metadata Extraction --> JSON-serializable Data Record + Validation
                valid_parse, metadata_record = parse_trajectory(
                    data_dir, trajectory_dir, uuid, lab, user, user_id, timestamp
                )
            if not valid_parse:
                errored_paths[outcome][rel_trajectory_dir] = "[Processing Error] JSON Metadata Parse Error"
                totals["errored"][outcome] = len(errored_paths[outcome])
                continue

            # Convert SVOs --> MP4s
            if extract_MP4_data:
                valid_convert, vid_paths = convert_mp4s(
                    data_dir,
                    trajectory_dir,
                    metadata_record["wrist_cam_serial"],
                    metadata_record["ext1_cam_serial"],
                    metadata_record["ext2_cam_serial"],
                    metadata_record["ext1_cam_extrinsics"],
                    metadata_record["ext2_cam_extrinsics"],
                )
                if not valid_convert:
                    errored_paths[outcome][rel_trajectory_dir] = "[Processing Error] Corrupted SVO / Failed Conversion"
                    totals["errored"][outcome] = len(errored_paths[outcome])
                    continue

                # Extend Metadata Record
                for key, vid_path in vid_paths.items():
                    metadata_record[key] = vid_path

            # Convert SVOs --> Depth
            if extract_depth_data:
                valid_convert, vid_paths = convert_depths(
                    data_dir,
                    trajectory_dir,
                    metadata_record["wrist_cam_serial"],
                    metadata_record["ext1_cam_serial"],
                    metadata_record["ext2_cam_serial"],
                    metadata_record["ext1_cam_extrinsics"],
                    metadata_record["ext2_cam_extrinsics"],
                    resolution=depth_resolution,
                    frequency=depth_frequency,
                )
                if not valid_convert:
                    errored_paths[outcome][rel_trajectory_dir] = "[Processing Error] Corrupted SVO / Failed Conversion"
                    totals["errored"][outcome] = len(errored_paths[outcome])
                    continue

                # Extend Metadata Record
                for key, vid_path in vid_paths.items():
                    metadata_record[key] = vid_path

            # Validate
            if not validate_metadata_record(metadata_record):
                errored_paths[outcome][rel_trajectory_dir] = "[Processing Error] Incomplete Metadata Record!"
                totals["errored"][outcome] = len(errored_paths[outcome])
                continue

            # Write JSON
            with open(trajectory_dir / f"metadata_{uuid}.json", "w") as f:
                json.dump(metadata_record, f)

            # Otherwise --> we're good for processing!
            processed_uuids[outcome][uuid] = rel_trajectory_dir
            errored_paths[outcome].pop(rel_trajectory_dir, None)
            totals["processed"][outcome] = len(processed_uuids[outcome])
            totals["errored"][outcome] = len(errored_paths[outcome])
            counter += 1

            # Note :: ZED SDK has an unfortunate problem with segmentation faults after processing > 2000 videos.
            #         Unfortunately, no good way to catch/handle a segfault from Python --> instead we just set
            #         a max limit `process_batch_limit` and trust that caching works.
            if counter > process_batch_limit:
                print("[*] EXITING TO PREVENT SVO SEGFAULT!")
                return


# === Stage 3 :: Uploading ===
def run_upload(
    data_dir: Path,
    lab: str,
    credentials_json: Path,
    totals: Dict[str, Dict[str, int]],
    processed_uuids: Dict[str, Dict[str, str]],
    uploaded_uuids: Dict[str, Dict[str, str]],
    bucket_name: str = "droid-data",
    prefix: str = "lab-uploads/",
) -> None:
    """Iterate through each successfully processed trajectory in `processed_uuids` and upload to S3."""
    with open(credentials_json, "r") as f:
        credentials = json.load(f)

    # Initialize S3 Client from Credentials & Validate
    client = boto3.client(
        "s3", aws_access_key_id=credentials["AccessKeyID"], aws_secret_access_key=credentials["SecretAccessKey"]
    )
    response = client.head_bucket(Bucket=bucket_name)
    assert (
        response["ResponseMetadata"]["HTTPStatusCode"] == 200
    ), "Problem connecting to S3 bucket; verify credentials JSON file!"

    # Start Uploading
    for outcome in processed_uuids:
        uuid2trajectory_generator = processed_uuids[outcome].items()
        for uuid, rel_trajectory_dir in tqdm(uuid2trajectory_generator, desc=f"[*] Stage 3 =>> `{outcome}` Uploading"):
            if uuid in uploaded_uuids[outcome]:
                continue

            # Recursively walk through each file in the `trajectory_dir` and upload one at a time!
            trajectory_dir = data_dir / rel_trajectory_dir
            for child in tqdm(
                list(trajectory_dir.rglob("*")), desc=f"     => Uploading `{rel_trajectory_dir}`", leave=False
            ):
                if child.is_file():
                    s3_path = str(Path(prefix) / lab / child.relative_to(data_dir))
                    client.upload_file(str(child), Bucket=bucket_name, Key=s3_path)

            # If we've managed to upload all files without error, then we're good for uploading!
            uploaded_uuids[outcome][uuid] = rel_trajectory_dir
            totals["uploaded"][outcome] = len(uploaded_uuids[outcome])



================================================
FILE: droid/postprocessing/util/__init__.py
================================================
[Empty file]


================================================
FILE: droid/postprocessing/util/svo2depth.py
================================================
"""
svo2mp4.py

Utility scripts for using the ZED Python SDK and FFMPEG to convert raw `.svo` files to `.mp4` files (including "fused"
MP4s with multiple camera feeds).
"""
import os
import subprocess
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import cv2
import numpy as np
import pyzed.sl as sl
from tqdm import tqdm
import imageio

def export_depth(
    svo_file: Path,
    depth_dir: Path,
    stereo_view: str = "left",
    resolution: tuple = (0,0),
    frequency: int = 1,
    show_progress: bool = False
) -> bool:
    """Reads an SVO file, dumping the export depth to the desired path; supports ZED SDK 3.8.* and 4.0.* ONLY."""
    
    # Create Depth Folder
    depth_out = depth_dir / f"{svo_file.stem}"
    os.makedirs(depth_out, exist_ok=True)
    sdk_version, use_sdk_4 = sl.Camera().get_sdk_version(), None
    if not (sdk_version.startswith("4.0") or sdk_version.startswith("3.8")):
        raise ValueError("Function `export_mp4` only supports ZED SDK 3.8 OR 4.0; if you see this, contact Sidd!")
    else:
        use_sdk_4 = sdk_version.startswith("4.0")

    # Configure PyZED --> set mostly from SVO Path, don't convert in realtime!
    initial_parameters = sl.InitParameters()
    initial_parameters.set_from_svo_file(str(svo_file))
    initial_parameters.svo_real_time_mode = False
    initial_parameters.coordinate_units = sl.UNIT.MILLIMETER
    initial_parameters.camera_image_flip = sl.FLIP_MODE.OFF
    if stereo_view == "right":
        initial_parameters.enable_right_side_measure = True
        depth_measure = sl.MEASURE.DEPTH_RIGHT
    else:
        depth_measure = sl.MEASURE.DEPTH

    # Create ZED Camera Object & Open SVO File
    zed = sl.Camera()
    err = zed.open(initial_parameters)
    if err != sl.ERROR_CODE.SUCCESS:
        zed.close()
        return False

    # Set Reading Resolution #
    depth_resolution = sl.Resolution(*resolution)

    # Create ZED Image Containers
    assert stereo_view in {"left", "right"}, f"Invalid View to Export `{stereo_view}`!"
    img_container = sl.Mat()

    # SVO Export
    n_frames, rt_parameters = zed.get_svo_number_of_frames(), sl.RuntimeParameters()
    if show_progress:
        pbar = tqdm(total=n_frames, desc="     => Exporting SVO Frames", leave=False)

    # Read & Transcode all Frames
    while True:
        grabbed = zed.grab(rt_parameters)

        # [NOTE SDK SEMANTICS] --> ZED SDK 4.0 introduces `sl.ERROR_CODE.END_OF_SVOFILE_REACHED`
        if (grabbed == sl.ERROR_CODE.SUCCESS) or (use_sdk_4 and (grabbed == sl.ERROR_CODE.END_OF_SVOFILE_REACHED)):
            svo_position = zed.get_svo_position()
            should_extract = svo_position % frequency == 0
            if should_extract:
                zed.retrieve_measure(img_container, depth_measure, resolution=depth_resolution)
                processed_depth = (img_container.get_data() * 1000).astype(np.uint16)
                imageio.imwrite(depth_out / '{0}.png'.format(svo_position), processed_depth)

            # Update Progress
            if show_progress:
                pbar.update()

            # [NOTE SDK SEMANTICS] --> Check if we've reached the end of the video
            if (svo_position >= (n_frames - 1)) or (use_sdk_4 and (grabbed == sl.ERROR_CODE.END_OF_SVOFILE_REACHED)):
                break

    # Cleanup & Return
    zed.close()
    if show_progress:
        pbar.close()

    return True


def convert_depths(
    data_dir: Path,
    demo_dir: Path,
    wrist_serial: str,
    ext1_serial: str,
    ext2_serial: str,
    ext1_extrinsics: List[float],
    ext2_extrinsics: List[float],
    resolution: tuple,
    frequency: int,
    do_fuse: bool = False,
) -> Tuple[bool, Optional[Dict[str, str]]]:
    """Convert each `serial.svo` to a valid MP4 file, updating the `data_record` path entries in-place."""
    svo_path, depth_path = demo_dir / "recordings" / "SVO", demo_dir / "recordings" / "Depth"
    os.makedirs(depth_path, exist_ok=True)
    for svo_file in svo_path.iterdir():
        successful_convert = export_depth(svo_file, depth_path, resolution=resolution, frequency=frequency, show_progress=True)
        if not successful_convert:
            return False, None

    # Associate Ext1 / Ext2 with left/right positions relative to the robot base; use computed extrinsics.
    #   => Extrinsics are saved as a 6-dim vector of [pos; rot] where:
    #       - `pos` is (x, y, z) offset --> moving left of robot is +y, moving right is -y
    #       - `rot` is rotation offset as Euler (`R.from_matrix(rmat).as_euler("xyz")`)
    #   => Therefore we can compute `left = ext1_serial if ext1_extrinsics[1] > ext2_extrinsics[1]`
    ext1_y, ext2_y = ext1_extrinsics[1], ext2_extrinsics[1]
    left_serial = ext1_serial if ext1_y > ext2_y else ext2_serial
    right_serial = ext2_serial if left_serial == ext1_serial else ext1_serial

    # Create Dictionary of SVO/MP4 Paths
    rel_svo_path, rel_depth_path = svo_path.relative_to(data_dir), depth_path.relative_to(data_dir)
    record_paths = {
        "wrist_svo_path": str(rel_svo_path / f"{wrist_serial}.svo"),
        "wrist_depth_path": str(rel_depth_path / f"{wrist_serial}"),
        "ext1_svo_path": str(rel_svo_path / f"{ext1_serial}.svo"),
        "ext1_depth_path": str(rel_depth_path / f"{ext1_serial}"),
        "ext2_svo_path": str(rel_svo_path / f"{ext2_serial}.svo"),
        "ext2_depth_path": str(rel_depth_path / f"{ext2_serial}"),
        "left_depth_path": str(rel_depth_path / f"{left_serial}"),
        "right_depth_path": str(rel_depth_path / f"{right_serial}"),
    }

    return True, record_paths



================================================
FILE: droid/postprocessing/util/svo2mp4.py
================================================
"""
svo2mp4.py

Utility scripts for using the ZED Python SDK and FFMPEG to convert raw `.svo` files to `.mp4` files (including "fused"
MP4s with multiple camera feeds).
"""
import os
import subprocess
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import cv2
import pyzed.sl as sl
from tqdm import tqdm


def export_mp4(svo_file: Path, mp4_dir: Path, stereo_view: str = "left", show_progress: bool = False) -> bool:
    """Reads an SVO file, dumping the export MP4 to the desired path; supports ZED SDK 3.8.* and 4.0.* ONLY."""
    mp4_out = mp4_dir / f"{svo_file.stem}.mp4"
    sdk_version, use_sdk_4 = sl.Camera().get_sdk_version(), None
    if not (sdk_version.startswith("4.0") or sdk_version.startswith("3.8")):
        raise ValueError("Function `export_mp4` only supports ZED SDK 3.8 OR 4.0; if you see this, contact Sidd!")
    else:
        use_sdk_4 = sdk_version.startswith("4.0")

    # Configure PyZED --> set mostly from SVO Path, don't convert in realtime!
    initial_parameters = sl.InitParameters()
    initial_parameters.set_from_svo_file(str(svo_file))
    initial_parameters.svo_real_time_mode = False
    initial_parameters.coordinate_units = sl.UNIT.MILLIMETER
    initial_parameters.camera_image_flip = sl.FLIP_MODE.OFF

    # Create ZED Camera Object & Open SVO File
    zed = sl.Camera()
    err = zed.open(initial_parameters)
    if err != sl.ERROR_CODE.SUCCESS:
        zed.close()
        return False

    # [NOTE SDK SEMANTICS] --> Get Image Size & FPS
    if use_sdk_4:
        fps = zed.get_camera_information().camera_configuration.fps
        resolution = zed.get_camera_information().camera_configuration.resolution
        width, height = resolution.width, resolution.height
    else:
        fps = zed.get_camera_information().camera_fps
        resolution = zed.get_camera_information().camera_resolution
        width, height = resolution.width, resolution.height

    # Create ZED Image Containers
    assert stereo_view in {"left", "right"}, f"Invalid View to Export `{stereo_view}`!"
    img_container = sl.Mat()

    # Create a VideoWriter with the MP4V Codec
    video_writer = cv2.VideoWriter(
        str(mp4_out),
        cv2.VideoWriter_fourcc(*"mp4v"),
        fps,
        (width, height),
    )
    if not video_writer.isOpened():
        print(f"Error Opening CV2 Video Writer; check the MP4 path `{mp4_out}` and permissions!")
        zed.close()
        return False

    # SVO Export
    n_frames, rt_parameters = zed.get_svo_number_of_frames(), sl.RuntimeParameters()
    if show_progress:
        pbar = tqdm(total=n_frames, desc="     => Exporting SVO Frames", leave=False)

    # Read & Transcode all Frames
    while True:
        grabbed = zed.grab(rt_parameters)

        # [NOTE SDK SEMANTICS] --> ZED SDK 4.0 introduces `sl.ERROR_CODE.END_OF_SVOFILE_REACHED`
        if (grabbed == sl.ERROR_CODE.SUCCESS) or (use_sdk_4 and (grabbed == sl.ERROR_CODE.END_OF_SVOFILE_REACHED)):
            svo_position = zed.get_svo_position()
            zed.retrieve_image(img_container, {"left": sl.VIEW.LEFT, "right": sl.VIEW.RIGHT}[stereo_view])

            # Copy image data into VideoWrite after converting to RGB
            rgb = cv2.cvtColor(img_container.get_data(), cv2.COLOR_RGBA2RGB)
            video_writer.write(rgb)

            # Update Progress
            if show_progress:
                pbar.update()

            # [NOTE SDK SEMANTICS] --> Check if we've reached the end of the video
            if (svo_position >= (n_frames - 1)) or (use_sdk_4 and (grabbed == sl.ERROR_CODE.END_OF_SVOFILE_REACHED)):
                break

    # Cleanup & Return
    video_writer.release()
    zed.close()
    if show_progress:
        pbar.close()

    return True


def convert_mp4s(
    data_dir: Path,
    demo_dir: Path,
    wrist_serial: str,
    ext1_serial: str,
    ext2_serial: str,
    ext1_extrinsics: List[float],
    ext2_extrinsics: List[float],
    do_fuse: bool = False,
) -> Tuple[bool, Optional[Dict[str, str]]]:
    """Convert each `serial.svo` to a valid MP4 file, updating the `data_record` path entries in-place."""
    svo_path, mp4_path = demo_dir / "recordings" / "SVO", demo_dir / "recordings" / "MP4"
    os.makedirs(mp4_path, exist_ok=True)
    for svo_file in svo_path.iterdir():
        successful_convert = export_mp4(svo_file, mp4_path, show_progress=True)
        if not successful_convert:
            return False, None

    # Associate Ext1 / Ext2 with left/right positions relative to the robot base; use computed extrinsics.
    #   => Extrinsics are saved as a 6-dim vector of [pos; rot] where:
    #       - `pos` is (x, y, z) offset --> moving left of robot is +y, moving right is -y
    #       - `rot` is rotation offset as Euler (`R.from_matrix(rmat).as_euler("xyz")`)
    #   => Therefore we can compute `left = ext1_serial if ext1_extrinsics[1] > ext2_extrinsics[1]`
    ext1_y, ext2_y = ext1_extrinsics[1], ext2_extrinsics[1]
    left_serial = ext1_serial if ext1_y > ext2_y else ext2_serial
    right_serial = ext2_serial if left_serial == ext1_serial else ext1_serial

    # Create Dictionary of SVO/MP4 Paths
    rel_svo_path, rel_mp4_path = svo_path.relative_to(data_dir), mp4_path.relative_to(data_dir)
    record_paths = {
        "wrist_svo_path": str(rel_svo_path / f"{wrist_serial}.svo"),
        "wrist_mp4_path": str(rel_mp4_path / f"{wrist_serial}.mp4"),
        "ext1_svo_path": str(rel_svo_path / f"{ext1_serial}.svo"),
        "ext1_mp4_path": str(rel_mp4_path / f"{ext1_serial}.mp4"),
        "ext2_svo_path": str(rel_svo_path / f"{ext2_serial}.svo"),
        "ext2_mp4_path": str(rel_mp4_path / f"{ext2_serial}.mp4"),
        "left_mp4_path": str(rel_mp4_path / f"{left_serial}.mp4"),
        "right_mp4_path": str(rel_mp4_path / f"{right_serial}.mp4"),
    }

    if do_fuse:
        # Build Fused Left/Right MP4 Files via FFMPEG
        left, right = str(mp4_path / f"{left_serial}.mp4"), str(mp4_path / f"{right_serial}.mp4")
        subprocess.run(
            f"ffmpeg -y -i {left} -i {right} -vsync 2 -filter_complex hstack {mp4_path / 'fused.mp4'!s}",
            shell=True,
            check=True,
            stdout=subprocess.DEVNULL,
            stderr=subprocess.DEVNULL,
        )

    return True, record_paths



================================================
FILE: droid/postprocessing/util/validate.py
================================================
"""
validate.py

Helper functions for validating almost every part of the postprocessing pipeline; the general principle is to
*fail-fast*; if there is any data in a format we don't recognize, we raise an informative error, and stop the
post-processing loop.

It is up to the individual users to correct these errors/manually override them.
"""
import os
import re
import shutil
from datetime import datetime
from pathlib import Path
from typing import Dict


# === Indexing / Formatting Validators ===
def validate_user2id(registered_lab_members: Dict[str, Dict[str, str]]) -> bool:
    unique_users, unique_ids = {}, {}
    for lab in sorted(registered_lab_members):
        for user, uuid in sorted(registered_lab_members[lab].items()):
            dup_user, dup_uuid = f"Duplicate User `{user}` in Lab `{lab}", f"Duplicate UUID `{uuid}` for User `{user}`"
            assert user not in unique_users, f"{dup_user}; already exists in Lab: {unique_users[user]}!"
            assert uuid not in unique_ids, f"{dup_uuid}; already exists for (Lab, User): {unique_ids[uuid]}!"
            unique_users[user], unique_ids[uuid] = lab, user

    # Global Uniqueness Assertion
    assert len(unique_users) == len(unique_ids), "Mismatch between number of unique Users and UUIDs!"
    return True


def validate_day_dir(day_dir: Path) -> bool:
    format_err_msg = f"Invalid directory `{day_dir}`; should match YYYY-MM-DD!"
    date_err_msg = f"Invalid directory `{day_dir}`; date is in the future!"
    assert re.match(r"^\d{4}-\d{2}-\d{2}", day_dir.name) is not None, format_err_msg
    assert datetime.strptime(day_dir.name, "%Y-%m-%d") <= datetime.now(), date_err_msg
    return True


def validate_svo_existence(trajectory_dir: Path) -> bool:
    svo_path = trajectory_dir / "recordings" / "SVO"
    if svo_path.exists() and (len([p for p in svo_path.iterdir() if p.name.endswith(".svo")]) == 3):
        return True

    # Check Common Failure Mode --> files at `trajectory_dir / recordings / *.svo`
    fallback_svo_path = trajectory_dir / "recordings"
    if fallback_svo_path.exists() and (len([p for p in fallback_svo_path.iterdir() if p.name.endswith(".svo")]) == 3):
        os.makedirs(svo_path, exist_ok=False)
        svo_files = list([p for p in fallback_svo_path.iterdir() if p.name.endswith(".svo")])
        for file in svo_files:
            shutil.move(file, svo_path / file.name)
        return len([p for p in svo_path.iterdir() if p.name.endswith(".svo")]) == 3

    return False


# === Metadata Record Validator ===
def validate_metadata_record(metadata_record: Dict) -> bool:
    for key in metadata_record:
        if metadata_record[key] is None:
            return False

    return True



================================================
FILE: droid/robot_ik/arm.py
================================================
import os

import numpy as np
from dm_control import mjcf
from dm_robotics.moma.models import types
from dm_robotics.moma.models.robots.robot_arms import robot_arm

from droid.misc.parameters import robot_type


class RobotArm(robot_arm.RobotArm):
    def _build(self, model_file):
        self._mjcf_root = mjcf.from_path(self._model_file)

    def _create_body(self):
        # Find MJCF elements that will be exposed as attributes.
        self._joints = self._mjcf_root.find_all("joint")
        self._bodies = self.mjcf_model.find_all("body")
        self._actuators = self.mjcf_model.find_all("actuator")
        self._wrist_site = self.mjcf_model.find("site", "wrist_site")
        self._base_site = self.mjcf_model.find("site", "base_site")

    def name(self) -> str:
        return self._name

    @property
    def joints(self):
        """List of joint elements belonging to the arm."""
        return self._joints

    @property
    def actuators(self):
        """List of actuator elements belonging to the arm."""
        return self._actuators

    @property
    def mjcf_model(self):
        """Returns the `mjcf.RootElement` object corresponding to this robot."""
        return self._mjcf_root

    def update_state(self, physics: mjcf.Physics, qpos: np.ndarray, qvel: np.ndarray) -> None:
        physics.bind(self._joints).qpos[:] = qpos
        physics.bind(self._joints).qvel[:] = qvel

    def set_joint_angles(self, physics: mjcf.Physics, qpos: np.ndarray) -> None:
        physics.bind(self._joints).qpos[:] = qpos

    @property
    def base_site(self) -> types.MjcfElement:
        return self._base_site

    @property
    def wrist_site(self) -> types.MjcfElement:
        return self._wrist_site

    def initialize_episode(self, physics: mjcf.Physics, random_state: np.random.RandomState):
        """Function called at the beginning of every episode."""
        del random_state  # Unused.
        return


class FrankaArm(RobotArm):
    def _build(self):
        self._name = "franka"
        dir_path = os.path.dirname(os.path.realpath(__file__))
        self._model_file = os.path.join(dir_path, "franka", "{0}.xml".format(robot_type))
        self._mjcf_root = mjcf.from_path(self._model_file)
        self._create_body()



================================================
FILE: droid/robot_ik/robot_ik_solver.py
================================================
import numpy as np
from dm_control import mjcf
from dm_robotics.moma.effectors import arm_effector, cartesian_6d_velocity_effector

from droid.robot_ik.arm import FrankaArm


class RobotIKSolver:
    def __init__(self):
        self.relative_max_joint_delta = np.array([0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2])
        self.max_joint_delta = self.relative_max_joint_delta.max()
        self.max_gripper_delta = 0.25
        self.max_lin_delta = 0.075
        self.max_rot_delta = 0.15
        self.control_hz = 15

        self._arm = FrankaArm()
        self._physics = mjcf.Physics.from_mjcf_model(self._arm.mjcf_model)
        self._effector = arm_effector.ArmEffector(arm=self._arm, action_range_override=None, robot_name=self._arm.name)

        self._effector_model = cartesian_6d_velocity_effector.ModelParams(self._arm.wrist_site, self._arm.joints)

        self._effector_control = cartesian_6d_velocity_effector.ControlParams(
            control_timestep_seconds=1 / self.control_hz,
            max_lin_vel=self.max_lin_delta,
            max_rot_vel=self.max_rot_delta,
            joint_velocity_limits=self.relative_max_joint_delta,
            nullspace_joint_position_reference=[0] * 7,
            nullspace_gain=0.025,
            regularization_weight=1e-2,
            enable_joint_position_limits=True,
            minimum_distance_from_joint_position_limit=0.3,
            joint_position_limit_velocity_scale=0.95,
            max_cartesian_velocity_control_iterations=300,
            max_nullspace_control_iterations=300,
        )

        self._cart_effector_6d = cartesian_6d_velocity_effector.Cartesian6dVelocityEffector(
            self._arm.name, self._effector, self._effector_model, self._effector_control
        )
        self._cart_effector_6d.after_compile(self._arm.mjcf_model, self._physics)

    ### Inverse Kinematics ###
    def cartesian_velocity_to_joint_velocity(self, cartesian_velocity, robot_state):
        cartesian_delta = self.cartesian_velocity_to_delta(cartesian_velocity)
        qpos = np.array(robot_state["joint_positions"])
        qvel = np.array(robot_state["joint_velocities"])

        self._arm.update_state(self._physics, qpos, qvel)
        self._cart_effector_6d.set_control(self._physics, cartesian_delta)
        joint_delta = self._physics.bind(self._arm.actuators).ctrl.copy()
        np.any(joint_delta)

        joint_velocity = self.joint_delta_to_velocity(joint_delta)

        return joint_velocity

    ### Velocity To Delta ###
    def gripper_velocity_to_delta(self, gripper_velocity):
        gripper_vel_norm = np.linalg.norm(gripper_velocity)

        if gripper_vel_norm > 1:
            gripper_velocity = gripper_velocity / gripper_vel_norm

        gripper_delta = gripper_velocity * self.max_gripper_delta

        return gripper_delta

    def cartesian_velocity_to_delta(self, cartesian_velocity):
        if isinstance(cartesian_velocity, list):
            cartesian_velocity = np.array(cartesian_velocity)

        lin_vel, rot_vel = cartesian_velocity[:3], cartesian_velocity[3:6]

        lin_vel_norm = np.linalg.norm(lin_vel)
        rot_vel_norm = np.linalg.norm(rot_vel)

        if lin_vel_norm > 1:
            lin_vel = lin_vel / lin_vel_norm
        if rot_vel_norm > 1:
            rot_vel = rot_vel / rot_vel_norm

        lin_delta = lin_vel * self.max_lin_delta
        rot_delta = rot_vel * self.max_rot_delta

        return np.concatenate([lin_delta, rot_delta])

    def joint_velocity_to_delta(self, joint_velocity):
        if isinstance(joint_velocity, list):
            joint_velocity = np.array(joint_velocity)

        relative_max_joint_vel = self.joint_delta_to_velocity(self.relative_max_joint_delta)
        max_joint_vel_norm = (np.abs(joint_velocity) / relative_max_joint_vel).max()

        if max_joint_vel_norm > 1:
            joint_velocity = joint_velocity / max_joint_vel_norm

        joint_delta = joint_velocity * self.max_joint_delta

        return joint_delta

    ### Delta To Velocity ###
    def gripper_delta_to_velocity(self, gripper_delta):
        return gripper_delta / self.max_gripper_delta

    def cartesian_delta_to_velocity(self, cartesian_delta):
        if isinstance(cartesian_delta, list):
            cartesian_delta = np.array(cartesian_delta)

        cartesian_velocity = np.zeros_like(cartesian_delta)
        cartesian_velocity[:3] = cartesian_delta[:3] / self.max_lin_delta
        cartesian_velocity[3:6] = cartesian_delta[3:6] / self.max_rot_delta

        return cartesian_velocity

    def joint_delta_to_velocity(self, joint_delta):
        if isinstance(joint_delta, list):
            joint_delta = np.array(joint_delta)

        return joint_delta / self.max_joint_delta



================================================
FILE: droid/robot_ik/franka/fr3.xml
================================================
<mujoco>
  <compiler angle="radian" meshdir="./mesh" texturedir="./texture" />
  <asset>
    <mesh name="link0" file="link0.obj" />
    <mesh name="link1" file="link1.obj" />
    <mesh name="link2" file="link2.obj" />
    <mesh name="link3" file="link3.obj" />
    <mesh name="link4" file="link4.obj" />
    <mesh name="link5" file="link5.obj" />
    <mesh name="link6" file="link6.obj" />
    <mesh name="link7" file="link7.obj" />
    <mesh name="hand" file="hand.obj" />
    <texture name="panda" file="panda.png" type="2d" />
    <material name="panda" texture="panda" shininess=".4" specular=".4" />
    <material name="panda_white" rgba="1 1 1 1" shininess="0.4" specular="0.4" />
  </asset>
  <default>
    <velocity kv="10" ctrllimited="true" ctrlrange="-3.1415928 3.1415928" forcelimited='true'/>
    <geom solref="0.002 1" solimp="0.95 0.99 0.001" conaffinity="1" friction="1.0 0.1 0.001" contype="1" condim="6" />
  </default>
  <worldbody>
    <body name="panda_link0">
      <site name="base_site" />
      <geom type="mesh" material="panda" mesh="link0" />
      <inertial pos="-4.1018e-02 -1.4e-04 4.9974e-02" mass="6.29769e-01" fullinertia="3.15e-03 3.88e-03 4.285e-03 8.2904e-07 1.5e-04 8.2299e-06" />
      <body name="panda_link1" pos="0 0 0.333">
        <inertial pos="3.875e-03 2.081e-03 -4.762e-02" mass="4.970684" fullinertia="7.0337e-01 7.0661e-01 9.1170e-03 -1.3900e-04 6.7720e-03 1.9169e-02" />
        <joint name="panda_joint1" pos="0 0 0" axis="0 0 1" limited="true" range="-2.7437 2.7437" damping="0.0665" frictionloss="0.2450" />
        <geom type="mesh" material="panda_white" mesh="link1" />
        <body name="panda_link2" pos="0 0 0" quat="0.707107 -0.707107 0 0">
          <inertial pos="-3.141e-03 -2.872e-02 3.495e-03" mass="0.646926" fullinertia="7.9620e-03 2.8110e-02 2.5995e-02 -3.9250e-03 1.0254e-02 7.0400e-04" />
          <joint name="panda_joint2" pos="0 0 0" axis="0 0 1" limited="true" range="-1.7837 1.7837" damping="0.1987" frictionloss="0.1523" />
          <geom type="mesh" material="panda_white" mesh="link2" />
          <body name="panda_link3" pos="0 -0.316 0" quat="0.707107 0.707107 0 0">
            <inertial pos="2.7518e-02 3.9252e-02 -6.6502e-02" mass="3.228604" fullinertia="3.7242e-02 3.6155e-02 1.0830e-02 -4.7610e-03 -1.1396e-02 -1.2805e-02" />
            <joint name="panda_joint3" pos="0 0 0" axis="0 0 1" limited="true" range="-2.9007 2.9007" damping="0.0399" frictionloss="0.1827" />
            <geom type="mesh" material="panda" mesh="link3" />
            <body name="panda_link4" pos="0.0825 0 0" quat="0.707107 0.707107 0 0">
              <inertial pos="-5.317e-02 1.04419e-01 2.7454e-02" mass="3.587895" fullinertia="2.5853e-02 1.9552e-02 2.8323e-02 7.7960e-03 -1.3320e-03 8.6410e-03" />
              <joint name="panda_joint4" pos="0 0 0" axis="0 0 1" limited="true" range="-3.0421 -0.1518" damping="0.2257" frictionloss="0.3591" />
              <geom type="mesh" material="panda" mesh="link4" />
              <body name="panda_link5" pos="-0.0825 0.384 0" quat="0.707107 -0.707107 0 0">
                <inertial pos="1.1953e-02 4.1065e-02 -3.8437e-02" mass="1.225946" fullinertia="3.5549e-02 2.9474e-02 8.6270e-03 -2.1170e-03 -4.0370e-03 2.2900e-04" />
                <joint name="panda_joint5" pos="0 0 0" axis="0 0 1" limited="true" range="-2.8065 2.8065" damping="0.1023" frictionloss="0.2669" />
                <geom type="mesh" material="panda" mesh="link5" />
                <body name="panda_link6" pos="0 0 0" quat="0.707107 0.707107 0 0">
                  <inertial pos="6.0149e-02 -1.4117e-02 -1.0517e-02" mass="1.666555 " fullinertia="1.9640e-03 4.3540e-03 5.4330e-03 1.0900e-04 -1.1580e-03 3.4100e-04" />
                  <joint name="panda_joint6" pos="0 0 0" axis="0 0 1" limited="true" range="0.5445 4.5169" damping="-0.0132" frictionloss="0.1658" />
                  <geom type="mesh" material="panda" mesh="link6" />
                  <body name="panda_link7" pos="0.088 0 0" quat="0.707107 0.707107 0 0">
                    <inertial pos="1.0517e-02 -4.252e-03 6.1597e-02" mass="7.35522e-01" fullinertia="1.2516e-02 1.0027e-02 4.8150e-03 -4.2800e-04 -1.1960e-03 -7.4100e-04" />
                    <joint name="panda_joint7" pos="0 0 0" axis="0 0 1" limited="true" range="-3.0159 3.0159" damping="0.0638" frictionloss="1.2109" />
                    <geom type="mesh" material="panda" mesh="link7" />
                    <body name="panda_link8" pos="0 0 0.107">
                      <body name="panda_hand" euler="0 0 -0.785398163397">
                        <inertial pos="-1e-02 0 3e-02" mass="7.3e-01" diaginertia="1e-03 2.5e-03 1.7e-03" />
                        <geom type="mesh" material="panda" mesh="hand" />
                        <site name="wrist_site"/>
                      </body>
                    </body>
                  </body>
                </body>
              </body>
            </body>
          </body>
        </body>
      </body>
    </body>
  </worldbody>
  <actuator>
    <velocity forcerange="-87 87" joint="panda_joint1" name="panda_joint1" />
    <velocity forcerange="-87 87" joint="panda_joint2" name="panda_joint2" />
    <velocity forcerange="-87 87" joint="panda_joint3" name="panda_joint3" />
    <velocity forcerange="-87 87" joint="panda_joint4" name="panda_joint4" />
    <velocity forcerange="-12 12" joint="panda_joint5" name="panda_joint5" />
    <velocity forcerange="-12 12" joint="panda_joint6" name="panda_joint6" />
    <velocity forcerange="-12 12" joint="panda_joint7" name="panda_joint7" />
  </actuator>
</mujoco>



================================================
FILE: droid/robot_ik/franka/panda.xml
================================================
<mujoco>
  <compiler angle="radian" meshdir="./mesh" texturedir="./texture" />
  <asset>
    <mesh name="link0" file="link0.obj" />
    <mesh name="link1" file="link1.obj" />
    <mesh name="link2" file="link2.obj" />
    <mesh name="link3" file="link3.obj" />
    <mesh name="link4" file="link4.obj" />
    <mesh name="link5" file="link5.obj" />
    <mesh name="link6" file="link6.obj" />
    <mesh name="link7" file="link7.obj" />
    <mesh name="hand" file="hand.obj" />
    <texture name="panda" file="panda.png" type="2d" />
    <material name="panda" texture="panda" shininess=".4" specular=".4" />
    <material name="panda_white" rgba="1 1 1 1" shininess="0.4" specular="0.4" />
  </asset>
  <default>
    <velocity kv="10" ctrllimited="true" ctrlrange="-3.1415928 3.1415928" forcelimited='true'/>
    <geom solref="0.002 1" solimp="0.95 0.99 0.001" conaffinity="1" friction="1.0 0.1 0.001" contype="1" condim="6" />
  </default>
  <worldbody>
    <body name="panda_link0">
      <site name="base_site" />
      <geom type="mesh" material="panda" mesh="link0" />
      <inertial pos="-4.1018e-02 -1.4e-04 4.9974e-02" mass="6.29769e-01" fullinertia="3.15e-03 3.88e-03 4.285e-03 8.2904e-07 1.5e-04 8.2299e-06" />
      <body name="panda_link1" pos="0 0 0.333">
        <inertial pos="3.875e-03 2.081e-03 -4.762e-02" mass="4.970684" fullinertia="7.0337e-01 7.0661e-01 9.1170e-03 -1.3900e-04 6.7720e-03 1.9169e-02" />
        <joint name="panda_joint1" pos="0 0 0" axis="0 0 1" limited="true" range="-2.8973 2.8973" damping="0.0665" frictionloss="0.2450" />
        <geom type="mesh" material="panda_white" mesh="link1" />
        <body name="panda_link2" pos="0 0 0" quat="0.707107 -0.707107 0 0">
          <inertial pos="-3.141e-03 -2.872e-02 3.495e-03" mass="0.646926" fullinertia="7.9620e-03 2.8110e-02 2.5995e-02 -3.9250e-03 1.0254e-02 7.0400e-04" />
          <joint name="panda_joint2" pos="0 0 0" axis="0 0 1" limited="true" range="-1.7628 1.7628" damping="0.1987" frictionloss="0.1523" />
          <geom type="mesh" material="panda_white" mesh="link2" />
          <body name="panda_link3" pos="0 -0.316 0" quat="0.707107 0.707107 0 0">
            <inertial pos="2.7518e-02 3.9252e-02 -6.6502e-02" mass="3.228604" fullinertia="3.7242e-02 3.6155e-02 1.0830e-02 -4.7610e-03 -1.1396e-02 -1.2805e-02" />
            <joint name="panda_joint3" pos="0 0 0" axis="0 0 1" limited="true" range="-2.8973 2.8973" damping="0.0399" frictionloss="0.1827" />
            <geom type="mesh" material="panda" mesh="link3" />
            <body name="panda_link4" pos="0.0825 0 0" quat="0.707107 0.707107 0 0">
              <inertial pos="-5.317e-02 1.04419e-01 2.7454e-02" mass="3.587895" fullinertia="2.5853e-02 1.9552e-02 2.8323e-02 7.7960e-03 -1.3320e-03 8.6410e-03" />
              <joint name="panda_joint4" pos="0 0 0" axis="0 0 1" limited="true" range="-3.0718 -0.0698" damping="0.2257" frictionloss="0.3591" />
              <geom type="mesh" material="panda" mesh="link4" />
              <body name="panda_link5" pos="-0.0825 0.384 0" quat="0.707107 -0.707107 0 0">
                <inertial pos="1.1953e-02 4.1065e-02 -3.8437e-02" mass="1.225946" fullinertia="3.5549e-02 2.9474e-02 8.6270e-03 -2.1170e-03 -4.0370e-03 2.2900e-04" />
                <joint name="panda_joint5" pos="0 0 0" axis="0 0 1" limited="true" range="-2.8973 2.8973" damping="0.1023" frictionloss="0.2669" />
                <geom type="mesh" material="panda" mesh="link5" />
                <body name="panda_link6" pos="0 0 0" quat="0.707107 0.707107 0 0">
                  <inertial pos="6.0149e-02 -1.4117e-02 -1.0517e-02" mass="1.666555 " fullinertia="1.9640e-03 4.3540e-03 5.4330e-03 1.0900e-04 -1.1580e-03 3.4100e-04" />
                  <joint name="panda_joint6" pos="0 0 0" axis="0 0 1" limited="true" range="-0.0175 3.7525" damping="-0.0132" frictionloss="0.1658" />
                  <geom type="mesh" material="panda" mesh="link6" />
                  <body name="panda_link7" pos="0.088 0 0" quat="0.707107 0.707107 0 0">
                    <inertial pos="1.0517e-02 -4.252e-03 6.1597e-02" mass="7.35522e-01" fullinertia="1.2516e-02 1.0027e-02 4.8150e-03 -4.2800e-04 -1.1960e-03 -7.4100e-04" />
                    <joint name="panda_joint7" pos="0 0 0" axis="0 0 1" limited="true" range="-2.8973 2.8973" damping="0.0638" frictionloss="1.2109" />
                    <geom type="mesh" material="panda" mesh="link7" />
                    <body name="panda_link8" pos="0 0 0.107">
                      <body name="panda_hand" euler="0 0 -0.785398163397">
                        <inertial pos="-1e-02 0 3e-02" mass="7.3e-01" diaginertia="1e-03 2.5e-03 1.7e-03" />
                        <geom type="mesh" material="panda" mesh="hand" />
                        <site name="wrist_site"/>
                      </body>
                    </body>
                  </body>
                </body>
              </body>
            </body>
          </body>
        </body>
      </body>
    </body>
  </worldbody>
  <actuator>
    <velocity forcerange="-87 87" joint="panda_joint1" name="panda_joint1" />
    <velocity forcerange="-87 87" joint="panda_joint2" name="panda_joint2" />
    <velocity forcerange="-87 87" joint="panda_joint3" name="panda_joint3" />
    <velocity forcerange="-87 87" joint="panda_joint4" name="panda_joint4" />
    <velocity forcerange="-12 12" joint="panda_joint5" name="panda_joint5" />
    <velocity forcerange="-12 12" joint="panda_joint6" name="panda_joint6" />
    <velocity forcerange="-12 12" joint="panda_joint7" name="panda_joint7" />
  </actuator>
</mujoco>



================================================
FILE: droid/training/model_trainer.py
================================================
import csv
import json
import os
import shutil
import time
from collections import OrderedDict, defaultdict

import matplotlib.pyplot as plt
import numpy as np
import torch
import torch.optim as optim
from tqdm import trange

from droid.data_loading.data_loader import create_train_test_data_loader
from droid.training.models.policy_network import ImagePolicy


def exp_launcher(variant, run_id, exp_id):
    variant["exp_name"] = os.path.join(variant["exp_name"], "run{0}/id{1}/".format(run_id, exp_id))

    # Set Random Seeds #
    torch.manual_seed(variant["seed"])
    np.random.seed(variant["seed"])

    # Set Compute Mode #
    use_gpu = variant.get("use_gpu", False)
    torch.device("cuda:0" if use_gpu else "cpu")

    # Prepare Dataset Generators #
    data_loader_kwargs = variant.get("data_loader_kwargs", {})
    data_processing_kwargs = variant.get("data_processing_kwargs", {})
    camera_kwargs = variant.get("camera_kwargs", {})
    train_dataloader, test_dataloader = create_train_test_data_loader(
        data_loader_kwargs=data_loader_kwargs, data_processing_kwargs=data_processing_kwargs, camera_kwargs=camera_kwargs
    )

    # Create Model #
    model = ImagePolicy(**variant.get("model_kwargs", {}))
    if use_gpu:
        model.cuda()

    # Create Trainer #
    trainer = ModelTrainer(
        model=model,
        train_dataloader=train_dataloader,
        test_dataloader=test_dataloader,
        exp_name=variant["exp_name"],
        variant=variant,
        **variant.get("training_kwargs", {}),
    )

    # Launch Experiment #
    trainer.train()


class ModelTrainer:
    def __init__(
        self,
        model,
        train_dataloader,
        test_dataloader,
        exp_name,
        variant,
        num_epochs=25,
        weight_decay=0.0,
        lr=1e-3,
        grad_steps_per_epoch=1000,
    ):
        self.model = model
        self.train_dataloader = iter(train_dataloader)
        self.test_dataloader = iter(test_dataloader)
        self.optimizer = None
        self.grad_steps_per_epoch = grad_steps_per_epoch
        self.weight_decay = weight_decay
        self.num_epochs = num_epochs
        self.lr = lr

        self.persistent_statistics = defaultdict(list)
        self.eval_statistics = defaultdict(list)
        self.variant = variant

        dir_path = os.path.dirname(os.path.realpath(__file__))
        self.log_dir = os.path.join(dir_path, "../../training_logs", exp_name)

    def compute_loss(self, batch, test=False):
        prefix = "test-" if test else "train-"
        loss = self.model.compute_loss(batch)
        self.eval_statistics[prefix + "Loss"].append(loss.item())
        return loss

    def save_policy(self, epoch):
        path = os.path.join(self.log_dir, "models", str(epoch) + ".pt")
        torch.save(self.model, path)

    def train_batch(self, batch):
        if self.optimizer is None:
            self.compute_loss(batch)
            params = list(self.model.parameters())
            self.optimizer = optim.Adam(params, lr=self.lr, weight_decay=self.weight_decay)

        self.optimizer.zero_grad()
        loss = self.compute_loss(batch)

        loss.backward()
        self.optimizer.step()

    def test_batch(self, batch):
        self.compute_loss(batch, test=True)

    def train_epoch(self, epoch):
        start_time = time.time()
        self.model.train()

        # Train On Batches #
        training_procedure = trange(self.grad_steps_per_epoch)
        training_procedure.set_description("Epoch {0}: Training".format(epoch))

        for _i in training_procedure:
            batch = next(self.train_dataloader)
            self.train_batch(batch)

        self.eval_statistics["train-epoch_duration"].append(time.time() - start_time)

    def test_epoch(self, epoch, test_batches=100):
        start_time = time.time()
        self.model.eval()

        # Test On Batches #
        testing_procedure = trange(test_batches)
        testing_procedure.set_description("Epoch {0}: Testing".format(epoch))

        for _i in testing_procedure:
            batch = next(self.test_dataloader)
            self.test_batch(batch)

        self.eval_statistics["test-epoch_duration"].append(time.time() - start_time)

    def prepare_logdir(self):
        if os.path.exists(self.log_dir):
            response = input("Directory Exists - Enter 'overwrite' to continue\n")
            if response == "overwrite":
                shutil.rmtree(self.log_dir)
            else:
                raise RuntimeError

        os.makedirs(self.log_dir)
        os.makedirs(self.log_dir + "graphs/")
        os.makedirs(self.log_dir + "models/")

        with open(self.log_dir + "variant.json", "w") as outfile:
            json.dump(self.variant, outfile)

    def output_diagnostics(self, epoch):
        stats = OrderedDict()
        for k in sorted(self.eval_statistics.keys()):
            stats[k] = np.mean(self.eval_statistics[k])
            self.persistent_statistics[k + "/mean"].append(stats[k])
            self.persistent_statistics[k + "/std"].append(np.std(self.eval_statistics[k]))

        self.update_plots(epoch)

        if epoch == 0:
            with open(self.log_dir + "progress.csv", "w", newline="") as f:
                writer = csv.DictWriter(f, fieldnames=stats.keys())
                writer.writeheader()

        with open(self.log_dir + "progress.csv", "a", newline="") as f:
            writer = csv.DictWriter(f, fieldnames=stats.keys())
            writer.writerow(stats)

        self.eval_statistics = defaultdict(list)
        np.save(self.log_dir + "logs.npy", self.persistent_statistics)

        print("\nEPOCH: ", epoch)
        for k, v in stats.items():
            spacing = ":" + " " * (30 - len(k))
            print(k + spacing + str(round(v, 5)))

    def update_plots(self, epoch):
        x_axis = np.arange(epoch + 1)
        for k in sorted(self.eval_statistics.keys()):
            plt.clf()
            mean = np.array(self.persistent_statistics[k + "/mean"])
            std = np.array(self.persistent_statistics[k + "/std"])
            plt.plot(x_axis, mean, color="blue")
            plt.fill_between(x_axis, mean - std, mean + std, facecolor="blue", alpha=0.5)
            plt.title(k)
            plt.savefig(self.log_dir + "graphs/{0}.png".format(k))

    def train(self):
        self.prepare_logdir()

        for epoch in range(self.num_epochs):
            self.test_epoch(epoch)
            self.train_epoch(epoch)
            self.save_policy(epoch)
            self.output_diagnostics(epoch)



================================================
FILE: droid/training/models/policy_network.py
================================================
import torch
import torch.utils.data
from torch import nn
from torch.nn import functional as F


class Residual(nn.Module):
    def __init__(self, in_channels, num_hiddens, num_residual_hiddens):
        super(Residual, self).__init__()
        self._block = nn.Sequential(
            nn.ReLU(True),
            nn.Conv2d(
                in_channels=in_channels,
                out_channels=num_residual_hiddens,
                kernel_size=3,
                stride=1,
                padding=1,
                bias=False,
            ),
            nn.ReLU(True),
            nn.Conv2d(in_channels=num_residual_hiddens, out_channels=num_hiddens, kernel_size=1, stride=1, bias=False),
        )

    def forward(self, x):
        return x + self._block(x)


class ResidualStack(nn.Module):
    def __init__(self, in_channels, num_hiddens, num_residual_layers, num_residual_hiddens):
        super(ResidualStack, self).__init__()
        self._num_residual_layers = num_residual_layers
        self._layers = nn.ModuleList(
            [Residual(in_channels, num_hiddens, num_residual_hiddens) for _ in range(self._num_residual_layers)]
        )

    def forward(self, x):
        for i in range(self._num_residual_layers):
            x = self._layers[i](x)
        return F.relu(x)


class Encoder(nn.Module):
    def __init__(self, in_channels, num_hiddens, num_residual_layers, num_residual_hiddens, embedding_dim):
        super(Encoder, self).__init__()

        self._conv_1 = nn.Conv2d(
            in_channels=in_channels, out_channels=num_hiddens // 2, kernel_size=4, stride=2, padding=1
        )
        self._conv_2 = nn.Conv2d(
            in_channels=num_hiddens // 2, out_channels=num_hiddens, kernel_size=4, stride=2, padding=1
        )
        self._conv_3 = nn.Conv2d(in_channels=num_hiddens, out_channels=num_hiddens, kernel_size=3, stride=1, padding=1)
        self._residual_stack = ResidualStack(
            in_channels=num_hiddens,
            num_hiddens=num_hiddens,
            num_residual_layers=num_residual_layers,
            num_residual_hiddens=num_residual_hiddens,
        )
        self._final_conv = nn.Conv2d(in_channels=num_hiddens, out_channels=embedding_dim, kernel_size=1, stride=1)

    def forward(self, inputs):
        batch_size = inputs.shape[0]

        x = self._conv_1(inputs)
        x = F.relu(x)

        x = self._conv_2(x)
        x = F.relu(x)

        x = self._conv_3(x)

        x = self._residual_stack(x)

        x = self._final_conv(x)

        return x.view(batch_size, -1)


class ImagePolicy(nn.Module):
    def __init__(
        self,
        embedding_dim=1,
        num_encoder_hiddens=128,
        num_residual_layers=3,
        num_residual_hiddens=64,
        representation_size=100,
        num_camera_layers=4,
        num_camera_hidden=400,
        num_state_layers=4,
        num_state_hidden=400,
        num_policy_layers=4,
        num_policy_hidden=400,
    ):
        super(ImagePolicy, self).__init__()

        self.representation_size = representation_size
        self.embedding_dim = embedding_dim

        self.num_encoder_hiddens = num_encoder_hiddens
        self.num_residual_layers = num_residual_layers
        self.num_residual_hiddens = num_residual_hiddens

        self.num_camera_layers = num_camera_layers
        self.num_camera_hidden = num_camera_hidden

        self.num_state_layers = num_state_layers
        self.num_state_hidden = num_state_hidden

        self.num_policy_layers = num_policy_layers
        self.num_policy_hidden = num_policy_hidden

        self.network_initialized = False
        self.loss = nn.HuberLoss()

    def create_camera_encoder(self, input_dim):
        network = nn.ModuleList([])

        encoder = Encoder(
            input_dim[1],
            self.num_encoder_hiddens,
            self.num_residual_layers,
            self.num_residual_hiddens,
            self.embedding_dim,
        )
        network.append(encoder)

        fake_input = torch.zeros(input_dim)
        output_size = encoder(fake_input).shape[1]

        fc_layers = self.create_fully_connected(
            output_size,
            self.representation_size,
            self.num_camera_layers,
            self.num_camera_hidden,
            output_activation=nn.LeakyReLU,
        )
        network.extend(fc_layers)

        return nn.Sequential(*network)

    def create_fully_connected(self, input_dim, output_dim, num_layers, num_hiddens, output_activation):
        hidden_layers = nn.ModuleList([])
        in_dim = input_dim

        for i in range(num_layers):
            if i == (num_layers - 1):
                out_dim, curr_activation = output_dim, output_activation()
            else:
                out_dim, curr_activation = num_hiddens, nn.LeakyReLU(True)

            curr_layer = nn.Linear(in_dim, out_dim)
            nn.init.xavier_uniform_(curr_layer.weight, gain=1)
            curr_layer.bias.data.uniform_(-1e-3, 1e-3)

            hidden_layers.append(curr_layer)
            hidden_layers.append(curr_activation)

            in_dim = out_dim

        return hidden_layers

    def initialize_networks(self, timestep):
        camera_dict = timestep["observation"]["camera"]
        state = timestep["observation"]["state"]
        actions = timestep["action"]

        # Create High Dimensional Networks #
        self.camera_encoder_dict = nn.ModuleDict({})
        for obs_type in camera_dict:
            self.camera_encoder_dict[obs_type] = nn.ModuleDict({})

            for cam_type in camera_dict[obs_type]:
                self.camera_encoder_dict[obs_type][cam_type] = self.create_camera_encoder(
                    camera_dict[obs_type][cam_type][0].shape
                )

        # Create State Network #
        state_encoder_network = self.create_fully_connected(
            state.shape[1],
            self.representation_size,
            self.num_state_layers,
            self.num_state_hidden,
            output_activation=nn.LeakyReLU,
        )
        self.state_encoder_network = nn.Sequential(*state_encoder_network)

        # Create Policy Network #
        latent = self.encode_timestep(timestep)
        policy_network = self.create_fully_connected(
            latent.shape[1], actions.shape[1], self.num_policy_layers, self.num_policy_hidden, output_activation=nn.Tanh
        )
        self.policy_network = nn.Sequential(*policy_network)

        # Mark As Initialized #
        self.network_initialized = True

    def compute_loss(self, timestep):
        if not self.network_initialized:
            self.initialize_networks(timestep)

        action = self.forward(timestep)
        bc_loss = self.loss(action, timestep["action"])
        return bc_loss

    def encode_timestep(self, timestep):
        # Process Timestep #
        camera_dict = timestep["observation"]["camera"]
        state = timestep["observation"]["state"]
        state.shape[0]
        latent_list = []

        # Encode State Observations #
        if state.shape[1] > 0:
            state_latent = self.state_encoder_network(state)
            latent_list.append(state_latent)

        # Encode Camera Observations #
        sorted_obs_type_keys = sorted(camera_dict.keys())
        camera_latent = []

        for obs_type in sorted_obs_type_keys:
            sorted_cam_type_keys = sorted(camera_dict[obs_type].keys())
            obs_type_latent = []

            for cam_type in sorted_cam_type_keys:
                obs_list = camera_dict[obs_type][cam_type]
                network = self.camera_encoder_dict[obs_type][cam_type]

                # Method 1 #
                encodings = [network(data) for data in obs_list]
                curr_camera_latent = torch.cat(encodings, dim=1)

                # # # Method 2 #

                # stacked_obs = torch.cat(obs_list, dim=0)
                # stacked_latent = network(stacked_obs)
                # curr_high_dim_latent_2 = stacked_latent.reshape(batch_size, -1)

                obs_type_latent.append(curr_camera_latent)

            if len(obs_type_latent):
                obs_type_latent = torch.cat(obs_type_latent, dim=1)
                camera_latent.append(obs_type_latent)

        if len(camera_latent):
            camera_latent = torch.cat(camera_latent, dim=1)
            latent_list.append(camera_latent)

        # Return Latent #
        return torch.cat(latent_list, dim=1)

    def forward(self, timestep):
        # Encode Timestep #
        latent = self.encode_timestep(timestep)

        # Pass Through Policy #
        action = self.policy_network(latent)

        return action



================================================
FILE: droid/trajectory_utils/misc.py
================================================
import time
from collections import defaultdict
from copy import deepcopy

import cv2
import numpy as np
from PIL import Image

from droid.calibration.calibration_utils import *
from droid.camera_utils.info import camera_type_to_string_dict
from droid.camera_utils.wrappers.recorded_multi_camera_wrapper import RecordedMultiCameraWrapper
from droid.misc.parameters import *
from droid.misc.time import time_ms
from droid.misc.transformations import change_pose_frame
from droid.trajectory_utils.trajectory_reader import TrajectoryReader
from droid.trajectory_utils.trajectory_writer import TrajectoryWriter


def collect_trajectory(
    env,
    controller=None,
    policy=None,
    horizon=None,
    save_filepath=None,
    metadata=None,
    wait_for_controller=False,
    obs_pointer=None,
    save_images=False,
    recording_folderpath=False,
    randomize_reset=False,
    reset_robot=True,
):
    """
    Collects a robot trajectory.
    - If policy is None, actions will come from the controller
    - If a horizon is given, we will step the environment accordingly
    - Otherwise, we will end the trajectory when the controller tells us to
    - If you need a pointer to the current observation, pass a dictionary in for obs_pointer
    """

    # Check Parameters #
    assert (controller is not None) or (policy is not None)
    assert (controller is not None) or (horizon is not None)
    if wait_for_controller:
        assert controller is not None
    if obs_pointer is not None:
        assert isinstance(obs_pointer, dict)
    if save_images:
        assert save_filepath is not None

    # Reset States #
    if controller is not None:
        controller.reset_state()
    env.camera_reader.set_trajectory_mode()

    # Prepare Data Writers If Necesary #
    if save_filepath:
        traj_writer = TrajectoryWriter(save_filepath, metadata=metadata, save_images=save_images)
    if recording_folderpath:
        env.camera_reader.start_recording(recording_folderpath)

    # Prepare For Trajectory #
    num_steps = 0
    if reset_robot:
        env.reset(randomize=randomize_reset)

    # Begin! #
    while True:
        # Collect Miscellaneous Info #
        controller_info = {} if (controller is None) else controller.get_info()
        skip_action = wait_for_controller and (not controller_info["movement_enabled"])
        control_timestamps = {"step_start": time_ms()}

        # Get Observation #
        obs = env.get_observation()
        if obs_pointer is not None:
            obs_pointer.update(obs)
        obs["controller_info"] = controller_info
        obs["timestamp"]["skip_action"] = skip_action

        # Get Action #
        control_timestamps["policy_start"] = time_ms()
        if policy is None:
            action, controller_action_info = controller.forward(obs, include_info=True)
        else:
            action = policy.forward(obs)
            controller_action_info = {}

        # Regularize Control Frequency #
        control_timestamps["sleep_start"] = time_ms()
        comp_time = time_ms() - control_timestamps["step_start"]
        sleep_left = (1 / env.control_hz) - (comp_time / 1000)
        if sleep_left > 0:
            time.sleep(sleep_left)

        # Moniter Control Frequency #
        # moniter_control_frequency = True
        # if moniter_control_frequency:
        # 	print('Sleep Left: ', sleep_left)
        # 	print('Feasible Hz: ', (1000 / comp_time))

        # Step Environment #
        control_timestamps["control_start"] = time_ms()
        if skip_action:
            action_info = env.create_action_dict(np.zeros_like(action))
        else:
            action_info = env.step(action)
        action_info.update(controller_action_info)

        # Save Data #
        control_timestamps["step_end"] = time_ms()
        obs["timestamp"]["control"] = control_timestamps
        timestep = {"observation": obs, "action": action_info}
        if save_filepath:
            traj_writer.write_timestep(timestep)

        # Check Termination #
        num_steps += 1
        if horizon is not None:
            end_traj = horizon == num_steps
        else:
            end_traj = controller_info["success"] or controller_info["failure"]

        # Close Files And Return #
        if end_traj:
            if recording_folderpath:
                env.camera_reader.stop_recording()
            if save_filepath:
                traj_writer.close(metadata=controller_info)
            return controller_info


def calibrate_camera(
    env,
    camera_id,
    controller,
    step_size=0.01,
    pause_time=0.5,
    image_freq=10,
    obs_pointer=None,
    wait_for_controller=False,
    reset_robot=True,
):
    """Returns true if calibration was successful, otherwise returns False
    3rd Person Calibration Instructions: Press A when board in aligned with the camera from 1 foot away.
    Hand Calibration Instructions: Press A when the hand camera is aligned with the board from 1 foot away."""

    if obs_pointer is not None:
        assert isinstance(obs_pointer, dict)

    # Get Camera + Set Calibration Mode #
    camera = env.camera_reader.get_camera(camera_id)
    env.camera_reader.set_calibration_mode(camera_id)
    assert pause_time > (camera.latency / 1000)

    # Select Proper Calibration Procedure #
    hand_camera = camera.serial_number == hand_camera_id
    intrinsics_dict = camera.get_intrinsics()
    if hand_camera:
        calibrator = HandCameraCalibrator(intrinsics_dict)
    else:
        calibrator = ThirdPersonCameraCalibrator(intrinsics_dict)

    if reset_robot:
        env.reset()
    controller.reset_state()

    while True:
        # Collect Controller Info #
        controller_info = controller.get_info()
        start_time = time.time()

        # Get Observation #
        state, _ = env.get_state()
        cam_obs, _ = env.read_cameras()

        for full_cam_id in cam_obs["image"]:
            if camera_id not in full_cam_id:
                continue
            cam_obs["image"][full_cam_id] = calibrator.augment_image(full_cam_id, cam_obs["image"][full_cam_id])
        if obs_pointer is not None:
            obs_pointer.update(cam_obs)

        # Get Action #
        action = controller.forward({"robot_state": state})
        action[-1] = 0  # Keep gripper open

        # Regularize Control Frequency #
        comp_time = time.time() - start_time
        sleep_left = (1 / env.control_hz) - comp_time
        if sleep_left > 0:
            time.sleep(sleep_left)

        # Step Environment #
        skip_step = wait_for_controller and (not controller_info["movement_enabled"])
        if not skip_step:
            env.step(action)

        # Check Termination #
        start_calibration = controller_info["success"]
        end_calibration = controller_info["failure"]

        # Close Files And Return #
        if start_calibration:
            break
        if end_calibration:
            return False

    # Collect Data #
    time.time()
    pose_origin = state["cartesian_position"]
    i = 0

    while True:
        # Check For Termination #
        controller_info = controller.get_info()
        if controller_info["failure"]:
            return False

        # Start #
        start_time = time.time()
        take_picture = (i % image_freq) == 0

        # Collect Observations #
        if take_picture:
            time.sleep(pause_time)
        state, _ = env.get_state()
        cam_obs, _ = env.read_cameras()

        # Add Sample + Augment Images #
        for full_cam_id in cam_obs["image"]:
            if camera_id not in full_cam_id:
                continue
            if take_picture:
                img = deepcopy(cam_obs["image"][full_cam_id])
                pose = state["cartesian_position"].copy()
                calibrator.add_sample(full_cam_id, img, pose)
            cam_obs["image"][full_cam_id] = calibrator.augment_image(full_cam_id, cam_obs["image"][full_cam_id])

        # Update Obs Pointer #
        if obs_pointer is not None:
            obs_pointer.update(cam_obs)

        # Move To Desired Next Pose #
        calib_pose = calibration_traj(i * step_size, hand_camera=hand_camera)
        desired_pose = change_pose_frame(calib_pose, pose_origin)
        action = np.concatenate([desired_pose, [0]])
        env.update_robot(action, action_space="cartesian_position", blocking=False)

        # Regularize Control Frequency #
        comp_time = time.time() - start_time
        sleep_left = (1 / env.control_hz) - comp_time
        if sleep_left > 0:
            time.sleep(sleep_left)

        # Check If Cycle Complete #
        cycle_complete = (i * step_size) >= (2 * np.pi)
        if cycle_complete:
            break
        i += 1

    # SAVE INTO A JSON
    for full_cam_id in cam_obs["image"]:
        if camera_id not in full_cam_id:
            continue
        success = calibrator.is_calibration_accurate(full_cam_id)
        if not success:
            return False
        transformation = calibrator.calibrate(full_cam_id)
        update_calibration_info(full_cam_id, transformation)

    return True


def replay_trajectory(
    env, filepath=None, assert_replayable_keys=["cartesian_position", "gripper_position", "joint_positions"]
):
    print("WARNING: STATE 'CLOSENESS' FOR REPLAYABILITY HAS NOT BEEN CALIBRATED")
    gripper_key = "gripper_velocity" if "velocity" in env.action_space else "gripper_position"

    # Prepare Trajectory Reader #
    traj_reader = TrajectoryReader(filepath, read_images=False)
    horizon = traj_reader.length()

    for i in range(horizon):
        # Get HDF5 Data #
        timestep = traj_reader.read_timestep()

        # Move To Initial Position #
        if i == 0:
            init_joint_position = timestep["observation"]["robot_state"]["joint_positions"]
            init_gripper_position = timestep["observation"]["robot_state"]["gripper_position"]
            action = np.concatenate([init_joint_position, [init_gripper_position]])
            env.update_robot(action, action_space="joint_position", blocking=True)

        # TODO: Assert Replayability #
        # robot_state = env.get_state()[0]
        # for key in assert_replayable_keys:
        # 	desired = timestep['observation']['robot_state'][key]
        # 	current = robot_state[key]
        # 	assert np.allclose(desired, current)

        # Regularize Control Frequency #
        time.sleep(1 / env.control_hz)

        # Get Action In Desired Action Space #
        arm_action = timestep["action"][env.action_space]
        gripper_action = timestep["action"][gripper_key]
        action = np.concatenate([arm_action, [gripper_action]])
        controller_info = timestep["observation"]["controller_info"]
        movement_enabled = controller_info.get("movement_enabled", True)

        # Follow Trajectory #
        if movement_enabled:
            env.step(action)


def load_trajectory(
    filepath=None,
    read_cameras=True,
    recording_folderpath=None,
    camera_kwargs={},
    remove_skipped_steps=False,
    num_samples_per_traj=None,
    num_samples_per_traj_coeff=1.5,
):
    read_hdf5_images = read_cameras and (recording_folderpath is None)
    read_recording_folderpath = read_cameras and (recording_folderpath is not None)

    traj_reader = TrajectoryReader(filepath, read_images=read_hdf5_images)
    if read_recording_folderpath:
        camera_reader = RecordedMultiCameraWrapper(recording_folderpath, camera_kwargs)

    horizon = traj_reader.length()
    timestep_list = []

    # Choose Timesteps To Save #
    if num_samples_per_traj:
        num_to_save = num_samples_per_traj
        if remove_skipped_steps:
            num_to_save = int(num_to_save * num_samples_per_traj_coeff)
        max_size = min(num_to_save, horizon)
        indices_to_save = np.sort(np.random.choice(horizon, size=max_size, replace=False))
    else:
        indices_to_save = np.arange(horizon)

    # Iterate Over Trajectory #
    for i in indices_to_save:
        # Get HDF5 Data #
        timestep = traj_reader.read_timestep(index=i)

        # If Applicable, Get Recorded Data #
        if read_recording_folderpath:
            timestamp_dict = timestep["observation"]["timestamp"]["cameras"]
            camera_type_dict = {
                k: camera_type_to_string_dict[v] for k, v in timestep["observation"]["camera_type"].items()
            }
            camera_obs = camera_reader.read_cameras(
                index=i, camera_type_dict=camera_type_dict, timestamp_dict=timestamp_dict
            )
            camera_failed = camera_obs is None

            # Add Data To Timestep If Successful #
            if camera_failed:
                break
            else:
                timestep["observation"].update(camera_obs)

        # Filter Steps #
        step_skipped = not timestep["observation"]["controller_info"].get("movement_enabled", True)
        delete_skipped_step = step_skipped and remove_skipped_steps

        # Save Filtered Timesteps #
        if delete_skipped_step:
            del timestep
        else:
            timestep_list.append(timestep)

    # Remove Extra Transitions #
    timestep_list = np.array(timestep_list)
    if (num_samples_per_traj is not None) and (len(timestep_list) > num_samples_per_traj):
        ind_to_keep = np.random.choice(len(timestep_list), size=num_samples_per_traj, replace=False)
        timestep_list = timestep_list[ind_to_keep]

    # Close Readers #
    traj_reader.close()
    if read_recording_folderpath:
        camera_reader.disable_cameras()

    # Return Data #
    return timestep_list


def visualize_timestep(timestep, max_width=1000, max_height=500, aspect_ratio=1.5, pause_time=15):
    # Process Image Data #
    obs = timestep["observation"]
    if "image" in obs:
        img_obs = obs["image"]
    elif "image" in obs["camera"]:
        img_obs = obs["camera"]["image"]
    else:
        raise ValueError

    camera_ids = sorted(img_obs.keys())
    sorted_image_list = []
    for cam_id in camera_ids:
        data = img_obs[cam_id]
        if type(data) == list:
            sorted_image_list.extend(data)
        else:
            sorted_image_list.append(data)

    # Get Ideal Number Of Rows #
    num_images = len(sorted_image_list)
    max_num_rows = int(num_images**0.5)
    for num_rows in range(max_num_rows, 0, -1):
        num_cols = num_images // num_rows
        if num_images % num_rows == 0:
            break

    # Get Per Image Shape #
    max_img_width, max_img_height = max_width // num_cols, max_height // num_rows
    if max_img_width > aspect_ratio * max_img_height:
        img_width, img_height = max_img_width, int(max_img_width / aspect_ratio)
    else:
        img_width, img_height = int(max_img_height * aspect_ratio), max_img_height

    # Fill Out Image Grid #
    img_grid = [[] for i in range(num_rows)]

    for i in range(len(sorted_image_list)):
        img = Image.fromarray(sorted_image_list[i])
        resized_img = img.resize((img_width, img_height), Image.Resampling.LANCZOS)
        img_grid[i % num_rows].append(np.array(resized_img))

    # Combine Images #
    for i in range(num_rows):
        img_grid[i] = np.hstack(img_grid[i])
    img_grid = np.vstack(img_grid)

    # Visualize Frame #
    cv2.imshow("Image Feed", img_grid)
    cv2.waitKey(pause_time)


def visualize_trajectory(
    filepath,
    recording_folderpath=None,
    remove_skipped_steps=False,
    camera_kwargs={},
    max_width=1000,
    max_height=500,
    aspect_ratio=1.5,
):
    traj_reader = TrajectoryReader(filepath, read_images=True)
    if recording_folderpath:
        if camera_kwargs is {}:
            camera_kwargs = defaultdict(lambda: {"image": True})
        camera_reader = RecordedMultiCameraWrapper(recording_folderpath, camera_kwargs)

    horizon = traj_reader.length()
    camera_failed = False

    for i in range(horizon):
        # Get HDF5 Data #
        timestep = traj_reader.read_timestep()

        # If Applicable, Get Recorded Data #
        if recording_folderpath:
            timestamp_dict = timestep["observation"]["timestamp"]["cameras"]
            camera_type_dict = {
                k: camera_type_to_string_dict[v] for k, v in timestep["observation"]["camera_type"].items()
            }
            camera_obs = camera_reader.read_cameras(
                index=i, camera_type_dict=camera_type_dict, timestamp_dict=timestamp_dict
            )
            camera_failed = camera_obs is None

            # Add Data To Timestep #
            if not camera_failed:
                timestep["observation"].update(camera_obs)

        # Filter Steps #
        step_skipped = not timestep["observation"]["controller_info"].get("movement_enabled", True)
        delete_skipped_step = step_skipped and remove_skipped_steps
        delete_step = delete_skipped_step or camera_failed
        if delete_step:
            continue

        # Get Image Info #
        assert "image" in timestep["observation"]
        img_obs = timestep["observation"]["image"]
        camera_ids = list(img_obs.keys())
        len(camera_ids)
        camera_ids.sort()

        # Visualize Timestep #
        visualize_timestep(
            timestep, max_width=max_width, max_height=max_height, aspect_ratio=aspect_ratio, pause_time=15
        )

    # Close Readers #
    traj_reader.close()
    if recording_folderpath:
        camera_reader.disable_cameras()



================================================
FILE: droid/trajectory_utils/trajectory_reader.py
================================================
import tempfile

import h5py
import imageio


def create_video_file(suffix=".mp4", byte_contents=None):
    # Create Temporary File #
    temp_file = tempfile.NamedTemporaryFile(suffix=suffix)
    filename = temp_file.name

    # If Byte Contents Provided, Write To File #
    if byte_contents is not None:
        with open(filename, "wb") as binary_file:
            binary_file.write(byte_contents)

    return filename


def get_hdf5_length(hdf5_file, keys_to_ignore=[]):
    length = None

    for key in hdf5_file.keys():
        if key in keys_to_ignore:
            continue

        curr_data = hdf5_file[key]
        if isinstance(curr_data, h5py.Group):
            curr_length = get_hdf5_length(curr_data, keys_to_ignore=keys_to_ignore)
        elif isinstance(curr_data, h5py.Dataset):
            curr_length = len(curr_data)
        else:
            raise ValueError

        if length is None:
            length = curr_length
        assert curr_length == length

    return length


def load_hdf5_to_dict(hdf5_file, index, keys_to_ignore=[]):
    data_dict = {}

    for key in hdf5_file.keys():
        if key in keys_to_ignore:
            continue

        curr_data = hdf5_file[key]
        if isinstance(curr_data, h5py.Group):
            data_dict[key] = load_hdf5_to_dict(curr_data, index, keys_to_ignore=keys_to_ignore)
        elif isinstance(curr_data, h5py.Dataset):
            data_dict[key] = curr_data[index]
        else:
            raise ValueError

    return data_dict


class TrajectoryReader:
    def __init__(self, filepath, read_images=True):
        self._hdf5_file = h5py.File(filepath, "r")
        is_video_folder = "observations/videos" in self._hdf5_file
        self._read_images = read_images and is_video_folder
        self._length = get_hdf5_length(self._hdf5_file)
        self._video_readers = {}
        self._index = 0

    def length(self):
        return self._length

    def read_timestep(self, index=None, keys_to_ignore=[]):
        # Make Sure We Read Within Range #
        if index is None:
            index = self._index
        else:
            assert not self._read_images
            self._index = index
        assert index < self._length

        # Load Low Dimensional Data #
        keys_to_ignore = [*keys_to_ignore.copy(), "videos"]
        timestep = load_hdf5_to_dict(self._hdf5_file, self._index, keys_to_ignore=keys_to_ignore)

        # Load High Dimensional Data #
        if self._read_images:
            camera_obs = self._uncompress_images()
            timestep["observations"]["image"] = camera_obs

        # Increment Read Index #
        self._index += 1

        # Return Timestep #
        return timestep

    def _uncompress_images(self):
        # WARNING: THIS FUNCTION HAS NOT BEEN TESTED. UNDEFINED BEHAVIOR FOR FAILED READING. #
        video_folder = self._hdf5_file["observations/videos"]
        camera_obs = {}

        for video_id in video_folder:
            # Create Video Reader If One Hasn't Been Made #
            if video_id not in self._video_readers:
                serialized_video = video_folder[video_id]
                filename = create_video_file(byte_contents=serialized_video)
                self._video_readers[video_id] = imageio.get_reader(filename)

            # Read Next Frame #
            camera_obs[video_id] = yield self._video_readers[video_id]
            # Future Note: Could Make Thread For Each Image Reader

        # Return Camera Observation #
        return camera_obs

    def close(self):
        self._hdf5_file.close()



================================================
FILE: droid/trajectory_utils/trajectory_writer.py
================================================
import os
import tempfile
from collections import defaultdict
from copy import deepcopy
from queue import Empty, Queue

import h5py
import imageio
import numpy as np

from droid.misc.subprocess_utils import run_threaded_command


def write_dict_to_hdf5(hdf5_file, data_dict, keys_to_ignore=["image", "depth", "pointcloud"]):
    for key in data_dict.keys():
        # Pass Over Specified Keys #
        if key in keys_to_ignore:
            continue

        # Examine Data #
        curr_data = data_dict[key]
        if type(curr_data) == list:
            curr_data = np.array(curr_data)
        dtype = type(curr_data)

        # Unwrap If Dictionary #
        if dtype == dict:
            if key not in hdf5_file:
                hdf5_file.create_group(key)
            write_dict_to_hdf5(hdf5_file[key], curr_data)
            continue

        # Make Room For Data #
        if key not in hdf5_file:
            if dtype != np.ndarray:
                dshape = ()
            else:
                dtype, dshape = curr_data.dtype, curr_data.shape
            hdf5_file.create_dataset(key, (1, *dshape), maxshape=(None, *dshape), dtype=dtype)
        else:
            hdf5_file[key].resize(hdf5_file[key].shape[0] + 1, axis=0)

        # Save Data #
        hdf5_file[key][-1] = curr_data


class TrajectoryWriter:
    def __init__(self, filepath, metadata=None, exists_ok=False, save_images=True):
        assert (not os.path.isfile(filepath)) or exists_ok
        self._filepath = filepath
        self._save_images = save_images
        self._hdf5_file = h5py.File(filepath, "w")
        self._queue_dict = defaultdict(Queue)
        self._video_writers = {}
        self._video_files = {}
        self._open = True

        # Add Metadata #
        if metadata is not None:
            self._update_metadata(metadata)

        # Start HDF5 Writer Thread #
        def hdf5_writer(data):
            return write_dict_to_hdf5(self._hdf5_file, data)

        run_threaded_command(self._write_from_queue, args=(hdf5_writer, self._queue_dict["hdf5"]))

    def write_timestep(self, timestep):
        if self._save_images:
            self._update_video_files(timestep)
        self._queue_dict["hdf5"].put(timestep)

    def _update_metadata(self, metadata):
        for key in metadata:
            self._hdf5_file.attrs[key] = deepcopy(metadata[key])

    def _write_from_queue(self, writer, queue):
        while self._open:
            try:
                data = queue.get(timeout=1)
            except Empty:
                continue
            writer(data)
            queue.task_done()

    def _update_video_files(self, timestep):
        image_dict = timestep["observations"]["image"]

        for video_id in image_dict:
            # Get Frame #
            img = image_dict[video_id]
            del image_dict[video_id]

            # Create Writer And Buffer #
            if video_id not in self._video_buffers:
                filename = self.create_video_file(video_id, ".mp4")
                self._video_writers[video_id] = imageio.get_writer(filename, macro_block_size=1)
                run_threaded_command(
                    self._write_from_queue, args=(self._video_writers[video_id].append_data, self._queue_dict[video_id])
                )

            # Add Image To Queue #
            self._queue_dict[video_id].put(img)

        del timestep["observations"]["image"]

    def create_video_file(self, video_id, suffix):
        temp_file = tempfile.NamedTemporaryFile(suffix=suffix)
        self._video_files[video_id] = temp_file
        return temp_file.name

    def close(self, metadata=None):
        # Add Metadata #
        if metadata is not None:
            self._update_metadata(metadata)

        # Finish Remaining Jobs #
        [queue.join() for queue in self._queue_dict.values()]

        # Close Video Writers #
        for video_id in self._video_writers:
            self._video_writers[video_id].close()

        # Save Serialized Videos #
        for video_id in self._video_files:
            # Create Folder #
            if "videos" not in self._hdf5_file["observations"]:
                self._hdf5_file["observations"].create_group("videos")

            # Get Serialized Video #
            self._video_files[video_id].seek(0)
            serialized_video = np.asarray(self._video_files[video_id].read())

            # Save Data #
            self._hdf5_file["observations"]["videos"].create_dataset(video_id, data=serialized_video)
            self._video_files[video_id].close()

        # Close File #
        self._hdf5_file.close()
        self._open = False



================================================
FILE: droid/user_interface/__init__.py
================================================
[Empty file]


================================================
FILE: droid/user_interface/data_collector.py
================================================
import os
import time
from copy import deepcopy
from datetime import date

import cv2
import h5py

import droid.trajectory_utils.misc as tu
from droid.calibration.calibration_utils import check_calibration_info
from droid.misc.parameters import hand_camera_id, droid_version, robot_serial_number, robot_type

# Prepare Data Folder #
dir_path = os.path.dirname(os.path.realpath(__file__))
data_dir = os.path.join(dir_path, "../../data")


class DataCollecter:
    def __init__(self, env, controller, policy=None, save_data=True, save_traj_dir=None):
        self.env = env
        self.controller = controller
        self.policy = policy

        self.last_traj_path = None
        self.traj_running = False
        self.traj_saved = False
        self.obs_pointer = {}

        # Get Camera Info #
        self.cam_ids = list(env.camera_reader.camera_dict.keys())
        self.cam_ids.sort()

        _, full_cam_ids = self.get_camera_feed()
        self.num_cameras = len(full_cam_ids)
        self.full_cam_ids = full_cam_ids
        self.advanced_calibration = False

        # Make Sure Log Directorys Exist #
        if save_traj_dir is None:
            save_traj_dir = data_dir
        self.success_logdir = os.path.join(save_traj_dir, "success", str(date.today()))
        self.failure_logdir = os.path.join(save_traj_dir, "failure", str(date.today()))
        if not os.path.isdir(self.success_logdir):
            os.makedirs(self.success_logdir)
        if not os.path.isdir(self.failure_logdir):
            os.makedirs(self.failure_logdir)
        self.save_data = save_data

    def reset_robot(self, randomize=False):
        self.env._robot.establish_connection()
        self.controller.reset_state()
        self.env.reset(randomize=randomize)

    def get_user_feedback(self):
        info = self.controller.get_info()
        return deepcopy(info)

    def enable_advanced_calibration(self):
        self.advanced_calibration = True
        self.env.camera_reader.enable_advanced_calibration()

    def disable_advanced_calibration(self):
        self.advanced_calibration = False
        self.env.camera_reader.disable_advanced_calibration()

    def set_calibration_mode(self, cam_id):
        self.env.camera_reader.set_calibration_mode(cam_id)

    def set_trajectory_mode(self):
        self.env.camera_reader.set_trajectory_mode()

    def collect_trajectory(self, info=None, practice=False, reset_robot=True):
        self.last_traj_name = time.asctime().replace(" ", "_")

        if info is None:
            info = {}
        info["time"] = self.last_traj_name
        info["robot_serial_number"] = "{0}-{1}".format(robot_type, robot_serial_number)
        info["version_number"] = droid_version

        if practice or (not self.save_data):
            save_filepath = None
            recording_folderpath = None
        else:
            if len(self.full_cam_ids) != 6:
                raise ValueError("WARNING: User is trying to collect data without all three cameras running!")
            save_filepath = os.path.join(self.failure_logdir, info["time"], "trajectory.h5")
            recording_folderpath = os.path.join(self.failure_logdir, info["time"], "recordings")
            if not os.path.isdir(recording_folderpath):
                os.makedirs(recording_folderpath)

        # Collect Trajectory #
        self.traj_running = True
        self.env._robot.establish_connection()
        controller_info = tu.collect_trajectory(
            self.env,
            controller=self.controller,
            metadata=info,
            policy=self.policy,
            obs_pointer=self.obs_pointer,
            reset_robot=reset_robot,
            recording_folderpath=recording_folderpath,
            save_filepath=save_filepath,
            wait_for_controller=True,
        )
        self.traj_running = False
        self.obs_pointer = {}

        # Sort Trajectory #
        self.traj_saved = controller_info["success"] and (save_filepath is not None)

        if self.traj_saved:
            self.last_traj_path = os.path.join(self.success_logdir, info["time"])
            os.rename(os.path.join(self.failure_logdir, info["time"]), self.last_traj_path)

    def calibrate_camera(self, cam_id, reset_robot=True):
        self.traj_running = True
        self.env._robot.establish_connection()
        success = tu.calibrate_camera(
            self.env,
            cam_id,
            controller=self.controller,
            obs_pointer=self.obs_pointer,
            wait_for_controller=True,
            reset_robot=reset_robot,
        )
        self.traj_running = False
        self.obs_pointer = {}
        return success

    def check_calibration_info(self, remove_hand_camera=False):
        info_dict = check_calibration_info(self.full_cam_ids)
        if remove_hand_camera:
            info_dict["old"] = [cam_id for cam_id in info_dict["old"] if (hand_camera_id not in cam_id)]
        return info_dict

    def get_gui_imgs(self, obs):
        all_cam_ids = list(obs["image"].keys())
        all_cam_ids.sort()

        gui_images = []
        for cam_id in all_cam_ids:
            img = cv2.cvtColor(obs["image"][cam_id], cv2.COLOR_BGRA2RGB)
            gui_images.append(img)

        return gui_images, all_cam_ids

    def get_camera_feed(self):
        if self.traj_running:
            if "image" not in self.obs_pointer:
                raise ValueError
            obs = deepcopy(self.obs_pointer)
        else:
            obs = self.env.read_cameras()[0]
        gui_images, cam_ids = self.get_gui_imgs(obs)
        return gui_images, cam_ids

    def change_trajectory_status(self, success=False):
        if (self.last_traj_path is None) or (success == self.traj_saved):
            return

        save_filepath = os.path.join(self.last_traj_path, "trajectory.h5")
        traj_file = h5py.File(save_filepath, "r+")
        traj_file.attrs["success"] = success
        traj_file.attrs["failure"] = not success
        traj_file.close()

        if success:
            new_traj_path = os.path.join(self.success_logdir, self.last_traj_name)
            os.rename(self.last_traj_path, new_traj_path)
            self.last_traj_path = new_traj_path
            self.traj_saved = True
        else:
            new_traj_path = os.path.join(self.failure_logdir, self.last_traj_name)
            os.rename(self.last_traj_path, new_traj_path)
            self.last_traj_path = new_traj_path
            self.traj_saved = False



================================================
FILE: droid/user_interface/eval_gui.py
================================================
from abc import ABC, abstractmethod
from datetime import date
import numpy as np


from tkinter import *
from droid.user_interface.gui import *
from droid.misc.time import time_ms

import customtkinter as ctk

ctk.set_appearance_mode("dark")  # Modes: system (default), light, dark
ctk.set_default_color_theme("blue")  # Themes: blue (default), dark-blue, green
ctk.DrawEngine.preferred_drawing_method = "circle_shapes"

from droid.controllers.oculus_controller import VRPolicy
from droid.robot_env import RobotEnv
from droid.user_interface.data_collector import DataCollecter

dir_path = os.path.dirname(os.path.realpath(__file__))
data_dir = os.path.join(dir_path, "../../evaluation_logs")

LAST_N_GOALS = 5
MIDDLE_COLUMN = 4
GOAL_IMAGE_ROW = 7

_DEFAULT_RESOLUTION = "1500x1200"
_ESCAPE_KEY = "<Escape>"
DEFAULT_LANG_TEXT = "Enter text for language\n conditioning"


# create a string enum for conditioning: goal and language
class Condition:
    GOAL = "goal"
    LANGUAGE = "language"


class GoalCondPolicy(ABC):
    def __init__(self):
        pass

    @abstractmethod
    def load_goal_imgs(self, img_dict):
        """
        img_dict is a dictionary of goal images,
        where the keys are the names of the cameras
        """
        pass

    @abstractmethod
    def load_lang(self, text):
        """
        text is a string for language conditioning
        """
        pass


class EvalGUI(ctk.CTk):
    # add env args param to init that gets passed to robot env but optional
    def __init__(self, policy, env=None, eval_dir=None, fullscreen=False):
        super().__init__()

        if not eval_dir:
            self.eval_traj_dir = os.path.join(data_dir, "evals", str(date.today()))
        else:
            self.eval_traj_dir = eval_dir
        if not os.path.isdir(self.eval_traj_dir):
            os.makedirs(self.eval_traj_dir)

        if not env:
            env = RobotEnv()

        controller = VRPolicy()
        robot = DataCollecter(
            env=env, controller=controller, policy=policy, save_data=False, save_traj_dir=self.eval_traj_dir
        )

        self.policy = policy
        self.geometry(_DEFAULT_RESOLUTION)
        self.attributes("-fullscreen", fullscreen)
        self.bind(_ESCAPE_KEY, lambda e: self.destroy())

        # Prepare Relevent Items #
        self.num_eval_trials = 0
        self.cam_ids = list(robot.cam_ids)
        self.camera_order = np.arange(robot.num_cameras)
        self.time_index = None
        self.robot = robot
        self.num_traj_saved = 0
        self.info = {
            "current_task": "",
            "eval_conditioning": [],  # radio button
            "user": "",
        }

        self.eval_goal_dirs = []
        # populate from past goal dirs
        self.fetch_goal_directories()
        # list all folders in eval_traj_dir
        if not os.path.isdir(self.eval_traj_dir):
            os.makedirs(self.eval_traj_dir)

        # Create Resizable Container #
        container = ctk.CTkFrame(master=self)
        container.pack(side="top", fill="both", expand=True)
        container.grid_rowconfigure(0, weight=1)
        container.grid_columnconfigure(0, weight=1)

        # Organize Frame Dict #
        self.frames = {}
        self.curr_frame = None
        for F in (
            CameraPage,
            CanRobotResetPage,
            CaptureGoal,
            ControllerOffPage,
            EnlargedImagePage,
            EvalConfigurationPage,
            RequestedBehaviorPage,
            RobotResetPage,
        ):
            self.frames[F] = F(container, self)
            self.frames[F].grid(row=0, column=0, sticky="nsew")

        post_reset_page = EvalConfigurationPage
        self.frames[CanRobotResetPage].set_next_page(post_reset_page)

        # Listen For Robot Reset #
        self.enter_presses = 0
        self.bind("<KeyPress-Return>", self.robot_reset, add="+")
        self.refresh_enter_variable()

        # Listen For Robot Controls #
        info_thread = threading.Thread(target=self.listen_for_robot_info)
        info_thread.daemon = True
        info_thread.start()

        # Update Camera Feed #
        self.camera_feed = None
        camera_thread = threading.Thread(target=self.update_camera_feed)
        camera_thread.daemon = True
        camera_thread.start()

        # Start Program! #
        self.last_frame_change = 0
        self.show_frame(EvalConfigurationPage)
        self.update_time_index()
        self.mainloop()

    def show_frame(self, frame_id, refresh_page=True, wait=False):
        if time.time() - self.last_frame_change < 0.1:
            return
        self.focus()

        self.last_frame_change = time.time()
        self.curr_frame, old_frame = self.frames[frame_id], self.curr_frame

        if hasattr(old_frame, "exit_page"):
            old_frame.exit_page()
        if hasattr(self.curr_frame, "initialize_page") and refresh_page:
            self.curr_frame.initialize_page()

        if wait:
            self.after(100, self.curr_frame.tkraise)
        else:
            self.curr_frame.tkraise()

        if hasattr(self.curr_frame, "launch_page"):
            self.after(100, self.curr_frame.launch_page)

    def swap_img_order(self, i, j):
        self.camera_order[i], self.camera_order[j] = self.camera_order[j], self.camera_order[i]

    def set_img(self, i, widget=None, width=None, height=None, use_camera_order=True):
        index = self.camera_order[i] if use_camera_order else i
        if self.camera_feed is None:
            return
        else:
            img = self.camera_feed[index]
        img = Image.fromarray(img)
        if width is not None:
            img = ImageOps.contain(img, (width, height), Image.Resampling.LANCZOS)
        img = ImageTk.PhotoImage(img)
        widget.configure(image=img)
        widget.image = img

    def get_goal_img_snapshots(self, idxs=[]):
        if self.camera_feed is None:
            return
        else:
            if not idxs:
                idxs = [i for i in range(len(self.camera_feed))]
            current_ts = time_ms()
            eval_traj_dir = f"{self.eval_traj_dir}/goals/{current_ts}"
            os.makedirs(eval_traj_dir)
            for idx in idxs:
                img = self.camera_feed[idx]
                img = Image.fromarray(img)
                # save pil image to disk
                img.save(f"{eval_traj_dir}/{self.cam_ids[idx]}.png")

            self.eval_goal_dirs.append(eval_traj_dir)
            self.frames[EvalConfigurationPage].update_goal_radio_btns()
            self.frames[EvalConfigurationPage].toggle_capture_goal()

    def fetch_goal_directories(self):
        main_goal_dir = f"{self.eval_traj_dir}/goals"
        if not os.path.isdir(main_goal_dir):
            os.makedirs(main_goal_dir)

        for folder in sorted(os.listdir(main_goal_dir)):
            # get full path of folder
            folder = os.path.join(main_goal_dir, folder)
            print(f"found goal folder: {folder}")
            self.eval_goal_dirs.append(folder)

    def update_time_index(self):
        if self.time_index is not None:
            self.time_index = (self.time_index + 1) % len(self.last_traj)
        self.after(50, self.update_time_index)

    def robot_reset(self, event):
        self.enter_presses += 1
        if self.enter_presses == 25:
            self.enter_presses = -50
            self.frames[RobotResetPage].set_home_frame(type(self.curr_frame))
            self.show_frame(RobotResetPage)

    def refresh_enter_variable(self):
        self.enter_presses = 0
        self.after(3000, self.refresh_enter_variable)

    def listen_for_robot_info(self):
        last_was_false = True
        controller_on = True
        while True:
            time.sleep(0.1)
            info = self.robot.get_user_feedback()
            trigger = info["success"] or info["failure"]
            if info["success"] and last_was_false:
                self.event_generate("<<KeyRelease-controllerA>>")
            if info["failure"] and last_was_false:
                self.event_generate("<<KeyRelease-controllerB>>")
            if trigger and last_was_false:
                self.event_generate("<<KeyRelease-controller>>")

            # if info["controller_on"] < controller_on:
            #     self.show_frame(ControllerOffPage)

            last_was_false = not trigger
            controller_on = info["controller_on"]

    def update_camera_feed(self, sleep=0.05):
        while True:
            try:
                self.camera_feed, self.cam_ids = self.robot.get_camera_feed()
            except:
                pass
            time.sleep(sleep)


class EvalConfigurationPage(ctk.CTkFrame):
    def __init__(self, parent, controller):
        super().__init__(parent)
        self.controller = controller

        # Update Based Off Activity #
        self.controller.bind("<KeyRelease>", self.moniter_keys, add="+")
        self.controller.bind("<ButtonRelease-1>", self.moniter_keys, add="+")

        self.grid_rowconfigure((0, 1, 2, 5, 7, 13, 14), weight=1)
        self.grid_columnconfigure((0, 1, 2, 5, 6, 7, 8), weight=1)

        # Title #
        title_lbl = ctk.CTkLabel(
            self, text="Eval Configuration", font=ctk.CTkFont(size=30, weight="bold", family="Helectiva")
        )
        title_lbl.grid(row=1, column=MIDDLE_COLUMN, sticky="W")

        pos_dict = {
            "goal conditioning": (6, MIDDLE_COLUMN),
        }
        self.conditioning_dict = defaultdict(BooleanVar)

        conditioning = ["image", "language"]
        toggle_funcs = [self.toggle_capture_goal, self.toggle_text_box]
        for i, key in enumerate(pos_dict):
            x_pos, y_pos = pos_dict[key]
            group_lbl = ctk.CTkLabel(self, text=key + ":", font=ctk.CTkFont(size=18, underline=True))
            group_lbl.grid(row=x_pos, column=y_pos, sticky="W")

            for i, key in enumerate(conditioning):
                task_ckbox = ctk.CTkCheckBox(
                    self, text=key, variable=self.conditioning_dict[key], command=toggle_funcs[i]
                )
                task_ckbox.grid(row=7 + i, column=MIDDLE_COLUMN, sticky="W")

        # Goal Conditioning #
        self.goal_dir_label = ctk.CTkLabel(
            self, text="last five goal directories" + ":", font=ctk.CTkFont(size=18, underline=True)
        )

        # create selectable ctk radio buttons for all items in self.eval_goal_dirs
        self.radio_buttons = []
        self.selected_goal_dir_idx = IntVar()
        self.update_goal_radio_btns()

        # Free Response Tasks #
        # self.lang_text_lbl = ctk.CTkLabel(self, text="Enter the text for language conditioning", font=ctk.CTkFont(size=20, underline=True))
        self.lang_text = ctk.CTkTextbox(self)
        self.lang_text.insert("0.0", text=DEFAULT_LANG_TEXT)

        self.toggle_text_box()

        # Ready Button #
        collect_btn = ctk.CTkButton(self, text="evaluate", command=self.eval_robot, corner_radius=20)
        collect_btn.grid(row=2, column=MIDDLE_COLUMN, sticky="W")

        # Create a Boolean variable to track the state of the button.
        self.controller.randomize = False

        # Create a Button widget.
        self.reset_randomize_btn = ctk.CTkSwitch(self, text="randomize reset", command=self.toggle_randomize_btn)
        self.reset_randomize_btn.grid(row=3, column=MIDDLE_COLUMN, sticky="W")

        # Create a Button widget.
        self.save_eval_trajs_btn = ctk.CTkSwitch(self, text="save evals", command=self.toggle_save_btn)
        self.save_eval_trajs_btn.grid(row=4, column=MIDDLE_COLUMN, sticky="W")
        # Practice Button #
        self.capture_goal_btn = ctk.CTkButton(
            self,
            text="capture\nnew goal",
            command=self.practice_robot,
        )

    def toggle_randomize_btn(self):
        self.controller.randomize = not self.controller.randomize

    def toggle_save_btn(self):
        self.controller.robot.save_data = not self.controller.robot.save_data

    def update_goal_radio_btns(self):
        # remove the old radio buttons
        for i in range(len(self.radio_buttons)):
            self.radio_buttons[i].grid_forget()
        self.radio_buttons = []
        for i, folder in enumerate(self.controller.eval_goal_dirs[::-1][:LAST_N_GOALS]):
            # strip off everything before ../
            folder = folder.split("../")[-1]
            # change self.selected_goal_dir_idx to the index of the selected radio button
            self.radio_buttons.append(
                ctk.CTkRadioButton(
                    self, text=folder, variable=self.selected_goal_dir_idx, value=i, command=self.goal_img_changed
                )
            )

    def goal_img_changed(self):
        if self.controller.policy is not None:
            self.controller.policy.load_goal_imgs(self.load_goal_imgs_from_dir())

    def load_goal_imgs_from_dir(self):
        goal_img_dir = self.controller.eval_goal_dirs[::-1][self.selected_goal_dir_idx.get()]
        goal_imgs = {}
        for img in os.listdir(goal_img_dir):
            # get full path of image
            img = os.path.join(goal_img_dir, img)
            # load image with key
            try:
                # for key extract the png file name
                img_key = img.split("/")[-1].split(".")[0]
                goal_imgs[img_key] = np.array(Image.open(img))
            except:
                print(f"could not load image: {img}")
        return goal_imgs

    def place_image_gc_elements(self):
        self.goal_dir_label.grid(row=GOAL_IMAGE_ROW, column=5)
        for i in range(len(self.radio_buttons)):
            self.radio_buttons[i].grid(row=GOAL_IMAGE_ROW + i + 1, column=5)
        self.capture_goal_btn.grid(row=GOAL_IMAGE_ROW + 1 + len(self.radio_buttons), column=5)

    def forget_image_gc_elements(self):
        self.goal_dir_label.grid_forget()
        for i in range(len(self.radio_buttons)):
            self.radio_buttons[i].grid_forget()
        self.capture_goal_btn.grid_forget()

    def toggle_text_box(self):
        if self.conditioning_dict["language"].get():
            self.lang_text.grid(row=8, column=5)
        else:
            self.lang_text.grid_forget()

    def toggle_capture_goal(self):
        if self.conditioning_dict["image"].get():
            self.place_image_gc_elements()
        else:
            self.forget_image_gc_elements()

    def moniter_keys(self, event):
        if self.controller.curr_frame != self:
            return

        # Toggle Camera View
        if event.keysym in ["Shift_L", "Shift_R"]:
            self.controller.frames[CameraPage].set_home_frame(SceneConfigurationPage)
            self.controller.show_frame(CameraPage, wait=True)

    def practice_robot(self):
        self.controller.frames[CaptureGoal].set_mode("practice_traj")
        self.controller.show_frame(CaptureGoal, wait=True)

    def eval_robot(self):
        # set the goal conditioning
        if self.controller.policy is not None:
            self.controller.policy.load_lang(self.lang_text.get("1.0", "end-1c"))

        if self.controller.eval_goal_dirs:
            print(f"goal img dir: {self.controller.eval_goal_dirs[::-1][self.selected_goal_dir_idx.get()]}")

        self.controller.frames[CaptureGoal].set_mode("traj")
        self.controller.show_frame(CaptureGoal, wait=True)


class CaptureGoal(ctk.CTkFrame):
    def __init__(self, parent, controller):
        super().__init__(parent)
        self.controller = controller

        self.n_rows = 1 if len(self.controller.camera_order) <= 2 else 2
        self.n_cols = math.ceil(len(self.controller.camera_order) / self.n_rows)

        # Moniter Key Events #
        self.controller.bind("<KeyRelease>", self.moniter_keys, add="+")
        self.controller.bind("<<KeyRelease-controller>>", self.moniter_keys, add="+")

        # Page Variables #
        self.title_str = StringVar()
        self.instr_str = StringVar()
        self.mode = "live"

        # Title #
        title_lbl = ctk.CTkLabel(self, textvariable=self.title_str, font=ctk.CTkFont(size=30, weight="bold"))
        title_lbl.place(relx=0.5, rely=0.02, anchor="n")

        # Instructions #
        instr_lbl = ctk.CTkLabel(self, textvariable=self.instr_str, font=ctk.CTkFont(size=24, slant="italic"))
        instr_lbl.place(relx=0.5, rely=0.06, anchor="n")

        # Timer #
        self.timer_on = False
        self.time_str = StringVar()
        self.timer = ctk.CTkButton(
            self,
            textvariable=self.time_str,
            # border_color="black",
            # border_spacing=3,
            font=ctk.CTkFont(size=40, weight="bold"),
            # border_width=10,
        )

        # Image Variables #
        self.image_boxes = []

        # Create Image Grid #
        for i in range(self.n_rows):
            self.rowconfigure(i, weight=1)
            for j in range(self.n_cols):
                if (i * self.n_cols + j) >= len(self.controller.camera_order):
                    continue
                self.columnconfigure(j, weight=1)

                # Add Image Box #
                button = tk.Button(
                    self, height=0, width=0, command=lambda idx=(i * self.n_cols + j): self.update_image_grid(idx)
                )
                button.grid(row=i, column=j, sticky="s" if self.n_rows > 1 else "")
                self.image_boxes.append(button)

                # Start Image Thread #
                camera_thread = threading.Thread(target=lambda idx=(i * self.n_cols + j): self.update_camera_feed(idx))
                camera_thread.daemon = True
                camera_thread.start()

        # if the mode is capture goal, add capture button
        self.capture_goal_btn = ctk.CTkButton(
            self,
            text="Capture",
            # highlightbackground="red",
            font=ctk.CTkFont(size=30, weight="bold"),
            # border_width=10,
            command=lambda save=False: self.controller.get_goal_img_snapshots(),
        )

        self.clicked_ids = []
        # select all cameras by default
        if self.mode != "traj":
            self.capture_goal_btn.place(relx=0.45, rely=0.55)
            for i in range(0, len(self.controller.camera_order)):
                self.update_image_grid(i)

        # Moniter Key Events only when this page is shown#
        self.controller.bind("<<KeyRelease-controllerA>>", self.press_A, add="+")
        self.controller.bind("<<KeyRelease-controllerB>>", self.press_B, add="+")

    def is_page_inactive(self):
        zoom = self.controller.frames[EnlargedImagePage]
        page_inactive = self.controller.curr_frame not in [self, zoom]
        return page_inactive

    def press_A(self, event):
        if self.is_page_inactive() or self.mode == "traj":
            return
        self.controller.get_goal_img_snapshots()

    def press_B(self, event):
        if self.is_page_inactive() or self.mode == "traj":
            return
        self.controller.show_frame(EvalConfigurationPage)

    def update_image_grid(self, i):
        if i not in self.clicked_ids:
            self.clicked_ids.append(i)
        else:
            self.clicked_ids.remove(i)

    def update_camera_feed(self, i, w_coeff=1.0, h_coeff=1.0):
        while True:
            not_active = self.controller.curr_frame != self
            not_ready = len(self.controller.camera_order) != len(self.controller.cam_ids)
            if not_active or not_ready:
                time.sleep(0.05)
                continue

            w, h = max(self.winfo_width(), 100), max(self.winfo_height(), 100)
            img_w = int(w / self.n_cols * w_coeff)
            img_h = int(h / self.n_rows * h_coeff)

            self.controller.set_img(i, widget=self.image_boxes[i], width=img_w, height=img_h)

    def moniter_keys(self, event):
        zoom = self.controller.frames[EnlargedImagePage]
        page_inactive = self.controller.curr_frame not in [self, zoom]
        if page_inactive:
            return

        shift = event.keysym in ["Shift_L", "Shift_R"]

        if self.mode == "live" and shift:
            self.controller.show_frame(self.home_frame, refresh_page=False)

    def initialize_page(self):
        # Clear Widges #
        self.timer.place_forget()

        # Update Text #
        if self.mode != "traj":
            self.title_str.set("Goal Conditioning")
            self.instr_str.set("press A to capture goal\n press B to go back to eval configuration page")
        else:
            self.title_str.set("Evaluating")
            if self.controller.robot.save_data:
                self.instr_str.set("press A to save eval as success\n press B to save eval as failure")
            else:
                self.instr_str.set("press A or B to exit eval")

        # Add Mode Specific Stuff #
        if "traj" in self.mode:
            self.controller.robot.reset_robot(randomize=self.controller.randomize)

            self.timer.place(relx=0.79, rely=0.01)
            self.update_timer(time.time())

            traj_thread = threading.Thread(target=self.collect_trajectory)
            traj_thread.daemon = True
            traj_thread.start()

    def collect_trajectory(self):
        info = self.controller.info.copy()
        practice = self.mode == "practice_traj"
        if self.mode != "traj":
            self.controller.robot.policy = None
        else:
            self.controller.robot.policy = self.controller.policy
        self.controller.robot.collect_trajectory(info=info, practice=practice, reset_robot=False)

        self.end_trajectory()

    def update_timer(self, start_time):
        time_passed = time.time() - start_time
        zoom = self.controller.frames[EnlargedImagePage]
        page_inactive = self.controller.curr_frame not in [self, zoom]
        hide_timer = "traj" not in self.mode
        if page_inactive or hide_timer:
            return

        minutes_str = str(int(time_passed / 60))
        curr_seconds = int(time_passed) % 60

        if curr_seconds < 10:
            seconds_str = "0{0}".format(curr_seconds)
        else:
            seconds_str = str(curr_seconds)

        if not self.controller.robot.traj_running:
            start_time = time.time()

        self.time_str.set("{0}:{1}".format(minutes_str, seconds_str))
        self.controller.after(100, lambda: self.update_timer(start_time))

    def end_trajectory(self):
        save = self.controller.robot.traj_saved
        practice = self.mode == "practice_traj"

        # Update Based Off Success / Failure #
        if practice:
            pass
        elif save:
            self.controller.num_traj_saved += 1
        else:
            self.controller.frames[RequestedBehaviorPage].keep_last_task()

        # Check For Scene Changes #
        self.controller.show_frame(CanRobotResetPage)

    def set_home_frame(self, frame):
        self.home_frame = frame

    def set_mode(self, mode):
        self.mode = mode
        if self.mode == "traj":
            self.capture_goal_btn.place_forget()
        else:
            self.capture_goal_btn.place(relx=0.45, rely=0.55)

    def edit_trajectory(self, save):
        if save:
            self.controller.robot.save_trajectory()
        else:
            self.controller.robot.delete_trajectory()
        self.controller.show_frame(RequestedBehaviorPage)



================================================
FILE: droid/user_interface/gui.py
================================================
# Tkinter Imports #
import math
import random
import threading
import time
import tkinter as tk
import webbrowser

# Functionality Imports #
from collections import defaultdict
from tkinter import *
from tkinter.font import *
from tkinter.ttk import *

import numpy as np
from PIL import Image, ImageOps, ImageTk

# Internal Imports #
from droid.camera_utils.info import get_camera_name
from droid.misc.parameters import robot_ip
from droid.user_interface.gui_parameters import *
from droid.user_interface.misc import *
from droid.user_interface.text import *


class RobotGUI(tk.Tk):
    def __init__(self, robot=None, fullscreen=False, right_controller=True):
        # Initialize #
        super().__init__()
        self.geometry("1500x1200")
        self.attributes("-fullscreen", fullscreen)
        self.bind("<Escape>", lambda e: self.destroy())
        if right_controller:
            self.oculus_controller = "right"
            self.button_a = "A"
            self.button_b = "B"
        else:
            self.oculus_controller = "left"
            self.button_a = "X"
            self.button_b = "Y"

        # Prepare Relevent Items #
        self.num_traj_saved = 0
        self.cam_ids = list(robot.cam_ids)
        self.camera_order = np.arange(robot.num_cameras)
        self.time_index = None
        self.robot = robot
        self.info = {
            "user": "",
            "fixed_tasks": [],
            "new_tasks": [],
            "current_task": "",
        }

        # Create Resizable Container #
        container = tk.Frame(self)
        container.pack(side="top", fill="both", expand=True)
        container.grid_rowconfigure(0, weight=1)
        container.grid_columnconfigure(0, weight=1)

        # Organize Frame Dict #
        self.frames = {}
        self.curr_frame = None
        for F in (
            LoginPage,
            RobotResetPage,
            CanRobotResetPage,
            ControllerOffPage,
            PreferredTasksPage,
            SceneConfigurationPage,
            CameraPage,
            EnlargedImagePage,
            RequestedBehaviorPage,
            SceneChangesPage,
            CalibrationPage,
            CalibrateCamera,
            IncompleteCalibration,
            OldCalibration,
            OldScene,
        ):
            self.frames[F] = F(container, self)
            self.frames[F].grid(row=0, column=0, sticky="nsew")

        # Listen For Robot Reset #
        self.enter_presses = 0
        self.bind("<KeyPress-Return>", self.robot_reset, add="+")
        self.refresh_enter_variable()

        # Listen For Robot Controls #
        info_thread = threading.Thread(target=self.listen_for_robot_info)
        info_thread.daemon = True
        info_thread.start()

        # Update Camera Feed #
        self.camera_feed = None
        camera_thread = threading.Thread(target=self.update_camera_feed)
        camera_thread.daemon = True
        camera_thread.start()

        # Start Program! #
        self.last_frame_change = 0
        self.show_frame(LoginPage)
        self.update_time_index()
        self.mainloop()

    def show_frame(self, frame_id, refresh_page=True, wait=False):
        if time.time() - self.last_frame_change < 0.1:
            return
        self.focus()

        self.last_frame_change = time.time()
        self.curr_frame, old_frame = self.frames[frame_id], self.curr_frame

        if hasattr(old_frame, "exit_page"):
            old_frame.exit_page()
        if hasattr(self.curr_frame, "initialize_page") and refresh_page:
            self.curr_frame.initialize_page()

        if wait:
            self.after(100, self.curr_frame.tkraise)
        else:
            self.curr_frame.tkraise()

        if hasattr(self.curr_frame, "launch_page"):
            self.after(100, self.curr_frame.launch_page)

    def swap_img_order(self, i, j):
        self.camera_order[i], self.camera_order[j] = self.camera_order[j], self.camera_order[i]

    def set_img(self, i, widget=None, width=None, height=None, use_camera_order=True):
        index = self.camera_order[i] if use_camera_order else i
        if self.camera_feed is None:
            return
        else:
            img = self.camera_feed[index]
        img = Image.fromarray(img)
        if width is not None:
            img = ImageOps.contain(img, (width, height), Image.Resampling.LANCZOS)
        img = ImageTk.PhotoImage(img)
        widget.configure(image=img)
        widget.image = img

    def update_time_index(self):
        if self.time_index is not None:
            self.time_index = (self.time_index + 1) % len(self.last_traj)
        self.after(50, self.update_time_index)

    def robot_reset(self, event):
        self.enter_presses += 1
        if self.enter_presses == 25:
            self.enter_presses = -50
            self.frames[RobotResetPage].set_home_frame(type(self.curr_frame))
            self.show_frame(RobotResetPage)

    def refresh_enter_variable(self):
        self.enter_presses = 0
        self.after(3000, self.refresh_enter_variable)

    def listen_for_robot_info(self):
        last_was_false = True
        controller_on = True
        while True:
            time.sleep(0.1)
            info = self.robot.get_user_feedback()
            trigger = info["success"] or info["failure"]
            if info["success"] and last_was_false:
                self.event_generate("<<KeyRelease-controllerA>>")
            if info["failure"] and last_was_false:
                self.event_generate("<<KeyRelease-controllerB>>")
            if trigger and last_was_false:
                self.event_generate("<<KeyRelease-controller>>")

            if info["controller_on"] < controller_on:
                self.show_frame(ControllerOffPage)

            last_was_false = not trigger
            controller_on = info["controller_on"]

    def update_camera_feed(self, sleep=0.05):
        while True:
            try:
                self.camera_feed, self.cam_ids = self.robot.get_camera_feed()
            except:
                pass
            time.sleep(sleep)


# Start up page
class LoginPage(tk.Frame):
    def __init__(self, parent, controller):
        super().__init__(parent)
        self.controller = controller
        self.curr_scene_id = None
        self.gui_info = load_gui_info()

        # load name, building, and scene ID

        # Title #
        title_lbl = Label(self, text="Login Page", font=Font(size=30, weight="bold"))
        title_lbl.place(relx=0.5, rely=0.05, anchor="n")

        # Warning #
        self.warning_text = StringVar()
        self.warning_text.set("Please use consistent spelling!")
        instr_lbl = Label(self, textvariable=self.warning_text, font=Font(size=20))
        instr_lbl.place(relx=0.5, rely=0.1, anchor="n")

        # Request Name #
        self.user = StringVar()
        if "user" in self.gui_info:
            self.user.set(self.gui_info["user"])
        name_lbl = Label(self, text="Full Name:", font=Font(size=15, underline=True))
        name_lbl.place(relx=0.5, rely=0.22, anchor="n")
        self.name_entry = tk.Entry(self, textvariable=self.user, width=35, font=Font(size=15))
        self.name_entry.place(relx=0.5, rely=0.25, anchor="n")

        # Request Building #
        self.building = StringVar()
        if "building" in self.gui_info:
            self.building.set(self.gui_info["building"])
        building_lbl = Label(self, text="Building:", font=Font(size=15, underline=True))
        building_lbl.place(relx=0.5, rely=0.32, anchor="n")
        self.building_entry = tk.Entry(self, textvariable=self.building, width=35, font=Font(size=15))
        self.building_entry.place(relx=0.5, rely=0.35, anchor="n")

        # New Scene Button #
        scene_change_lbl = Label(self, text="Has The Scene Changed?", font=Font(size=15, underline=True))
        scene_change_lbl.place(relx=0.5, rely=0.42, anchor="n")

        yes_btn = tk.Button(
            self,
            text="Yes",
            highlightbackground="Red",
            font=Font(size=15, weight="bold"),
            width=2,
            height=2,
            borderwidth=10,
            command=self.click_yes,
        )
        yes_btn.place(relx=0.47, rely=0.45, anchor="n")

        no_btn = tk.Button(
            self,
            text="No",
            highlightbackground="Blue",
            font=Font(size=15, weight="bold"),
            width=2,
            height=2,
            borderwidth=10,
            command=self.click_no,
        )
        no_btn.place(relx=0.53, rely=0.45, anchor="n")

        # Begin Button #
        begin_btn = tk.Button(
            self,
            text="BEGIN",
            highlightbackground="green",
            font=Font(size=30, weight="bold"),
            padx=3,
            pady=5,
            borderwidth=10,
            command=self.check_completeness,
        )
        begin_btn.place(relx=0.5, rely=0.8, anchor=CENTER)

    def click_yes(self):
        self.curr_scene_id = generate_scene_id()

    def click_no(self):
        self.curr_scene_id = self.gui_info["scene_id"]

    def check_completeness(self):
        name = self.user.get()
        building = self.building.get()
        name_num_words = len([x for x in name.split(" ") if x != ""])
        name_correct = (name_num_words >= 2) and (missing_name_text not in name)
        building_correct = len(building) >= 3
        if not name_correct:
            self.user.set(missing_name_text)
        elif not building_correct:
            self.building.set(missing_building_text)
        elif self.curr_scene_id is None:
            self.warning_text.set("Please mark the scene as new or old")
        else:
            self.controller.info["user"] = name
            self.controller.info["building"] = building
            self.controller.info["scene_id"] = self.curr_scene_id
            update_gui_info(user=name, building=building, scene_id=self.curr_scene_id)
            self.controller.show_frame(SceneConfigurationPage)


class RobotResetPage(tk.Frame):
    def __init__(self, parent, controller):
        super().__init__(parent)
        self.controller = controller

        title_lbl = Label(self, text="Resetting Robot...", font=Font(size=30, weight="bold"))
        title_lbl.pack(pady=15)

        description_lbl = Label(self, text="Please stand by :)", font=Font(size=18))
        description_lbl.pack(pady=5)

    def launch_page(self):
        self.controller.robot.reset_robot()
        self.controller.show_frame(self.home_frame)

    def set_home_frame(self, frame):
        self.home_frame = frame


class CanRobotResetPage(tk.Frame):
    def __init__(self, parent, controller):
        super().__init__(parent)
        self.controller = controller
        self.controller.bind("<<KeyRelease-controller>>", self.moniter_keys, add="+")

        self.title_str = StringVar()
        self.instr_str = StringVar()

        title_lbl = Label(self, text="Proceed With Robot Reset?", font=Font(size=30, weight="bold"))
        title_lbl.pack(pady=15)

        description_lbl = Label(self, text="Press '" + self.controller.button_a + "' when ready", font=Font(size=18))
        description_lbl.pack(pady=5)

    def set_next_page(self, page):
        self.next_page = page

    def moniter_keys(self, event):
        if self.controller.curr_frame != self:
            return
        self.controller.frames[RobotResetPage].set_home_frame(self.next_page)
        self.controller.show_frame(RobotResetPage)


class ControllerOffPage(tk.Frame):
    def __init__(self, parent, controller):
        super().__init__(parent)
        self.controller = controller
        self.controller.bind("<KeyRelease-space>", self.moniter_keys, add="+")

        title_lbl = Label(self, text="WARNING: Controller off", font=Font(size=30, weight="bold"))
        title_lbl.pack(pady=15)

        description_lbl = Label(self, text=controller_off_msg, font=Font(size=18))
        description_lbl.pack(pady=5)

    def moniter_keys(self, event):
        if self.controller.curr_frame != self:
            return
        self.controller.show_frame(LoginPage)


class CalibrationPage(tk.Frame):
    def __init__(self, parent, controller):
        super().__init__(parent)
        self.controller = controller

        # Title #
        how_to_title_lbl = Label(self, text="Calibration Hub", font=Font(size=30, weight="bold"))
        how_to_title_lbl.pack(pady=5)

        # Warning #
        instr_lbl = tk.Label(self, text=color_spectrum_explantion, font=Font(size=20, slant="italic"))
        instr_lbl.pack(pady=5)

        # How To Text #
        how_to_text_lbl = Label(self, text=how_to_calibrate_text, font=Font(size=18))
        how_to_text_lbl.pack(pady=20)

        longest_name = max([len(get_camera_name(cam_id)) for cam_id in controller.cam_ids])

        self.button_dict = {}
        for i in range(len(controller.cam_ids)):
            cam_id = controller.cam_ids[i]
            cam_name = get_camera_name(cam_id)

            camera_btn = tk.Button(
                self,
                text=cam_name,
                font=Font(size=30, weight="bold"),
                width=longest_name,
                command=lambda cam_idx=cam_id: self.calibrate_camera(cam_idx),
                borderwidth=10,
            )
            camera_btn.place(relx=0.5, rely=0.5 + i * 0.08, anchor="n")
            self.button_dict[cam_id] = camera_btn

        # Back Button #
        back_btn = tk.Button(
            self,
            text="BACK",
            highlightbackground="white",
            font=Font(size=30, weight="bold"),
            padx=3,
            pady=5,
            borderwidth=10,
            command=lambda: controller.show_frame(SceneConfigurationPage),
        )
        back_btn.place(relx=0.5, rely=0.9, anchor="n")

        # Calibration Mode Buttons #
        self.standard_btn = tk.Button(
            self,
            text="Standard Mode",
            highlightbackground="green",
            font=Font(size=30, weight="bold"),
            padx=3,
            pady=5,
            borderwidth=10,
            command=lambda: self.change_calibration_mode(False),
        )
        self.standard_btn.place(relx=0.15, rely=0.02, anchor="n")

        self.advanced_btn = tk.Button(
            self,
            text="Advanced Mode",
            highlightbackground="red",
            font=Font(size=30, weight="bold"),
            padx=3,
            pady=5,
            borderwidth=10,
            command=lambda: self.change_calibration_mode(True),
        )
        self.advanced_btn.place(relx=0.85, rely=0.02, anchor="n")

    def change_calibration_mode(self, advanced_on):
        if advanced_on:
            self.controller.robot.enable_advanced_calibration()
            self.standard_btn.configure(highlightbackground="red")
            self.advanced_btn.configure(highlightbackground="green")
        else:
            self.controller.robot.disable_advanced_calibration()
            self.standard_btn.configure(highlightbackground="green")
            self.advanced_btn.configure(highlightbackground="red")

    def calibrate_camera(self, cam_id):
        self.controller.robot.set_calibration_mode(cam_id)
        long_wait = self.controller.robot.advanced_calibration
        time.sleep(5.0 if long_wait else 0.1)

        self.controller.frames[CalibrateCamera].set_camera_id(cam_id)
        self.controller.show_frame(CalibrateCamera, wait=True)

    def initialize_page(self):
        info_dict = self.controller.robot.check_calibration_info()
        for cam_id in self.button_dict.keys():
            is_missing = any([cam_id in missing_id for missing_id in info_dict["missing"]])
            is_old = any([cam_id in old_id for old_id in info_dict["old"]])
            if is_missing:
                color = "red"
            elif is_old:
                color = "blue"
            else:
                color = "black"

            self.button_dict[cam_id].config(highlightbackground=color)

    def exit_page(self):
        if self.controller.curr_frame != self.controller.frames[CalibrateCamera]:
            self.controller.robot.set_trajectory_mode()


class IncompleteCalibration(tk.Frame):
    def __init__(self, parent, controller):
        super().__init__(parent)
        self.controller = controller

        # Title #
        how_to_title_lbl = Label(self, text="Calibration Incomplete", font=Font(size=30, weight="bold"))
        how_to_title_lbl.pack(pady=5)

        # Warning #
        instr_lbl = tk.Label(self, text=missing_calibration_text, font=Font(size=20, slant="italic"))
        instr_lbl.pack(pady=5)

        # Back Button #
        back_btn = tk.Button(
            self,
            text="CALIBRATE",
            highlightbackground="red",
            font=Font(size=30, weight="bold"),
            padx=3,
            pady=5,
            borderwidth=10,
            command=lambda: controller.show_frame(CalibrationPage),
        )
        back_btn.place(relx=0.5, rely=0.5, anchor="n")


class OldCalibration(tk.Frame):
    def __init__(self, parent, controller):
        super().__init__(parent)
        self.controller = controller

        # Title #
        how_to_title_lbl = Label(self, text="Calibration Warning", font=Font(size=30, weight="bold"))
        how_to_title_lbl.pack(pady=5)

        # Warning #
        instr_lbl = tk.Label(self, text=old_calibration_text, font=Font(size=20, slant="italic"))
        instr_lbl.pack(pady=5)

        # Back Button #
        back_btn = tk.Button(
            self,
            text="CALIBRATE",
            highlightbackground="blue",
            font=Font(size=30, weight="bold"),
            padx=3,
            pady=5,
            borderwidth=10,
            command=lambda: controller.show_frame(CalibrationPage),
        )
        back_btn.place(relx=0.4, rely=0.5, anchor="n")

        # Proceed Button #
        proceed_btn = tk.Button(
            self,
            text="PROCEED",
            highlightbackground="green",
            font=Font(size=30, weight="bold"),
            padx=3,
            pady=5,
            borderwidth=10,
            command=lambda: controller.show_frame(RequestedBehaviorPage),
        )
        proceed_btn.place(relx=0.6, rely=0.5, anchor="n")


class OldScene(tk.Frame):
    def __init__(self, parent, controller):
        super().__init__(parent)
        self.controller = controller

        # Title #
        how_to_title_lbl = Label(self, text="Scene Warning", font=Font(size=30, weight="bold"))
        how_to_title_lbl.pack(pady=5)

        # Warning #
        instr_lbl = tk.Label(self, text=old_scene_text, font=Font(size=20, slant="italic"))
        instr_lbl.pack(pady=5)

        # Back Button #
        back_btn = tk.Button(
            self,
            text="BACK",
            highlightbackground="blue",
            font=Font(size=30, weight="bold"),
            padx=3,
            pady=5,
            borderwidth=10,
            command=lambda: controller.show_frame(SceneConfigurationPage),
        )
        back_btn.place(relx=0.4, rely=0.5, anchor="n")

        # Proceed Button #
        proceed_btn = tk.Button(
            self,
            text="PROCEED",
            highlightbackground="green",
            font=Font(size=30, weight="bold"),
            padx=3,
            pady=5,
            borderwidth=10,
            command=lambda: controller.show_frame(RequestedBehaviorPage),
        )
        proceed_btn.place(relx=0.6, rely=0.5, anchor="n")


class PreferredTasksPage(tk.Frame):
    def __init__(self, parent, controller):
        super().__init__(parent)
        self.controller = controller
        self.controller.bind("<KeyRelease>", self.moniter_keys, add="+")

        # Title #
        title_lbl = Label(self, text="Preferred Tasks", font=Font(size=30, weight="bold"))
        title_lbl.place(relx=0.5, rely=0.05, anchor="n")

        # Shift Instructions #
        instr_lbl = tk.Label(self, text=preferred_task_text, font=Font(size=20, slant="italic"))
        instr_lbl.place(relx=0.5, rely=0.1, anchor="n")

        # Fixed Task Selection #
        pos_dict = {
            "Articulated Tasks": (0.05, 0.2),
            "Free Object Tasks": (0.05, 0.4),
            "Tool Usage Tasks": (0.55, 0.2),
            "Deformable Object Tasks": (0.55, 0.4),
        }

        for key in preferred_tasks.keys():
            x_pos, y_pos = pos_dict[key]
            group_lbl = tk.Label(self, text=key + ":", font=Font(size=20, underline=True))
            group_lbl.place(relx=x_pos, rely=y_pos)
            for i, task in enumerate(preferred_tasks[key]):
                task_ckbox = tk.Checkbutton(self, text=task, font=Font(size=15), variable=BooleanVar())
                task_ckbox.place(relx=x_pos + 0.01, rely=y_pos + (i + 1) * 0.04)

        # Free Response Tasks #
        notes_lbl = tk.Label(self, text="Personal Notes:", font=Font(size=20, underline=True))
        notes_lbl.place(relx=0.05, rely=0.6)

        self.notes_txt = tk.Text(self, height=15, width=65, font=Font(size=15))
        self.notes_txt.place(relx=0.05, rely=0.64)

        # Back Button #
        back_btn = tk.Button(
            self,
            text="BACK",
            highlightbackground="red",
            font=Font(size=30, weight="bold"),
            padx=3,
            pady=5,
            borderwidth=10,
            command=lambda: controller.show_frame(SceneConfigurationPage),
        )
        back_btn.place(relx=0.7, rely=0.75)

    def moniter_keys(self, event):
        if self.controller.curr_frame != self:
            return

        # Toggle Camera View
        if event.keysym in ["Shift_L", "Shift_R"]:
            self.controller.frames[CameraPage].set_home_frame(PreferredTasksPage)
            self.controller.show_frame(CameraPage, wait=True)

    def initialize_page(self):
        self.notes_txt.focus()


class SceneConfigurationPage(tk.Frame):
    def __init__(self, parent, controller):
        super().__init__(parent)
        self.controller = controller

        # Update Based Off Activity #
        self.controller.bind("<KeyRelease>", self.moniter_keys, add="+")
        self.controller.bind("<ButtonRelease-1>", self.moniter_keys, add="+")

        # Title #
        title_lbl = Label(self, text="Scene Configuration", font=Font(size=30, weight="bold"))
        title_lbl.place(relx=0.5, rely=0.05, anchor="n")

        # Button Box #
        bx, by = 0.12, 0.045
        box_lbl = tk.Button(
            self, text=" " * 25, highlightbackground="blue", font=Font(slant="italic", weight="bold"), padx=12, pady=40
        )
        box_lbl.place(relx=bx, rely=by, anchor="n")

        # Task Ideas Button #
        ideas_btn = tk.Button(self, text="Task Ideas", font=Font(weight="bold"), height=1, width=16)
        ideas_btn.bind("<Button-1>", lambda e: webbrowser.open_new(task_ideas_link))
        ideas_btn.place(relx=bx, rely=by + 0.005, anchor="n")

        # Preferred Tasks Button #
        preferred_tasks_btn = tk.Button(
            self,
            text="Preferred Tasks",
            font=Font(weight="bold"),
            height=1,
            width=16,
            command=lambda: controller.show_frame(PreferredTasksPage),
        )
        preferred_tasks_btn.place(relx=bx, rely=by + 0.035, anchor="n")

        # Franka Website Button #
        franka_btn = tk.Button(self, text="Franka Website", font=Font(weight="bold"), height=1, width=16)
        franka_btn.bind("<Button-1>", lambda e: webbrowser.open_new("https://{0}/desk/".format(robot_ip)))
        franka_btn.place(relx=bx, rely=by + 0.065, anchor="n")

        # Shift Instructions #
        instr_lbl = tk.Label(self, text=shift_text, font=Font(size=20, slant="italic"))
        instr_lbl.place(relx=0.5, rely=0.1, anchor="n")

        # Fixed Task Selection #
        self.task_dict = defaultdict(BooleanVar)
        pos_dict = {
            "Articulated Tasks": (0.005, 0.2),
            "Free Object Tasks": (0.005, 0.4),
            "Tool Usage Tasks": (0.51, 0.2),
            "Deformable Object Tasks": (0.51, 0.4),
        }

        for key in all_tasks.keys():
            x_pos, y_pos = pos_dict[key]
            group_lbl = tk.Label(self, text=key + ":", font=Font(size=20, underline=True))
            group_lbl.place(relx=x_pos, rely=y_pos)
            for i, task in enumerate(all_tasks[key]):
                task_ckbox = tk.Checkbutton(self, text=task, font=Font(size=15), variable=self.task_dict[task])
                task_ckbox.place(relx=x_pos + 0.01, rely=y_pos + (i + 1) * 0.04)

        # Free Response Tasks #
        group_lbl = tk.Label(self, text=freewrite_text, font=Font(size=20, underline=True))
        group_lbl.place(relx=0.01, rely=0.6)

        self.task_txt = tk.Text(self, height=15, width=65, font=Font(size=15))
        self.task_txt.place(relx=0.02, rely=0.64)

        # Ready Button #
        collect_btn = tk.Button(
            self,
            text="COLLECT",
            highlightbackground="green",
            font=Font(size=30, weight="bold"),
            height=1,
            width=8,
            borderwidth=10,
            command=self.finish_setup,
        )
        collect_btn.place(relx=0.75, rely=0.68)

        # Practice Button #
        practice_btn = tk.Button(
            self,
            text="PRACTICE",
            highlightbackground="red",
            font=Font(size=30, weight="bold"),
            height=1,
            width=8,
            borderwidth=10,
            command=self.practice_robot,
        )
        practice_btn.place(relx=0.75, rely=0.82)

        # New Scene Button #
        new_scene_btn = tk.Button(
            self,
            text="NEW SCENE",
            highlightbackground="black",
            font=Font(size=30, weight="bold"),
            height=1,
            width=8,
            borderwidth=10,
            command=self.mark_new_scene,
        )
        new_scene_btn.place(relx=0.55, rely=0.68)

        # Calibrate Button #
        calibrate_btn = tk.Button(
            self,
            text="CALIBRATE",
            highlightbackground="blue",
            font=Font(size=30, weight="bold"),
            height=1,
            width=8,
            borderwidth=10,
            command=lambda: self.controller.show_frame(CalibrationPage),
        )
        calibrate_btn.place(relx=0.55, rely=0.82)

    def moniter_keys(self, event):
        if self.controller.curr_frame != self:
            return

        # Toggle Camera View
        if event.keysym in ["Shift_L", "Shift_R"]:
            self.controller.frames[CameraPage].set_home_frame(SceneConfigurationPage)
            self.controller.show_frame(CameraPage, wait=True)

        # Update Fixed Tasks #
        self.controller.info["fixed_tasks"] = []
        for task, val in self.task_dict.items():
            if val.get():
                self.controller.info["fixed_tasks"].append(task)

        # Update New Tasks #
        self.controller.info["new_tasks"] = self.get_new_tasks()

    def finish_setup(self):
        # Check tasks are filled out #
        fixed_tasks = self.controller.info["fixed_tasks"]
        new_tasks = self.controller.info["new_tasks"]
        if len(fixed_tasks) + len(new_tasks) == 0:
            self.task_txt.delete("1.0", END)
            self.task_txt.insert(1.0, no_tasks_text)
            return

        # Check that cameras are calibrated #
        calib_info_dict = self.controller.robot.check_calibration_info(remove_hand_camera=True)
        if len(calib_info_dict["missing"]) > 0:
            self.controller.show_frame(IncompleteCalibration)
            return
        if len(calib_info_dict["old"]) > 0:
            self.controller.show_frame(OldCalibration)
            return

        # Check that scene isn't stale #
        last_scene_change = load_gui_info()["scene_id_timestamp"]
        stale_scene = (time.time() - last_scene_change) > 3600
        if stale_scene:
            self.controller.show_frame(OldScene)
            return

        # If everything is okay, proceed
        self.controller.show_frame(RequestedBehaviorPage)

    def mark_new_scene(self):
        new_scene_id = generate_scene_id()
        self.controller.info["scene_id"] = new_scene_id
        update_gui_info(scene_id=new_scene_id)

    def get_new_tasks(self):
        new_tasks = self.task_txt.get("1.0", END).replace("\n", "")
        new_tasks = new_tasks.replace(no_tasks_text, "").split(";")
        new_tasks = [t for t in new_tasks if (not t.isspace() and len(t))]
        return new_tasks

    def practice_robot(self):
        self.controller.frames[CameraPage].set_mode("practice_traj")
        self.controller.show_frame(CameraPage, wait=True)

    def initialize_page(self):
        self.controller.frames[CameraPage].set_mode("live")
        self.task_txt.focus()


class RequestedBehaviorPage(tk.Frame):
    def __init__(self, parent, controller):
        super().__init__(parent)
        self.controller = controller
        self.keep_task = False

        # Title #
        title_lbl = Label(self, text="Requested Behavior", font=Font(size=30, weight="bold"))
        title_lbl.place(relx=0.5, rely=0.05, anchor="n")

        # Task #
        self.task_text = StringVar()
        task_lbl = Label(self, textvariable=self.task_text, font=Font(size=30))
        task_lbl.place(relx=0.5, rely=0.4, anchor="center")

        # Instructions #
        instr_lbl = tk.Label(self, text="Press '" + self.controller.button_a + "' to begin, or '" +
                                        self.controller.button_b + "' to resample", font=Font(size=20, slant="italic"))
        instr_lbl.place(relx=0.5, rely=0.1, anchor="n")

        # Change Status Box #
        bx, by = 0.15, 0.045
        box_lbl = tk.Button(self, text=" " * 48, highlightbackground="black", pady=26)
        box_lbl.place(relx=bx, rely=by, anchor="n")

        success_btn = tk.Button(
            self,
            text="Relabel Last Trajectory As Success",
            font=Font(weight="bold"),
            highlightbackground="green",
            height=1,
            width=32,
        )
        success_btn.bind("<Button-1>", lambda e: self.change_trajectory_status(True))
        success_btn.place(relx=bx, rely=by + 0.005, anchor="n")

        failure_btn = tk.Button(
            self,
            text="Relabel Last Trajectory As Failure",
            font=Font(weight="bold"),
            highlightbackground="red",
            height=1,
            width=32,
        )
        failure_btn.bind("<Button-1>", lambda e: self.change_trajectory_status(False))
        failure_btn.place(relx=bx, rely=by + 0.035, anchor="n")

        # Resample Button #
        resample_btn = tk.Button(
            self,
            text="RESAMPLE",
            highlightbackground="blue",
            font=Font(size=30, weight="bold"),
            padx=3,
            pady=5,
            borderwidth=10,
            command=lambda: self.resample(None),
        )
        resample_btn.place(relx=0.5, rely=0.7)

        # Back Button #
        back_btn = tk.Button(
            self,
            text="BACK",
            highlightbackground="red",
            font=Font(size=30, weight="bold"),
            padx=3,
            pady=5,
            borderwidth=10,
            command=lambda: controller.show_frame(SceneConfigurationPage),
        )
        back_btn.place(relx=0.4, rely=0.7)

        # Update Based Off Activity #
        controller.bind("<<KeyRelease-controllerA>>", self.start_trajectory, add="+")
        controller.bind("<<KeyRelease-controllerB>>", self.resample, add="+")

    def change_trajectory_status(self, success):
        if self.controller.curr_frame != self:
            return
        self.controller.num_traj_saved += success - (1 - success)
        self.controller.robot.change_trajectory_status(success=success)

    def resample(self, e):
        if self.controller.curr_frame != self:
            return
        self.sample_new_task()

    def initialize_page(self):
        self.controller.frames[CameraPage].set_mode("traj")
        if not self.keep_task:
            self.sample_new_task()
        else:
            self.keep_task = False

    def sample_new_task(self):
        if np.random.uniform() < compositional_task_prob:
            task = self.sample_compositional_task()
        else:
            task = self.sample_single_task()

        self.controller.info["current_task"] = task
        self.task_text.set(task)
        self.controller.update_idletasks()

    def sample_compositional_task(self):
        comp_type = np.random.randint(4)
        tasks = [self.sample_single_task() for i in range(comp_type)]
        return compositional_tasks[comp_type](*tasks)

    def sample_single_task(self):
        fixed_tasks = self.controller.info["fixed_tasks"]
        ft_weight = np.array([self.get_task_weight(t) for t in fixed_tasks])
        ft_weight = (ft_weight / ft_weight.sum()) * (1 - new_task_prob)

        new_tasks = self.controller.info["new_tasks"]
        nt_weight = (np.ones(len(new_tasks)) / len(new_tasks)) * new_task_prob

        tasks = fixed_tasks + new_tasks
        weights = np.concatenate([ft_weight, nt_weight])

        return random.choices(tasks, weights=weights)[0]

    def get_task_weight(self, task):
        task_type = [t for t in task_weights.keys() if t in task]
        assert len(task_type) == 1
        return task_weights[task_type[0]]

    def start_trajectory(self, event):
        if self.controller.curr_frame != self:
            return
        self.controller.show_frame(CameraPage, wait=True)

    def keep_last_task(self):
        self.keep_task = True


class SceneChangesPage(tk.Frame):
    def __init__(self, parent, controller):
        super().__init__(parent)
        self.controller = controller

        # Title #
        title_lbl = Label(self, text="Requested Scene Changes", font=Font(size=30, weight="bold"))
        title_lbl.place(relx=0.5, rely=0.05, anchor="n")

        # Shift Instructions #
        instr_lbl = tk.Label(self, text=shift_text, font=Font(size=20, slant="italic"))
        instr_lbl.place(relx=0.5, rely=0.1, anchor="n")
        self.controller.bind("<KeyRelease>", self.show_camera_feed, add="+")

        # Changes #
        self.change_text = StringVar()
        change_lbl = Label(self, textvariable=self.change_text, font=Font(size=30))
        change_lbl.place(relx=0.5, rely=0.4, anchor="center")

        # Resample Button #
        resample_btn = tk.Button(
            self,
            text="RESAMPLE",
            highlightbackground="red",
            font=Font(size=30, weight="bold"),
            padx=3,
            pady=5,
            borderwidth=10,
            command=self.sample_change,
        )
        resample_btn.place(relx=0.34, rely=0.7)

        # Ready Button #
        ready_btn = tk.Button(
            self,
            text="DONE",
            highlightbackground="green",
            font=Font(size=30, weight="bold"),
            padx=3,
            pady=5,
            borderwidth=10,
            command=lambda: self.controller.show_frame(SceneConfigurationPage),
        )
        ready_btn.place(relx=0.54, rely=0.7)

    def show_camera_feed(self, event):
        if self.controller.curr_frame != self:
            return
        if event.keysym in ["Shift_L", "Shift_R"]:
            self.controller.frames[CameraPage].set_home_frame(SceneChangesPage)
            self.controller.show_frame(CameraPage, wait=True)

    def sample_change(self):
        num_traj = self.controller.num_traj_saved
        move_robot = (num_traj % move_robot_frequency == 0) and (num_traj > 0)

        if move_robot:
            curr_text = move_robot_text
        else:
            curr_text = random.choice(scene_changes)
        self.change_text.set(curr_text)
        self.controller.update_idletasks()

    def initialize_page(self):
        self.controller.frames[CameraPage].set_mode("live")
        self.sample_change()


class CameraPage(tk.Frame):
    def __init__(self, parent, controller):
        super().__init__(parent)
        self.controller = controller

        self.n_rows = 1 if len(self.controller.camera_order) <= 2 else 2
        self.n_cols = math.ceil(len(self.controller.camera_order) / self.n_rows)

        # Moniter Key Events #
        self.controller.bind("<KeyRelease>", self.moniter_keys, add="+")
        self.controller.bind("<<KeyRelease-controller>>", self.moniter_keys, add="+")

        # Page Variables #
        self.title_str = StringVar()
        self.instr_str = StringVar()
        self.mode = "live"

        # Title #
        title_lbl = Label(self, textvariable=self.title_str, font=Font(size=30, weight="bold"))
        title_lbl.place(relx=0.5, rely=0.02, anchor="n")

        # Instructions #
        instr_lbl = tk.Label(self, textvariable=self.instr_str, font=Font(size=24, slant="italic"))
        instr_lbl.place(relx=0.5, rely=0.06, anchor="n")

        # Save / Delete Buttons #
        self.save_btn = tk.Button(
            self,
            text="SAVE",
            highlightbackground="green",
            font=Font(size=30, weight="bold"),
            padx=3,
            pady=5,
            borderwidth=10,
            command=lambda save=True: self.edit_trajectory(save),
        )
        self.delete_btn = tk.Button(
            self,
            text="DELETE",
            highlightbackground="red",
            font=Font(size=30, weight="bold"),
            padx=3,
            pady=5,
            borderwidth=10,
            command=lambda save=False: self.edit_trajectory(save),
        )

        # Timer #
        self.timer_on = False
        self.time_str = StringVar()
        self.timer = tk.Button(
            self,
            textvariable=self.time_str,
            highlightbackground="black",
            font=Font(size=40, weight="bold"),
            padx=3,
            pady=5,
            borderwidth=10,
        )

        # Image Variables #
        self.clicked_id = None
        self.image_boxes = []

        # Create Image Grid #
        for i in range(self.n_rows):
            self.rowconfigure(i, weight=1)
            for j in range(self.n_cols):
                if (i * self.n_cols + j) >= len(self.controller.camera_order):
                    continue
                self.columnconfigure(j, weight=1)

                # Add Image Box #
                button = tk.Button(
                    self, height=0, width=0, command=lambda idx=(i * self.n_cols + j): self.update_image_grid(idx)
                )
                button.grid(row=i, column=j, sticky="s" if self.n_rows > 1 else "")
                self.image_boxes.append(button)

                # Start Image Thread #
                camera_thread = threading.Thread(target=lambda idx=(i * self.n_cols + j): self.update_camera_feed(idx))
                camera_thread.daemon = True
                camera_thread.start()

    def update_image_grid(self, i):
        if self.clicked_id is None:
            # Get Image Of Interest
            self.clicked_id = i
        elif self.clicked_id == i:
            # If Double Clicked, Enlarge It
            self.controller.frames[EnlargedImagePage].set_image_index(i)
            self.controller.show_frame(EnlargedImagePage, wait=True)
            self.clicked_id = None
        else:
            # If Alternate Image Clicked, Swap Them
            self.controller.swap_img_order(self.clicked_id, i)
            self.clicked_id = None

    def update_camera_feed(self, i, w_coeff=1.0, h_coeff=1.0):
        while True:
            not_active = self.controller.curr_frame != self
            not_ready = len(self.controller.camera_order) != len(self.controller.cam_ids)
            if not_active or not_ready:
                time.sleep(0.05)
                continue

            w, h = max(self.winfo_width(), 100), max(self.winfo_height(), 100)
            img_w = int(w / self.n_cols * w_coeff)
            img_h = int(h / self.n_rows * h_coeff)

            self.controller.set_img(i, widget=self.image_boxes[i], width=img_w, height=img_h)

    def moniter_keys(self, event):
        zoom = self.controller.frames[EnlargedImagePage]
        page_inactive = self.controller.curr_frame not in [self, zoom]
        if page_inactive:
            return

        shift = event.keysym in ["Shift_L", "Shift_R"]

        if self.mode == "live" and shift:
            self.controller.show_frame(self.home_frame, refresh_page=False)

    def initialize_page(self):
        # Clear Widges #
        self.save_btn.place_forget()
        self.delete_btn.place_forget()
        self.timer.place_forget()

        # Update Text #
        title = camera_page_title[self.mode]

        instr = camera_page_instr[self.mode]
        if self.controller.oculus_controller == "left":
            if self.mode == 'traj' or self.mode == 'practice_traj':
                instr = instr.replace("A", "X")
                instr = instr.replace("B", "Y")

        self.title_str.set(title)
        self.instr_str.set(instr)

        # Add Mode Specific Stuff #
        if "traj" in self.mode:
            self.controller.robot.reset_robot(randomize=True)

            self.timer.place(relx=0.79, rely=0.01)
            self.update_timer(time.time())

            traj_thread = threading.Thread(target=self.collect_trajectory)
            traj_thread.daemon = True
            traj_thread.start()

    def collect_trajectory(self):
        info = self.controller.info.copy()
        practice = self.mode == "practice_traj"
        self.controller.robot.collect_trajectory(info=info, practice=practice, reset_robot=False)
        self.end_trajectory()

    def update_timer(self, start_time):
        time_passed = time.time() - start_time
        zoom = self.controller.frames[EnlargedImagePage]
        page_inactive = self.controller.curr_frame not in [self, zoom]
        hide_timer = "traj" not in self.mode
        if page_inactive or hide_timer:
            return

        minutes_str = str(int(time_passed / 60))
        curr_seconds = int(time_passed) % 60

        if curr_seconds < 10:
            seconds_str = "0{0}".format(curr_seconds)
        else:
            seconds_str = str(curr_seconds)

        if not self.controller.robot.traj_running:
            start_time = time.time()

        self.time_str.set("{0}:{1}".format(minutes_str, seconds_str))
        self.controller.after(100, lambda: self.update_timer(start_time))

    def end_trajectory(self):
        save = self.controller.robot.traj_saved
        practice = self.mode == "practice_traj"

        # Update Based Off Success / Failure #
        if practice:
            pass
        elif save:
            self.controller.num_traj_saved += 1
        else:
            self.controller.frames[RequestedBehaviorPage].keep_last_task()

        # Check For Scene Changes #
        num_traj = self.controller.num_traj_saved
        move_robot = (num_traj % move_robot_frequency == 0) and (num_traj > 0)
        scene_change = (np.random.uniform() < scene_change_prob) or move_robot

        # Move To Next Page
        time.sleep(0.1)  # Prevents bug where robot doesnt wait to reset
        if practice:
            post_reset_page = SceneConfigurationPage
        elif scene_change:
            post_reset_page = SceneChangesPage
        else:
            post_reset_page = RequestedBehaviorPage
        self.controller.frames[CanRobotResetPage].set_next_page(post_reset_page)
        self.controller.show_frame(CanRobotResetPage)

    def set_home_frame(self, frame):
        self.home_frame = frame

    def set_mode(self, mode):
        self.mode = mode

    def edit_trajectory(self, save):
        if save:
            self.controller.robot.save_trajectory()
        else:
            self.controller.robot.delete_trajectory()
        self.controller.show_frame(RequestedBehaviorPage)


class EnlargedImagePage(tk.Frame):
    def __init__(self, parent, controller):
        super().__init__(parent)
        self.controller = controller

        # Return When Double Clicked #
        controller.bind("<Button-1>", self.return_to_camera_grid, add="+")

        # Image Variables #
        self.image_box = Label(self)
        self.image_box.pack(fill=BOTH, expand=YES, anchor=CENTER)
        self.img_index = 0

        # Camera Feed Thread #
        camera_thread = threading.Thread(target=self.update_camera_feed)
        camera_thread.daemon = True
        camera_thread.start()

    def set_image_index(self, img_index):
        self.img_index = img_index

    def return_to_camera_grid(self, e):
        if self.controller.curr_frame != self:
            return
        self.controller.show_frame(CameraPage, refresh_page=False, wait=True)

    def update_camera_feed(self):
        while True:
            not_active = self.controller.curr_frame != self
            not_ready = len(self.controller.camera_order) != len(self.controller.cam_ids)
            if not_active or not_ready:
                time.sleep(0.05)
                continue
            w, h = max(self.winfo_width(), 250), max(self.winfo_height(), 250)
            self.controller.set_img(self.img_index, widget=self.image_box, width=w, height=h)


class CalibrateCamera(tk.Frame):
    def __init__(self, parent, controller, num_views=2):
        super().__init__(parent)
        self.controller = controller
        self.relevant_indices = []
        self.num_views = num_views
        self.live = False

        # Moniter Key Events #
        self.controller.bind("<<KeyRelease-controllerA>>", self.press_A, add="+")
        self.controller.bind("<<KeyRelease-controllerB>>", self.press_B, add="+")

        # Page Variables #
        self.title_str = StringVar()
        self.instr_str = StringVar()
        self.mode = "live"

        # Title #
        title_lbl = Label(self, textvariable=self.title_str, font=Font(size=30, weight="bold"))
        title_lbl.place(relx=0.5, rely=0.02, anchor="n")

        # Instructions #
        instr_lbl = tk.Label(self, textvariable=self.instr_str, font=Font(size=24, slant="italic"))
        instr_lbl.place(relx=0.5, rely=0.06, anchor="n")

        # Create Image Grid #
        self.image_boxes = []
        self.rowconfigure(0, weight=1)

        for i in range(self.num_views):
            self.columnconfigure(i, weight=1)
            button = tk.Button(self, height=0, width=0)
            button.grid(row=0, column=i, sticky="")
            self.image_boxes.append(button)

            camera_thread = threading.Thread(target=lambda idx=i: self.update_camera_feed(idx))
            camera_thread.daemon = True
            camera_thread.start()

        # Back Button #
        self.back_btn = tk.Button(
            self,
            text="BACK",
            highlightbackground="white",
            font=Font(size=30, weight="bold"),
            padx=3,
            pady=5,
            borderwidth=10,
            command=lambda: controller.show_frame(CalibrationPage),
        )

    def press_A(self, event):
        if self.controller.curr_frame != self:
            return
        if self.live:
            return

        self.back_btn.place_forget()
        traj_thread = threading.Thread(target=self.collect_trajectory)
        traj_thread.daemon = True
        traj_thread.start()

    def press_B(self, event):
        if self.controller.curr_frame != self:
            return
        if self.live:
            return

        self.controller.show_frame(CalibrationPage)

    def collect_trajectory(self):
        self.controller.robot.reset_robot()
        self.live = True

        cam_name = get_camera_name(self.cam_id)
        self.title_str.set("Calibrating Camera: " + cam_name)
        self.instr_str.set("Press '" + self.controller.button_a + "' to begin camera calibration, and '" +
                           self.controller.button_b + "' to terminate early")
        success = self.controller.robot.calibrate_camera(self.cam_id, reset_robot=False)

        self.end_trajectory(success)

    def end_trajectory(self, success):
        self.live = False

        if success:
            time.sleep(0.1)  # Prevents bug where robot doesnt wait to reset
            self.controller.frames[CanRobotResetPage].set_next_page(CalibrationPage)
            self.controller.show_frame(CanRobotResetPage)
        else:
            time.sleep(0.1)  # Prevents bug where robot doesnt wait to reset
            self.controller.frames[CanRobotResetPage].set_next_page(CalibrateCamera)
            self.controller.show_frame(CanRobotResetPage)
            self.title_str.set("CALIBRATION FAILED")
            self.instr_str.set("Press '" + self.controller.button_a + "' to retry, or '" + self.controller.button_b +
                               "' to go back to the calibration hub")
            self.back_btn.place(relx=0.5, rely=0.85, anchor="n")

    def set_camera_id(self, cam_id):
        cam_name = get_camera_name(cam_id)
        self.title_str.set("Camera View: " + cam_name)
        self.instr_str.set("Press '" + self.controller.button_a + "' to begin, or '" + self.controller.button_b +
                           "' to go back to the calibration hub")
        self.back_btn.place(relx=0.5, rely=0.85, anchor="n")
        self.relevant_indices = []
        self.cam_id = cam_id

        curr_cam_ids = self.controller.cam_ids.copy()
        for i in range(len(curr_cam_ids)):
            full_id = curr_cam_ids[i]
            if cam_id in full_id:
                self.relevant_indices.append(i)

    def update_camera_feed(self, i, w_coeff=1.0, h_coeff=1.0):
        while True:
            not_active = self.controller.curr_frame != self
            not_ready = len(self.relevant_indices) != self.num_views
            if not_active or not_ready:
                time.sleep(0.05)
                continue

            w, h = max(self.winfo_width(), 100), max(self.winfo_height(), 100)
            img_w = int(w / self.num_views * w_coeff)
            img_h = int(h / self.num_views * h_coeff)
            index = self.relevant_indices[i]

            self.controller.set_img(index, widget=self.image_boxes[i], width=img_w, height=img_h, use_camera_order=False)

    # def set_camera_id(self, cam_id):
    #     cam_name = get_camera_name(cam_id)
    #     self.title_str.set('Camera View: ' + cam_name)
    #     self.instr_str.set("Press 'A' to begin, or 'B' to go back to the calibration hub")
    #     self.back_btn.place(relx=0.5, rely=0.85, anchor='n')
    #     self.relevant_indices = []
    #     self.cam_id = cam_id

    #     while True:
    #         new_relevant_indices = []
    #         curr_cam_ids = self.controller.cam_ids.copy()

    #         for i in range(len(curr_cam_ids)):
    #             full_id = curr_cam_ids[i]
    #             if cam_id in full_id: new_relevant_indices.append(i)

    #         enough = len(new_relevant_indices) == self.num_views
    #         done = len(curr_cam_ids) == self.num_views
    #         if enough and done: break
    #         time.sleep(0.05)

    #     self.relevant_indices = new_relevant_indices



================================================
FILE: droid/user_interface/gui_info.json
================================================
{"user": "Ethan Foster", "building": "Gates", "scene_id_timestamp": 1686263566.9465497, "scene_id": 4823049285}


================================================
FILE: droid/user_interface/gui_parameters.py
================================================
# IMPORTANT VARIABLES #
new_task_prob = 0.1
compositional_task_prob = 0.1
scene_change_prob = 0.2
move_robot_frequency = 100

task_weights = {
    "Press button": 1,
    "Open or close hinged object": 1,
    "Open or close slidable objects": 1,
    "Turn twistable object": 1,
    "Move object into or out of container": 1,
    "Move lid on or off of container": 1,
    "Move object to a new position and orientation": 1,
    "Use cup to pour something granular": 1,
    "Use object to pick up something": 1,
    "Use cloth to clean something": 1,
    "Use object to stir something": 1,
    "Open or close curtain": 1,
    "Hang or unhang object": 1,
    "Fold, spread out, or clump object": 1,
}

# LOW PRIORITY VARIABLES #
reset_hold_time = 5

# Links #
task_ideas_link = "https://docs.google.com/document/d/1HCthaZrzdlnAxBYd4vCkXHYLTvm94SurF68xtgyJl4s/edit?usp=sharing"
debugging_link = "https://docs.google.com/document/d/1y6NJgLyHbJUwBxdUxahp6LDZ0cCGDDyIgAfMwECCU14/edit?usp=sharing"



================================================
FILE: droid/user_interface/misc.py
================================================
import json
import os
import time

import numpy as np

dir_path = os.path.dirname(os.path.realpath(__file__))
gui_info_filepath = os.path.join(dir_path, "gui_info.json")


def load_gui_info():
    if not os.path.isfile(gui_info_filepath):
        return {}
    with open(gui_info_filepath, "r") as jsonFile:
        gui_info = json.load(jsonFile)
    return gui_info


def update_gui_info(user=None, building=None, scene_id=None):
    gui_info = load_gui_info()
    if user is not None:
        gui_info["user"] = user
    if building is not None:
        gui_info["building"] = building
    if scene_id is not None:
        existing_id = gui_info.get("scene_id", None)
        if existing_id != scene_id:
            gui_info["scene_id_timestamp"] = time.time()
        gui_info["scene_id"] = scene_id

    with open(gui_info_filepath, "w") as jsonFile:
        json.dump(gui_info, jsonFile)


def generate_scene_id():
    return np.random.randint(0, 1e10)



================================================
FILE: droid/user_interface/text.py
================================================
all_tasks = {
    "Articulated Tasks": [
        "Press button (ex: light switch, elevators, microwave button)",
        "Open or close hinged object (ex: hinged door, microwave, oven, book, dryer, toilet, box)",
        "Open or close slidable objects (ex: toaster, drawers, sliding doors, dressers)",
        "Turn twistable object (ex: faucets, lamps, stove knobs)",
    ],
    "Free Object Tasks": [
        "Move object into or out of container (ex: drawer, clothes hamper, plate, trashcan, washer)",
        "Move lid on or off of container (ex: pot, cup, pill bottle)",
        "Move object to a new position and orientation (ex: grasping, relocating, flipping)",
    ],
    "Tool Usage Tasks": [
        "Use cup to pour something granular (ex: nuts, rice, dried pasta, coffee beans)",
        "Use object to pick up something (ex: spoon + almonds, spatula + bread, fork + banana)",
        "Use cloth to clean something (ex: table, window, mirror, plate)",
        "Use object to stir something (ex: almonds in a bowl, dried pasta in a pot)",
    ],
    "Deformable Object Tasks": [
        "Open or close curtain (ex: blanket on bed, shower curtain, shades)",
        "Hang or unhang object (ex: towel on hook, clothes over chair)",
        "Fold, spread out, or clump object (ex: cloth, charging cord, clothes)",
    ],
}

preferred_tasks = {
    "Articulated Tasks": [
        "Flip a lightswitch",
        "Open or close a hinged cabinet",
        "Open or close a sliding drawer",
        "Push down the lever on a toaster",
    ],
    "Free Object Tasks": [
        "Move a lid onto or off of a pot",
        "Put an object in or take an object out of a [pot, cabinet, drawer, clothes hamper]",
        "Move an object to a new position and orientation",
    ],
    "Tool Usage Tasks": [
        "Use cup to pour something granular",
        "Use a spatula to pick up an object",
        "Use a cloth to wipe a table",
        "Use a big spoon to stir a pot",
    ],
    "Deformable Object Tasks": [
        "Pull a blanket on a bed forward or backward",
        "Hang or unhang fabric (towel, clothes, etc) on a hook",
        "Fold or unfold fabric (towel, clothes, etc)",
    ],
}

compositional_tasks = [
    lambda: "Do anything you like that takes multiple steps to complete.",
    lambda t: "Do any task, and then reset the scene.\n\nSuggested task:\n* {0}".format(t),
    lambda t1, t2: "Do any two tasks consecutively.\n\nSuggested tasks:\n* {0}\n* {1}".format(t1, t2),
    lambda t1, t2, t3: "Do any three tasks consecutively.\n\nSuggested tasks:\n* {0}\n* {1}\n* {2}".format(t1, t2, t3),
]

how_to_text = (
    "1. Move the robot to a desired location (remember to unlock + relock wheels) \n\n2. Confirm that robot can reach"
    " everything of interest \n\n3. Confirm that ALL interaction objects are within ALL camera views \n\n4. Check the"
    " task categories that are doable from the CURRENT scene \n\n5. Try to come up with roughly three tasks of your own"
)

how_to_calibrate_text = (
    "Follow these steps EVERYTIME you move a camera:\n\n1. Attach the ChArUco board to the gripper when calibrating 3rd"
    " person cameras, or place it on a table when calibrating the hand camera\n\n2. Once the trajectory starts, move the"
    " gripper such that the board is clearly visible by the camera (1-2 feet away)\n\n3. Press 'A' to begin the"
    " calibration trajectory, the GUI will inform you if calibration fails\n\nTip: You can name cameras in the"
    " parameters file\n\nWarning: 'Advanced Mode' has higher calibration success, but is slow and buggy!"
)

data_collection_text = (
    "* Use as much action noise as possible, such that you can still perform the tasks\n\n* Make sure that you"
    " prioritize data collection for everything in 'Preferred Tasks'\n\n* Create tasks like those in 'Task Ideas'. Keep"
    " things simple and realistic :)\n\n* Finish trajectories in such a way that the robot can be reset (ex: nothing in"
    " gripper)\n\n* Although we want you to stick to the requested tasks, use your best judgement\n\n* Avoid setting up"
    " scenes that cover repetitive task categories\n\n* At any time, hold 'Return' for 5 seconds to reset the robot"
)

warnings_text = (
    "* DO NOT use the robot to grasp fragile items (ex: glass, eggs) \n\n* NEVER get the robot or any of its equipment"
    " wet \n\n* ALWAYS keep the mobile base on all four wheels \n\n* STAY out of camera view and refrain from talking"
    " during data collection"
)

scene_changes = [
    "Change the table height slightly (1-6 inches)",
    "Change the table height significantly (6+ inches)",
    "Move the table position and angle slightly (1-6 square inches, 1-15 degrees)",
    "Move the table position and angle significantly (6+ square inches, 15+ degrees)",
    "Move a varied camera's pose significantly and recalibrate",
    "Add an object to the scene",
    "Remove (if applicable) an object from the scene",
    "If possible, change the lighting in the room (ex: dim a light, close a window)",
]

noise_text = "Use the slider to adjust the action noise"
task_suggestions_text = "Suggestions are organized below by room"
use_checkboxes_text = "Use the checkboxes to keep track of your progress"
missing_name_text = "Enter your first AND last name"
missing_building_text = "Enter a building name"
move_robot_text = "MANDATORY: Move the robot setup to an entirely new location"
preferred_task_text = "Use the checkboxes below to keep track of your progress"
no_tasks_text = "There are no tasks to sample. Please click or enter some :)"
freewrite_text = "Enter Your Own Tasks Here (Seperate Entries With Semicolons)"
shift_text = "Press 'Shift' to toggle camera feed"
controller_off_msg = "Place it on your head to wake it up. When ready, press space to continue :)"
missing_calibration_text = "You must finish calibrating all cameras in order to proceed"
old_calibration_text = (
    "You have cameras that haven't been calibrated in over an hour. Are you sure you want to continue?"
)
old_scene_text = "You haven't marked a scene change in over an hour. Are you sure you want to continue?"
color_spectrum_explantion = (
    "Blue: Camera has not been calibrated in over an hour\n\nRed: Camera has never been calibrated"
)

camera_page_title = {
    "live": "Camera Feed",
    "traj": "*Live Trajectory*",
    "replay": "<Replaying Last Trajectory>",
    "practice_traj": "*Practice Run*",
}

camera_page_instr = {
    "live": "Click two images to swap them, or double click one image to enlarge it (click again to return)",
    "traj": "Press 'A' to mark a success, or 'B' to mark a failure",
    "replay": "Would you like to save or delete this trajectory?",
    "practice_traj": "Press 'A' or 'B' to end the trial run",
}



================================================
FILE: scripts/README.md
================================================
# Postprocessing Instructions

Running `scripts/postprocess.py` will iterate through all folders/files dumped as part of data collection, verify
that each trajectory contains the relevant information, convert all SVO files to MP4s, and then uploads all
trajectory data to Amazon S3.

This README only applies to `scripts/postprocess.py`; to get started, see the Quickstart section below. The remaining
sections provide more context about what this script is doing, as well as helpful tips for debugging common errors.

**Note:** This script tries to be smart and efficient by caching intermediate states/errors as it runs. You can trust
that script will never delete or overwrite existing files, nor will it overwrite anything in S3. However,
cache invalidation is hard -- if the script logs an error, and you fix it, it may/may not update the "errored_paths"
entry of the cache file properly. It's on you to manually fix those entries in `<lab>-cache.json` for now (or message
Sidd if you have a better solution)!

## Quickstart

To get started, make sure you're on the data collection laptop! Pull the latest version of the DROID repository;
there should be a folder `cache/postprocessing` at the repository root. If your lab has uploaded data to Google
Drive prior to August 1st, 2023, you should see a file `<lab>-cache.json` in this folder!

To upload new demonstrations:

1. Install new Python dependencies for post-processing (`pip install -e .`); should install `boto3` and `pyrallis`.

2. Read and familiarize yourself with the script `scripts/postprocess.py`, especially the arguments in the
   dataclass `DROIDUploadConfig`. Note the labs and members defined in `REGISTERED_MEMBERS`.
   + If your lab is not in the dictionary, add it (along with your first member's name + associated 8-character ID).
   + The comments above the dictionary definition walk through how to add new members (and generate IDs)!

3. Create a file `droid-credentials.json` (or whatever, just make sure **it is not tracked by git**) with the AWS
   Access Key and Secret Key that was sent to your lab leads by email.
   + The JSON file should be formatted as ```{"AccessKeyID": <ACCESS KEY>, "SecretAccessKey": <SECRET KEY>}```
   + Update `DROIDUploadConfig.credentials_json` with the path to this JSON file.
   + **If you don't have AWS Credentials:** Check the section "AWS S3 Access" below!

4. Update `DROIDUploadConfig.lab` with your unique lab identifier from `REGISTERED_MEMBERS`. Update
   `DROIDUploadConfig.data_dir` with the path to the folder containing your demonstrations. This folder is
   automatically created by the data collection GUI, and should be formatted following the
   "Expected Data Directory Structure" section below.

5. Run the script from the root of the repository `python scripts/postprocess.py`.
   + If you run into errors, or the script outputs "Errors in... [FIX IMMEDIATELY]" or "Errors in... [VERIFY]",
     consult the "Debugging Common Failures" section below.

**Note:** This script will occasionally exit with a message "EXITING TO PREVENT SVO SEGFAULT" due to a known issue with
bulk converting SVO files to MP4s. If you see this, just rerun the script (because of caching, it'll just pick up where
it left off).

---

### AWS S3 Access

One member per lab should email Sidd to get credentials to upload demonstrations to the S3 bucket. Each lab will only
get one set of credentials, to be stored on the data collection laptop; the credentials only allow for uploading to
the DROID S3 bucket (no other AWS permissions).

Make sure to check that your lab doesn't already have a set of valid credentials (the following users should already
have credentials):

```
- Sasha Khazatsky (Lab(s): IRIS, TRI)
- Joey Hejna (Lab: ILIAD)
- Marion Lepert (Lab: IPRL)
- Mohan Kumar (Lab: GuptaLab)
- Sungjae Park (Lab: CLVR)
```

If your name is on the above list, your credentials should've been sent to you (search your inbox for an email from
LastPass -- subject line should be something like `sravya.reddy@tri.global shared secure information with you`).

### Expected Data Directory Structure

This script expects that the `data_dir` containing demonstrations has the following structure. This structure is strict,
and follows the format dumped by the Data Collection script. If your data is not in this format, please reformat
prior to running the script!

```
- <data_dir>
    - success/ (Contains the user-labelled "success" trajectories)
        - <YYYY>-<MM>-<DD>/ (Corresponds to the day the trajectory was collected on)
            - <Day>_<Mon>_<DD>_<HH>:<MM>:<SS>_<YYYY>/ (Path to an individual trajectory directory)
                - trajectory.h5 (HDF5 file containing states, actions, camera data as time series)
                - recordings/
                    - SVO/ (Directory containing ZED SVO files -- one for each of the three cameras)
                        - <wrist_serial.svo>
                        - <ext1_serial.svo>
                        - <ext2_serial.svo>

    - [Optional] failure/ (Contains the user-labelled "failure" or otherwise unsuccessful trajectories)
        - .../ (Same format as above)
```

The script will complain/log any deviations from the above format... but try to keep things organized!

### Debugging Common Failures

Running `python scripts/postprocess.py` will iterate through all demonstrations in `data_dir` throwing either "hard"
or "silent" errors. Hard errors stop execution of the script until they are fixed, while "silent" errors are logged
and reported to you at the end of execution (and are stored in the `<lab>-cache.json` file).

#### Debugging "Hard" Failures

- `Unexpected Directory <PATH> -- did you accidentally nest directories?`
  + This error indicates finding a directory "<YYYY>-<MM>-<DD>/" nested in another "<YYYY>-<MM>-<DD>/" directory.
    This usually only happens if a user has accidentally moved / copied directories. Fix by moving the nested directory
    up the file tree (merging directories if necessary), or deleting the nested directory (if duplicate).

- `Invalid Directory <PATH> -- check timestamp format!`
  + This error indicates that a directory was found under `success/` or `failure/` that **does not** follow the above
    format ("<YYYY>-<MM>-<DD>"). Fix by removing the offending directory (or moving it to the appropriate place).

- `User alias <NAME> not in REGISTERED_LAB_MEMBERS or REGISTERED_ALIASES`
  + This error indicates that the given name (should be "<F>irst <Last>", case-sensitive) is not in the top-level
    `REGISTERED_LAB_MEMBERS` dictionary. If this is a new user, add them to the dictionary, following the comment
    above the dictionary definition.
  + **However:** If this is a typo or alias of an existing user (e.g., "Sasha -> Alexander", "Sdid" --> "Sidd), add this
    alias to the `REGISTERED_ALIASES` dictionary instead. **In general, avoid using multiple names for the same user!**

- `Lab <LAB> not in REGISTERED_LAB_MEMBERS`
  + This error means that `<LAB>` is not recognized. If you're a lab joining the project, add an entry to
  + `REGISTERED_LAB_MEMBERS`. Otherwise, check the `lab` parameter you set in `DROIDUploadConfig` or message Sidd.

- `Problem connection to S3 bucket; verify credentials JSON file!`
  + There's an issue connection to AWS S3; verify the credentials in the JSON file were copied correctly, otherwise
    message Sidd.

If there is any other error thrown during script execution, message Sidd to debug!

#### Debugging "Silent" Failures

These failures are logged during execution, and only reported in aggregate in the final message printed by the script.
To get individual errors, look at your lab's cache file `cache/postprocessing/<lab>-cache.json`. Under the `totals`
key, you'll see a JSON dictionary with the following:

```
{
    "totals": {
      ...

      // You want to look at this entry!
      "errored": {
        "success": <Ns> (# of trajectories from success/ subdirectory>),
        "failure": <Nf> (# of trajectories from failure/ subdirectory)
      },
    },
    ...

    // Actual error messages!
    "errored_paths": {
      "success": {
        "<Relative Path from `data_dir`>": "<ERROR MSG",
      },
      "failure": {
        "<Relative Path from `data_dir`>": "<ERROR MSG",
      }
    }
}
```

**Note that `success` and `failure` here refer to the type of demonstration (successful/unsuccesful), and NOT return
values from the postprocessing script!**

Based on the type of demonstration (`success/` or `failure/`), you might want to handle the following errors
differently. Where possible, we've outlined how each case should be handled:

- `[Indexing Error] Missing/Invalid HDF5! If the HDF5 is missing/corrupt, you can delete this trajectory!`
  + [Case: `success` OR `failure`] Make sure the `trajectory.h5` is actually missing from the directory (or is
    unreadable/corrupt). If it's unreparable, this demonstration has no useful data, so remove this directory.

- `[Indexing Error] Missing SVO Files! Ensure all 3 SVO files are in <timestamp>/recordings/SVO/<serial>.svo!`
  + [Case: `success/`] Successful trajectories should have *all 3 SVO* files present! If this is not the case, relabel
    this trajectory as a `failure/`.
  + [Case: `failure/`] Failure trajectories might not have all 3 SVO files; in this case, just keep the directory on
    disk (it will not be uploaded at this time). We'll figure out what to do about these soon!

- `[Processing Error] JSON Metadata Parse Error`
  + [Case: `success/`] This error only happens if the `trajectory.h5` file is corrupt or otherwise missing information.
    Fix the H5 file if possible, otherwise relabel this trajectory as a `failure/`.
  + [Case: `failure/`] If `trajectory.h5` is corrupt/unreadable, keep this directory on disk (it will not be uploaded).
    Similar to above, we'll figure out what to do about these soon!

- `[Processing Error] Corrupted SVO / Failed Conversion`
  + [Case: `success/`] This error happens if there's an issue with SVO conversion -- check that the SVO files are
    actually unreadable, and if so, mark as a `failure/`.
  + [Case: `failure/`] Same as above; keep on disk, but it will not be uploaded!

When you've handled *all errors* the value of `"totals" --> "success/"` in `<lab>-cache.json` should be 0!



================================================
FILE: scripts/main.py
================================================
from droid.controllers.oculus_controller import VRPolicy
from droid.robot_env import RobotEnv
from droid.user_interface.data_collector import DataCollecter
from droid.user_interface.gui import RobotGUI
import argparse

parser = argparse.ArgumentParser(description='Process a boolean argument for right_controller.')

# Adding the right_controller argument
parser.add_argument('--left_controller', action='store_true', help='Use left oculus controller')
parser.add_argument('--right_controller', action='store_true', help='Use right oculus controller')


args = parser.parse_args()

# Make the robot env
env = RobotEnv()

if args.left_controller:
    controller = VRPolicy(right_controller=False)
    # Make the data collector
    data_collector = DataCollecter(env=env, controller=controller)
    # Make the GUI
    user_interface = RobotGUI(robot=data_collector, right_controller=False)
else:
    controller = VRPolicy(right_controller=True)
    # Make the data collector
    data_collector = DataCollecter(env=env, controller=controller)
    # Make the GUI
    user_interface = RobotGUI(robot=data_collector, right_controller=True)







================================================
FILE: scripts/postprocess.py
================================================
"""
postprocess.py

Core script for processing & uploading collected demonstration data to the DROID Amazon S3 bucket.

Performs the following:
    - Checks for "cached" uploads in `DROID/cache/<lab>-cache.json; avoids repetitive work.
    - Parses out relevant metadata from each trajectory --> *errors* on "unexpected format" (fail-fast).
    - Converts all SVO files to the relevant MP4s --> logs "corrupt" data (silent).
    - Runs validation logic on all *new* demonstrations --> errors on "corrupt" data (fail-fast).
    - Writes JSON metadata for all *new* demonstrations for easy data querying.
    - Uploads each demonstration "day" data to Amazon S3 bucket and updates `DROID/cache/<lab>-cache.json`.

Note :: Must run on hardware with the ZED SDK; highly recommended to run this on the data collection laptop directly!

Run from DROID directory root with: `python scripts/postprocess.py --lab <LAB_ID>
"""
import json
import os
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, Tuple

import pyrallis

from droid.postprocessing.parse import parse_datetime
from droid.postprocessing.stages import run_indexing, run_processing, run_upload
from droid.postprocessing.util.validate import validate_user2id

# Registry of known labs and associated users. Note that we canonicalize names as "<F>irst <L>ast".
#   => We map each "name" to a unique ID (8 characters); when adding new users, make sure to pick a unique ID!
#         + Simple ID generator --> python -c "import uuid; print(str(uuid.uuid4())[:8])"
# fmt: off
REGISTERED_MEMBERS: Dict[str, Dict[str, str]] = {
    "Lab Name": {
        "First Last": "4b1a56cc",
    },
}
validate_user2id(REGISTERED_MEMBERS)

# Try to be as consistent as possible using the "canonical" names in the dictionary above; however, for unavoidable
# cases (e.g., unfortunate typos, using names/nicknames interchangeably), update the dictionary below!
REGISTERED_ALIASES: Dict[str, Tuple[str, str]] = {
    **{user: (lab, user) for lab, users in REGISTERED_MEMBERS.items() for user in users},

    # Note: Add duplicates/typos below (follow format)!
    **{
        "Firstt Last": ("Lab Name", "First Last"),
    }
}
# fmt: on


@dataclass
class DROIDUploadConfig:
    # fmt: off
    lab: str                                        # Lab ID (all uppercase) -- e.g., "CLVR", "ILIAD", "REAL"
    data_dir: Path = Path("data")                   # Path to top-level directory with "success"/"failure" directories

    # Stage Handling
    do_index: bool = True                           # Whether to run an initial indexing pass prior to processing stage
    do_process: bool = True                         # Whether to run processing (can skip if just want to upload)
    do_upload: bool = True                          # Whether to run uploading to S3

    # Important :: Only update once you're sure *all* demonstrations prior to this date have been uploaded!
    #   > If not running low on disk, leave alone!
    start_date: str = "2023-01-01"                  # Start indexing/processing/uploading demos starting from this date

    # AWS/S3 Upload Credentials
    credentials_json: Path = Path(                  # Path to JSON file with Access Key/Secret Key (don't push to git!)
        "droid-credentials.json"
    )

    # Cache Parameters
    cache_dir: Path = Path("cache/postprocessing")  # Relative path to `cache` directory; defaults to repository root
    # fmt: on


@pyrallis.wrap()
def postprocess(cfg: DROIDUploadConfig) -> None:
    print(f"[*] Starting Data Processing & Upload for Lab `{cfg.lab}`")

    # Initialize Cache Data Structure --> Load Uploaded/Processed List from `cache_dir` (if exists)
    #   => Note that in calls to each stage, cache is updated *in-place* (allows for the try/finally to work)
    cache = {
        "lab": cfg.lab,
        "start_date": cfg.start_date,
        "totals": {k: {"success": 0, "failure": 0} for k in ["scanned", "indexed", "processed", "uploaded", "errored"]},
        "scanned_paths": {"success": {}, "failure": {}},
        "indexed_uuids": {"success": {}, "failure": {}},
        "processed_uuids": {"success": {}, "failure": {}},
        "uploaded_uuids": {"success": {}, "failure": {}},
        "errored_paths": {"success": {}, "failure": {}},
    }
    os.makedirs(cfg.cache_dir, exist_ok=True)
    if (cfg.cache_dir / f"{cfg.lab}-cache.json").exists():
        with open(cfg.cache_dir / f"{cfg.lab}-cache.json", "r") as f:
            loaded_cache = json.load(f)

            # Only replace cache on matched `start_date` --> not an ideal solution (cache invalidation is hard!)
            if loaded_cache["start_date"] == cache["start_date"]:
                cache = loaded_cache

    # === Run Post-Processing Stages ===
    try:
        start_datetime = parse_datetime(cfg.start_date, mode="day")

        # Stage 1 --> "Indexing"
        if cfg.do_index:
            run_indexing(
                cfg.data_dir,
                cfg.lab,
                start_datetime,
                aliases=REGISTERED_ALIASES,
                members=REGISTERED_MEMBERS,
                totals=cache["totals"],
                scanned_paths=cache["scanned_paths"],
                indexed_uuids=cache["indexed_uuids"],
                errored_paths=cache["errored_paths"],
            )
        else:
            print("[*] Stage 1 =>> Skipping Indexing!")

        # Stage 2 --> "Processing"
        if cfg.do_process:
            run_processing(
                cfg.data_dir,
                cfg.lab,
                aliases=REGISTERED_ALIASES,
                members=REGISTERED_MEMBERS,
                totals=cache["totals"],
                indexed_uuids=cache["indexed_uuids"],
                processed_uuids=cache["processed_uuids"],
                errored_paths=cache["errored_paths"],
            )
        else:
            print("[*] Stage 2 =>> Skipping Processing!")

        # Stage 3 --> "Upload"
        if cfg.do_upload:
            run_upload(
                cfg.data_dir,
                cfg.lab,
                cfg.credentials_json,
                totals=cache["totals"],
                processed_uuids=cache["processed_uuids"],
                uploaded_uuids=cache["uploaded_uuids"],
            )
        else:
            print("[*] Stage 3 =>> Skipping Uploading!")

    finally:
        # Dump `cache` on any interruption!
        with open(cfg.cache_dir / f"{cfg.lab}-cache.json", "w") as f:
            json.dump(cache, f, indent=2)

        # Print Statistics
        print(
            "[*] Terminated Post-Processing Script --> Summary:\n"
            f"\t- Scanned:      {sum(cache['totals']['scanned'].values())}\n"
            f"\t- Indexed:      {sum(cache['totals']['indexed'].values())}\n"
            f"\t- Processed:    {sum(cache['totals']['processed'].values())}\n"
            f"\t- Uploaded:     {sum(cache['totals']['uploaded'].values())}\n"
            f"\t- Total Errors: {sum(cache['totals']['errored'].values())}\n"
            f"\t  -> Errors in `success/` [FIX IMMEDIATELY]: {cache['totals']['errored']['success']}\n"
            f"\t  -> Errors in `failure/` [VERIFY]: {cache['totals']['errored']['failure']}\n\n"
            f"[*] See `{cfg.lab}-cache.json` in {cfg.cache_dir} for more information!\n"
        )


if __name__ == "__main__":
    postprocess()



================================================
FILE: scripts/convert/svo_to_depth.py
================================================
"""
Script for extracting MP4 data from SVO files.

Performs the following:
    - Checks for "cached" uploads in `DROID/cache/<lab>-cache.json; avoids repetitive work.
    - Parses out relevant metadata from each trajectory --> *errors* on "unexpected format" (fail-fast).
    - Converts all SVO files to the relevant Depth --> logs "corrupt" data (silent).
    - Runs validation logic on all *new* demonstrations --> errors on "corrupt" data (fail-fast).
    - Writes JSON metadata for all *new* demonstrations for easy data querying.
    - Uploads each demonstration "day" data to Amazon S3 bucket and updates `DROID/cache/<lab>-cache.json`.

IMPORTANT INFORMATION:
    - Resizing is done through the ZED SDK. Make sure you keep this consistent during policy evaluation.
    - Depth is given in millimeters, but before saving it we multiple the values by 1000 and convert to uint16
    - Upon decoding the depth, you should divide the values by 1000 to return to millimeters

Note :: Must run on hardware with the ZED SDK; highly recommended to run this on the data collection laptop directly!

Run from DROID directory root with: `python scripts/convert/svo_to_depth.py'


"""

import json
import os
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, Tuple

import pyrallis

from droid.postprocessing.parse import parse_datetime
from droid.postprocessing.stages import run_indexing, run_processing
from droid.postprocessing.util.validate import validate_user2id

# Registry of known labs and associated users. Note that we canonicalize names as "<F>irst <L>ast".
#   => We map each "name" to a unique ID (8 characters); when adding new users, make sure to pick a unique ID!
#         + Simple ID generator --> python -c "import uuid; print(str(uuid.uuid4())[:8])"
# fmt: off
REGISTERED_MEMBERS: Dict[str, Dict[str, str]] = {
    "Lab Name": {
        "First Last": "4b1a56cc",
    },
}
validate_user2id(REGISTERED_MEMBERS)

# Try to be as consistent as possible using the "canonical" names in the dictionary above; however, for unavoidable
# cases (e.g., unfortunate typos, using names/nicknames interchangeably), update the dictionary below!
REGISTERED_ALIASES: Dict[str, Tuple[str, str]] = {
    **{user: (lab, user) for lab, users in REGISTERED_MEMBERS.items() for user in users},

    # Note: Add duplicates/typos below (follow format)!
    **{
        "First Last": ("Lab Name", "First Last"),
    }
}
# fmt: on

@dataclass
class DROIDConversionConfig:
    # fmt: off
    lab: str                                        # Lab ID (all uppercase) -- e.g., "CLVR", "ILIAD", "REAL"
    data_dir: Path = Path("data")                   # Path to top-level directory, if lab_agnostic=False it should contain "success"/"failure" directories
    lab_agnostic: bool = True                       # Determines whether to only convert your lab's data or all data

    # Stage Handling
    do_index: bool = True                           # Whether to run an initial indexing pass prior to processing stage
    do_process: bool = True                         # Whether to run processing (can skip if just want to upload)

    # Processing Details
    process_failures: bool = False                  # Whether to include failures in the processing stage
    extract_MP4_data: bool = False                  # Whether to extract MP4 data from the SVO files
    extract_depth_data: bool = True                 # Whether to extract depth data from the SVO files
    depth_resolution: tuple = (0,0)                 # Resolution to extract depth data at, use a value of (0,0) to avoid resizing
    depth_frequency: int = 1                        # The timestep frequency at which we extract depth data

    # Important :: Only update once you're sure *all* demonstrations prior to this date have been processed!
    #   > If not running low on disk, leave alone!
    start_date: str = "2023-01-01"                  # Start indexing/processing/uploading demos starting from this date

    # Cache Parameters
    cache_dir: Path = Path("cache/postprocessing")  # Relative path to `cache` directory; defaults to repository root
    # fmt: on


@pyrallis.wrap()
def postprocess(cfg: DROIDConversionConfig) -> None:
    print(f"[*] Starting Data Processing & Upload for Lab `{cfg.lab}`")

    # Initialize Cache Data Structure --> Load Uploaded/Processed List from `cache_dir` (if exists)
    #   => Note that in calls to each stage, cache is updated *in-place* (allows for the try/finally to work)
    cache = {
        "lab": cfg.lab,
        "start_date": cfg.start_date,
        "totals": {k: {"success": 0, "failure": 0} for k in ["scanned", "indexed", "processed", "uploaded", "errored"]},
        "scanned_paths": {"success": {}, "failure": {}},
        "indexed_uuids": {"success": {}, "failure": {}},
        "processed_uuids": {"success": {}, "failure": {}},
        "uploaded_uuids": {"success": {}, "failure": {}},
        "errored_paths": {"success": {}, "failure": {}},
    }
    os.makedirs(cfg.cache_dir, exist_ok=True)
    if (cfg.cache_dir / f"{cfg.lab}-cache.json").exists():
        with open(cfg.cache_dir / f"{cfg.lab}-cache.json", "r") as f:
            loaded_cache = json.load(f)

            # Only replace cache on matched `start_date` --> not an ideal solution (cache invalidation is hard!)
            if loaded_cache["start_date"] == cache["start_date"]:
                cache = loaded_cache

    # === Run Post-Processing Stages ===
    try:
        start_datetime = parse_datetime(cfg.start_date, mode="day")

        # Stage 1 --> "Indexing"
        if cfg.do_index:
            run_indexing(
                cfg.data_dir,
                cfg.lab,
                start_datetime,
                aliases=REGISTERED_ALIASES,
                members=REGISTERED_MEMBERS,
                totals=cache["totals"],
                scanned_paths=cache["scanned_paths"],
                indexed_uuids=cache["indexed_uuids"],
                errored_paths=cache["errored_paths"],
                process_failures=cfg.process_failures,
                lab_agnostic=cfg.lab_agnostic,
                search_existing_metadata=True,
            )
        else:
            print("[*] Stage 1 =>> Skipping Indexing!")

        # Stage 2 --> "Processing"
        if cfg.do_process:
            run_processing(
                cfg.data_dir,
                cfg.lab,
                aliases=REGISTERED_ALIASES,
                members=REGISTERED_MEMBERS,
                totals=cache["totals"],
                indexed_uuids=cache["indexed_uuids"],
                processed_uuids=cache["processed_uuids"],
                errored_paths=cache["errored_paths"],
                extract_MP4_data=cfg.extract_MP4_data,
                extract_depth_data=cfg.extract_depth_data,
                depth_resolution=cfg.depth_resolution,
                depth_frequency=cfg.depth_frequency,
                search_existing_metadata=True,
            )
        else:
            print("[*] Stage 2 =>> Skipping Processing!")


    finally:
        # Dump `cache` on any interruption!
        with open(cfg.cache_dir / f"{cfg.lab}-cache.json", "w") as f:
            json.dump(cache, f, indent=2)

        # Print Statistics
        print(
            "[*] Terminated Post-Processing Script --> Summary:\n"
            f"\t- Scanned:      {sum(cache['totals']['scanned'].values())}\n"
            f"\t- Indexed:      {sum(cache['totals']['indexed'].values())}\n"
            f"\t- Processed:    {sum(cache['totals']['processed'].values())}\n"
            f"\t- Uploaded:     {sum(cache['totals']['uploaded'].values())}\n"
            f"\t- Total Errors: {sum(cache['totals']['errored'].values())}\n"
            f"\t  -> Errors in `success/` [FIX IMMEDIATELY]: {cache['totals']['errored']['success']}\n"
            f"\t  -> Errors in `failure/` [VERIFY]: {cache['totals']['errored']['failure']}\n\n"
            f"[*] See `{cfg.lab}-cache.json` in {cfg.cache_dir} for more information!\n"
        )


if __name__ == "__main__":
    postprocess()



================================================
FILE: scripts/convert/svo_to_mp4.py
================================================
"""
Script for extracting MP4 data from SVO files.

Performs the following:
    - Checks for "cached" uploads in `DROID/cache/<lab>-cache.json; avoids repetitive work.
    - Parses out relevant metadata from each trajectory --> *errors* on "unexpected format" (fail-fast).
    - Converts all SVO files to the relevant MP4s --> logs "corrupt" data (silent).
    - Runs validation logic on all *new* demonstrations --> errors on "corrupt" data (fail-fast).
    - Writes JSON metadata for all *new* demonstrations for easy data querying.
    - Uploads each demonstration "day" data to Amazon S3 bucket and updates `DROID/cache/<lab>-cache.json`.

Note :: Must run on hardware with the ZED SDK; highly recommended to run this on the data collection laptop directly!

Run from DROID directory root with: `python scripts/convert/svo_to_mp4.py'
"""

import json
import os
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, Tuple

import pyrallis

from droid.postprocessing.parse import parse_datetime
from droid.postprocessing.stages import run_indexing, run_processing
from droid.postprocessing.util.validate import validate_user2id

# Registry of known labs and associated users. Note that we canonicalize names as "<F>irst <L>ast".
#   => We map each "name" to a unique ID (8 characters); when adding new users, make sure to pick a unique ID!
#         + Simple ID generator --> python -c "import uuid; print(str(uuid.uuid4())[:8])"
# fmt: off
REGISTERED_MEMBERS: Dict[str, Dict[str, str]] = {
    "Lab Name": {
        "First Last": "4b1a56cc",
    },
}
validate_user2id(REGISTERED_MEMBERS)

# Try to be as consistent as possible using the "canonical" names in the dictionary above; however, for unavoidable
# cases (e.g., unfortunate typos, using names/nicknames interchangeably), update the dictionary below!
REGISTERED_ALIASES: Dict[str, Tuple[str, str]] = {
    **{user: (lab, user) for lab, users in REGISTERED_MEMBERS.items() for user in users},

    # Note: Add duplicates/typos below (follow format)!
    **{
        "First Last": ("Lab Name", "First Last"),
    }
}
# fmt: on

@dataclass
class DROIDConversionConfig:
    # fmt: off
    lab: str                                        # Lab ID (all uppercase) -- e.g., "CLVR", "ILIAD", "REAL"
    data_dir: Path = Path("data")                   # Path to top-level directory, if lab_agnostic=False it should contain "success"/"failure" directories
    lab_agnostic: bool = True                       # Determines whether to only convert your lab's data or all data

    # Stage Handling
    do_index: bool = True                           # Whether to run an initial indexing pass prior to processing stage
    do_process: bool = True                         # Whether to run processing (can skip if just want to upload)

    # Processing Details
    process_failures: bool = False                  # Whether to include failures in the processing stage
    extract_MP4_data: bool = True                   # Whether to extract MP4 data from the SVO files
    extract_depth_data: bool = False                # Whether to extract depth data from the SVO files

    # Important :: Only update once you're sure *all* demonstrations prior to this date have been processed!
    #   > If not running low on disk, leave alone!
    start_date: str = "2023-01-01"                  # Start indexing/processing/uploading demos starting from this date

    # Cache Parameters
    cache_dir: Path = Path("cache/postprocessing")  # Relative path to `cache` directory; defaults to repository root
    # fmt: on


@pyrallis.wrap()
def postprocess(cfg: DROIDConversionConfig) -> None:
    print(f"[*] Starting Data Processing & Upload for Lab `{cfg.lab}`")

    # Initialize Cache Data Structure --> Load Uploaded/Processed List from `cache_dir` (if exists)
    #   => Note that in calls to each stage, cache is updated *in-place* (allows for the try/finally to work)
    cache = {
        "lab": cfg.lab,
        "start_date": cfg.start_date,
        "totals": {k: {"success": 0, "failure": 0} for k in ["scanned", "indexed", "processed", "uploaded", "errored"]},
        "scanned_paths": {"success": {}, "failure": {}},
        "indexed_uuids": {"success": {}, "failure": {}},
        "processed_uuids": {"success": {}, "failure": {}},
        "uploaded_uuids": {"success": {}, "failure": {}},
        "errored_paths": {"success": {}, "failure": {}},
    }
    os.makedirs(cfg.cache_dir, exist_ok=True)
    if (cfg.cache_dir / f"{cfg.lab}-cache.json").exists():
        with open(cfg.cache_dir / f"{cfg.lab}-cache.json", "r") as f:
            loaded_cache = json.load(f)

            # Only replace cache on matched `start_date` --> not an ideal solution (cache invalidation is hard!)
            if loaded_cache["start_date"] == cache["start_date"]:
                cache = loaded_cache

    # === Run Post-Processing Stages ===
    try:
        start_datetime = parse_datetime(cfg.start_date, mode="day")

        # Stage 1 --> "Indexing"
        if cfg.do_index:
            run_indexing(
                cfg.data_dir,
                cfg.lab,
                start_datetime,
                aliases=REGISTERED_ALIASES,
                members=REGISTERED_MEMBERS,
                totals=cache["totals"],
                scanned_paths=cache["scanned_paths"],
                indexed_uuids=cache["indexed_uuids"],
                errored_paths=cache["errored_paths"],
                process_failures=cfg.process_failures,
                lab_agnostic=cfg.lab_agnostic,
                search_existing_metadata=True,
            )
        else:
            print("[*] Stage 1 =>> Skipping Indexing!")

        # Stage 2 --> "Processing"
        if cfg.do_process:
            run_processing(
                cfg.data_dir,
                cfg.lab,
                aliases=REGISTERED_ALIASES,
                members=REGISTERED_MEMBERS,
                totals=cache["totals"],
                indexed_uuids=cache["indexed_uuids"],
                processed_uuids=cache["processed_uuids"],
                errored_paths=cache["errored_paths"],
                extract_MP4_data=cfg.extract_MP4_data,
                extract_depth_data=cfg.extract_depth_data,
                search_existing_metadata=True,
            )
        else:
            print("[*] Stage 2 =>> Skipping Processing!")


    finally:
        # Dump `cache` on any interruption!
        with open(cfg.cache_dir / f"{cfg.lab}-cache.json", "w") as f:
            json.dump(cache, f, indent=2)

        # Print Statistics
        print(
            "[*] Terminated Post-Processing Script --> Summary:\n"
            f"\t- Scanned:      {sum(cache['totals']['scanned'].values())}\n"
            f"\t- Indexed:      {sum(cache['totals']['indexed'].values())}\n"
            f"\t- Processed:    {sum(cache['totals']['processed'].values())}\n"
            f"\t- Uploaded:     {sum(cache['totals']['uploaded'].values())}\n"
            f"\t- Total Errors: {sum(cache['totals']['errored'].values())}\n"
            f"\t  -> Errors in `success/` [FIX IMMEDIATELY]: {cache['totals']['errored']['success']}\n"
            f"\t  -> Errors in `failure/` [VERIFY]: {cache['totals']['errored']['failure']}\n\n"
            f"[*] See `{cfg.lab}-cache.json` in {cfg.cache_dir} for more information!\n"
        )


if __name__ == "__main__":
    postprocess()



================================================
FILE: scripts/convert/to_tfrecord.py
================================================
import os

import numpy as np
import tensorflow as tf
import tqdm
from absl import app, flags, logging
from tqdm_multiprocess import TqdmMultiProcessPool

from droid.data_loading.trajectory_sampler import crawler
from droid.trajectory_utils.misc import load_trajectory

"""
AVAILABLE KEYS:

action/cartesian_position
action/cartesian_velocity
action/gripper_position
action/gripper_velocity
action/joint_position
action/joint_velocity
action/robot_state/cartesian_position
action/robot_state/gripper_position
action/robot_state/joint_positions
action/robot_state/joint_torques_computed
action/robot_state/joint_velocities
action/robot_state/motor_torques_measured
action/robot_state/prev_command_successful
action/robot_state/prev_controller_latency_ms
action/robot_state/prev_joint_torques_computed
action/robot_state/prev_joint_torques_computed_safened
action/target_cartesian_position
action/target_gripper_position
observation/camera_extrinsics/13062452_left
observation/camera_extrinsics/13062452_left_gripper_offset
observation/camera_extrinsics/13062452_right
observation/camera_extrinsics/13062452_right_gripper_offset
observation/camera_extrinsics/19824535_left
observation/camera_extrinsics/19824535_right
observation/camera_extrinsics/20521388_left
observation/camera_extrinsics/20521388_right
observation/camera_extrinsics/23404442_left
observation/camera_extrinsics/23404442_right
observation/camera_extrinsics/24259877_left
observation/camera_extrinsics/24259877_right
observation/camera_extrinsics/26405488_left
observation/camera_extrinsics/26405488_right
observation/camera_extrinsics/29838012_left
observation/camera_extrinsics/29838012_right
observation/camera_type/13062452
observation/camera_type/20521388
observation/camera_type/24259877
observation/controller_info/controller_on
observation/controller_info/failure
observation/controller_info/movement_enabled
observation/controller_info/success
observation/robot_state/cartesian_position
observation/robot_state/gripper_position
observation/robot_state/joint_positions
observation/robot_state/joint_torques_computed
observation/robot_state/joint_velocities
observation/robot_state/motor_torques_measured
observation/robot_state/prev_command_successful
observation/robot_state/prev_controller_latency_ms
observation/robot_state/prev_joint_torques_computed
observation/robot_state/prev_joint_torques_computed_safened
observation/timestamp/cameras/13062452_estimated_capture
observation/timestamp/cameras/13062452_frame_received
observation/timestamp/cameras/13062452_read_end
observation/timestamp/cameras/13062452_read_start
observation/timestamp/cameras/20521388_estimated_capture
observation/timestamp/cameras/20521388_frame_received
observation/timestamp/cameras/20521388_read_end
observation/timestamp/cameras/20521388_read_start
observation/timestamp/cameras/24259877_estimated_capture
observation/timestamp/cameras/24259877_frame_received
observation/timestamp/cameras/24259877_read_end
observation/timestamp/cameras/24259877_read_start
observation/timestamp/control/control_start
observation/timestamp/control/policy_start
observation/timestamp/control/sleep_start
observation/timestamp/control/step_end
observation/timestamp/control/step_start
observation/timestamp/robot_state/read_end
observation/timestamp/robot_state/read_start
observation/timestamp/robot_state/robot_timestamp_nanos
observation/timestamp/robot_state/robot_timestamp_seconds
observation/timestamp/skip_action
observation/image/24259877_left
observation/image/24259877_right
observation/image/13062452_left
observation/image/13062452_right
observation/image/20521388_left
observation/image/20521388_right
"""


def flatten(x):
    d = {}
    for k, v in x.items():
        if isinstance(v, dict):
            for k2, v2 in flatten(v).items():
                d[k + "/" + k2] = v2
        else:
            d[k] = v
    return d


def tensor_feature(value):
    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[tf.io.serialize_tensor(value).numpy()]))


def resize_and_encode(image, size):
    return tf.io.encode_jpeg(tf.cast(tf.round(tf.image.resize(image, size, method="bicubic")), tf.uint8))


FLAGS = flags.FLAGS
flags.DEFINE_string("input_path", "franka_data/success", "Path to input directory")
flags.DEFINE_string("output_path", "franka_data/tfrecords", "Path to output directory")
flags.DEFINE_bool("overwrite", False, "Overwrite existing tfrecords")
flags.DEFINE_float("train_fraction", 0.8, "Fraction of data to use for training")
flags.DEFINE_integer("shard_size", 200, "Maximum number of trajectories per tfrecord")
flags.DEFINE_integer("num_workers", 8, "Number of workers to use for parallel processing")


KEEP_KEYS = [
    "action/cartesian_position",
    "action/cartesian_velocity",
    "action/gripper_position",
    "action/gripper_velocity",
    "action/target_cartesian_position",
    "action/target_gripper_position",
    "observation/image/24259877_left",
    "observation/image/24259877_right",
    "observation/image/13062452_left",
    "observation/image/13062452_right",
    "observation/image/20521388_left",
    "observation/image/20521388_right",
]

FRAMESKIP = 1
IMAGE_SIZE = (180, 320)


def create_tfrecord(paths, output_path, tqdm_func, global_tqdm):
    writer = tf.io.TFRecordWriter(output_path)

    for path in paths:
        h5_filepath = os.path.join(path, "trajectory.h5")
        recording_folderpath = os.path.join(path, "recordings", "MP4")

        # this is really inefficient, should really do frame skipping inside `load_trajectory` but I didn't want to mess
        # with it for now
        traj = load_trajectory(h5_filepath, recording_folderpath=recording_folderpath)
        traj = traj[::FRAMESKIP]

        # each element of `traj` is a possibly nested dict; flatten them and make sure they all have the same keys
        traj_flat = [flatten(t) for t in traj]
        assert all(t.keys() == traj_flat[0].keys() for t in traj_flat)

        # convert to a single dict of lists, processing images and discarding unwanted keys along the way
        out = {}
        for key in traj_flat[0].keys():
            if key not in KEEP_KEYS:
                continue
            if "image" in key:
                out[key] = [resize_and_encode(t[key], IMAGE_SIZE) for t in traj_flat]
            else:
                out[key] = [t[key] for t in traj_flat]
        example = tf.train.Example(features=tf.train.Features(feature={k: tensor_feature(v) for k, v in out.items()}))

        writer.write(example.SerializeToString())

        global_tqdm.update(1)

    writer.close()


def main(_):
    if tf.io.gfile.exists(FLAGS.output_path):
        if FLAGS.overwrite:
            logging.info(f"Deleting {FLAGS.output_path}")
            tf.io.gfile.rmtree(FLAGS.output_path)
        else:
            logging.info(f"{FLAGS.output_path} exists, exiting")
            return

    all_paths = crawler(FLAGS.input_path)
    all_paths = [p for p in all_paths if os.path.exists(p + "/trajectory.h5") and os.path.exists(p + "/recordings/MP4")]

    # train/test split
    # might be a good idea to do a stratified split, but that would require significant changes in `crawler`
    np.random.shuffle(all_paths)
    train_paths = all_paths[: int(len(all_paths) * FLAGS.train_fraction)]
    test_paths = all_paths[int(len(all_paths) * FLAGS.train_fraction) :]

    # shard paths
    train_shards = np.array_split(train_paths, np.ceil(len(train_paths) / FLAGS.shard_size))
    test_shards = np.array_split(test_paths, np.ceil(len(test_paths) / FLAGS.shard_size))

    # create output paths
    tf.io.gfile.makedirs(os.path.join(FLAGS.output_path, "train"))
    tf.io.gfile.makedirs(os.path.join(FLAGS.output_path, "test"))
    train_output_paths = [os.path.join(FLAGS.output_path, "train", f"{i}.tfrecord") for i in range(len(train_shards))]
    test_output_paths = [os.path.join(FLAGS.output_path, "test", f"{i}.tfrecord") for i in range(len(test_shards))]

    # create tasks (see tqdm_multiprocess documenation)
    tasks = [(create_tfrecord, (train_shards[i], train_output_paths[i])) for i in range(len(train_shards))] + [
        (create_tfrecord, (test_shards[i], test_output_paths[i])) for i in range(len(test_shards))
    ]

    # run tasks
    pool = TqdmMultiProcessPool(FLAGS.num_workers)
    with tqdm.tqdm(total=len(all_paths), dynamic_ncols=True) as pbar:
        pool.map(pbar, tasks, lambda _: None, lambda _: None)


if __name__ == "__main__":
    app.run(main)



================================================
FILE: scripts/evaluation/evaluate_policy.py
================================================
from droid.evaluation.eval_launcher_robomimic import eval_launcher, get_goal_im
import matplotlib.pyplot as plt
import os
import argparse
import cv2


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('-g', '--capture_goal', action='store_true')
    parser.add_argument('-l', '--lang_cond', action='store_true')
    parser.add_argument('-c', '--ckpt_path', type=str, default=None, 
        help='Path to Pytorch checkpoint (.pth) corresponding to the policy you want to evaluate.')
    args = parser.parse_args()

    variant = dict(
        exp_name="policy_test",
        save_data=False,
        use_gpu=True,
        seed=0,
        policy_logdir="test",
        task="",
        layout_id=None,
        model_id=50,
        camera_kwargs=dict(),
        data_processing_kwargs=dict(
            timestep_filtering_kwargs=dict(),
            image_transform_kwargs=dict(),
        ),
        ckpt_path=args.ckpt_path,
    )

    if args.capture_goal:
        valid_values = ["yes", "no"]
        goal_im_ok = False

        while goal_im_ok is not True:
            program_mode = input("Move the robot into programming mode, adjust the scene as needed, and then press Enter to acknowledge: ")
            exec_mode = input("Now move the robot into execution mode, press Enter to acknowledge: ")
            print("Capturing Goal Image")
            goal_ims = get_goal_im(variant, run_id=1, exp_id=0)
            # Sort the goal_ims by key and display in a 3 x 2 grid
            sort_goal_ims = [cv2.cvtColor(image[1][:, :, :3], cv2.COLOR_BGR2RGB) for image in sorted(goal_ims.items(), key=lambda x: x[0])]
            fig, axes = plt.subplots(3, 2, figsize=(8,10))
            axes = axes.flatten()
            for i in range(len(sort_goal_ims)):
                axes[i].imshow(sort_goal_ims[i])
                axes[i].axis('off')
            plt.tight_layout()
            plt.show()

            user_input = input("Do these goal images look reasonable? Enter Yes or No: ").lower()
            while user_input not in valid_values:
                print("Invalid input, Please enter Yes or No")
            goal_im_ok = user_input == "yes"
        input("Now reset the scene for the policy to execute, press Enter to acknowledge: ")
    if args.lang_cond:
        user_input = input("Provide a language command for the robot to complete: ").lower()
        if not os.path.exists('eval_params'):
            os.makedirs('eval_params')
        with open('eval_params/lang_command.txt', 'w') as file:
            file.write(user_input)
    
    print("Evaluating Policy")
    eval_launcher(variant, run_id=1, exp_id=0)



================================================
FILE: scripts/evaluation/evaluate_rt1.py
================================================
import argparse
from droid.user_interface.eval_gui import EvalGUI
from droid.evaluation.rt1_wrapper import RT1Policy


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--checkpoint_path", type=str, required=True)
    args = parser.parse_args()

    policy = RT1Policy(
        args.checkpoint_path,
        camera_obs_keys=[
            "26638268_left",
            "26638268_right",
            "22246076_left",
            "22246076_right",
            "16291792_left",
            "16291792_right",
        ],
    )
    EvalGUI(policy=policy)


if __name__ == "__main__":
    main()



================================================
FILE: scripts/evaluation/results.ipynb
================================================
# Jupyter notebook converted to Python script.

import rllab.viskit.core as core
import glob
import matplotlib.pyplot as plt

%matplotlib inline
import numpy as np

%load_ext autoreload
%autoreload 2
from rlkit.visualization import plot_util as plot

plt.style.use("ggplot")
import matplotlib

plot.configure_matplotlib(matplotlib)

envs = ["RemoveLid-v0", "MugDishRack-v0", "CarrotPlate-v0"]

dirs = ["/Users/sasha/Desktop/rotation_project/data/bridge_test/"]

filter_func = lambda e: e["params"]["env_name"] == envs[1]
exps = plot.load_exps(dirs, suppress_output=True)
# exps = plot.load_exps(dirs, filter_fn=filter_func, suppress_output=True)

envs = ["carrot", "cup", "faucet"]

dirs = ["/Users/sasha/Desktop/rotation_project/data/bridge_test/"]

filter_func = lambda e: e["params"]["setting"] == envs[2]
# exps = plot.load_exps(dirs, suppress_output=True)
exps = plot.load_exps(dirs, filter_fn=filter_func, suppress_output=True)

vary = [
    "target_percentage",
]
split = ["target_sample_limit", "setting"]
default_vary = {}

plot.split(
    exps,
    ["target-BC Loss"],
    vary,
    split,
    default_vary=default_vary,
    smooth=plot.padded_ma_filter(10),
    figsize=(12, 8),
    print_max=False,
    # ylim=(0, 1),
    # xlim=(0, 1000),
    # method_order=(0, 2, 1),
    plot_seeds=False,
    plot_error_bars=True,
    print_final=False,
    print_min=False,
    print_plot=True,
)
# Output:
#   {'target_sample_limit': {'833.3333333333334', '1666.6666666666667', '2500.0'}, 'setting': {'faucet'}}

#   <Figure size 864x1728 with 3 Axes>

envs = ["MugDishRack-v0", "CarrotPlate-v0"]

dirs = ["/Users/sasha/Desktop/rotation_project/data/domain_confusion/"]

filter_func = lambda e: e["params"]["env_name"] == envs[1]
# exps = plot.load_exps(dirs, suppress_output=True)
exps = plot.load_exps(dirs, filter_fn=filter_func, suppress_output=True)

#############################################################

vary = ["target_percentage", "ce_weight"]
split = ["target_traj_limit", "env_name"]
default_vary = {}

plot.split(
    exps,
    # ['Success Rate'],
    # ['target-BC Loss'],
    # ['env-picked_up'],
    # ['test-BC Loss'],
    ["target-BC Loss"],
    # ['target-Classifier Accuracy'],
    vary,
    split,
    default_vary=default_vary,
    smooth=plot.padded_ma_filter(10),
    figsize=(12, 8),
    print_max=False,
    # ylim=(0, 1),
    # xlim=(0, 1000),
    # method_order=(0, 2, 1),
    plot_seeds=False,
    plot_error_bars=True,
    print_final=False,
    print_min=False,
    print_plot=True,
)
# Output:
#   {'target_traj_limit': {'10', '5', '25', '50'}, 'env_name': {'CarrotPlate-v0'}}

#   <Figure size 864x2304 with 4 Axes>

vary = ["target_percentage", "ce_weight"]
split = ["target_traj_limit", "env_name"]
default_vary = {}

plot.split(
    exps,
    ["Success Rate"],
    # ['target-BC Loss'],
    # ['env-picked_up'],
    # ['test-BC Loss'],
    # ['target-BC Loss'],
    # ['target-Classifier Accuracy'],
    vary,
    split,
    default_vary=default_vary,
    smooth=plot.padded_ma_filter(10),
    figsize=(12, 8),
    print_max=False,
    ylim=(0, 1),
    # xlim=(0, 1000),
    # method_order=(0, 2, 1),
    plot_seeds=False,
    plot_error_bars=True,
    print_final=False,
    print_min=False,
    print_plot=True,
)
# Output:
#   {'target_traj_limit': {'10', '5', '25', '50'}, 'env_name': {'CarrotPlate-v0'}}

#   <Figure size 864x2304 with 4 Axes>

dirs = ["/Users/sasha/Desktop/rotation_project/data/val_demos/"]

exps = plot.load_exps(dirs, suppress_output=True)


vary = [
    "target_percentage",
]
split = [
    "target_traj_limit",
]
default_vary = {}

plot.split(
    exps,
    ["Success Rate"],
    # ['target-BC Loss'],
    # ['env-picked_up'],
    # ['test-BC Loss'],
    # ['target-BC Loss'],
    # ['target-Classifier Accuracy'],
    vary,
    split,
    default_vary=default_vary,
    smooth=plot.padded_ma_filter(10),
    figsize=(12, 8),
    print_max=False,
    ylim=(0, 1),
    # xlim=(0, 1000),
    # method_order=(0, 2, 1),
    plot_seeds=False,
    plot_error_bars=True,
    print_final=False,
    print_min=False,
    print_plot=True,
)
# Output:
#   {'target_traj_limit': {'10', '25', '100', '5', '50'}}

#   <Figure size 864x2880 with 5 Axes>

dirs = [
    "/Users/sasha/Desktop/rotation_project/data/val_demos/run10",
    "/Users/sasha/Desktop/rotation_project/data/val_demos/run12",
]

exps = plot.load_exps(dirs, suppress_output=True)

#############################################################

vary = ["target_percentage", "ce_weight"]
split = [
    "target_traj_limit",
]
default_vary = {}

plot.split(
    exps,
    ["Success Rate"],
    # ['target-BC Loss'],
    # ['env-picked_up'],
    # ['test-BC Loss'],
    # ['target-BC Loss'],
    # ['target-Classifier Accuracy'],
    vary,
    split,
    default_vary=default_vary,
    smooth=plot.padded_ma_filter(10),
    figsize=(12, 8),
    print_max=False,
    ylim=(0, 1),
    # xlim=(0, 1000),
    # method_order=(0, 2, 1),
    plot_seeds=False,
    plot_error_bars=True,
    print_final=False,
    print_min=False,
    print_plot=True,
)
# Output:
#   {'target_traj_limit': {'10', '5', '25'}}

#   <Figure size 864x1728 with 3 Axes>

#############################################################

vary = ["target_percentage", "ce_weight"]
split = [
    "target_traj_limit",
]
default_vary = {}

plot.split(
    exps,
    # ['Success Rate'],
    # ['target-BC Loss'],
    # ['env-picked_up'],
    # ['test-BC Loss'],
    # ['target-BC Loss'],
    ["target-Classifier Accuracy"],
    vary,
    split,
    default_vary=default_vary,
    smooth=plot.padded_ma_filter(10),
    figsize=(12, 8),
    print_max=False,
    ylim=(0, 1),
    # xlim=(0, 1000),
    # method_order=(0, 2, 1),
    plot_seeds=False,
    plot_error_bars=True,
    print_final=False,
    print_min=False,
    print_plot=True,
)
# Output:
#   {'target_traj_limit': {'10', '5', '25'}}

#   <Figure size 864x1728 with 3 Axes>

dirs = [
    "/Users/sasha/Desktop/rotation_project/data/val_demos/run1",
    "/Users/sasha/Desktop/rotation_project/data/val_demos/run2",
    "/Users/sasha/Desktop/rotation_project/data/val_demos/run3",
]

exps = plot.load_exps(dirs, suppress_output=True)


vary = [
    "target_percentage",
]
split = [
    "target_traj_limit",
]
default_vary = {}

plot.split(
    exps,
    # ['Success Rate'],
    ["target-BC Loss"],
    # ['env-picked_up'],
    # ['test-BC Loss'],
    # ['target-BC Loss'],
    # ['target-Classifier Accuracy'],
    vary,
    split,
    default_vary=default_vary,
    smooth=plot.padded_ma_filter(10),
    figsize=(12, 8),
    print_max=False,
    # ylim=(0, 1),
    # xlim=(0, 1000),
    # method_order=(0, 2, 1),
    plot_seeds=False,
    plot_error_bars=True,
    print_final=False,
    print_min=False,
    print_plot=True,
)
# Output:
#   {'target_traj_limit': {'10', '25', '100', '5', '50'}}

#   <Figure size 864x2880 with 5 Axes>

dirs = [
    "/Users/sasha/Desktop/rotation_project/data/val_demos/run35",
    "/Users/sasha/Desktop/rotation_project/data/val_demos/run36",
]

# exps = plot.load_exps(dirs, suppress_output=True)
filter_func = lambda e: e["params"]["action_key"] == "noisy_actions"
exps = plot.load_exps(dirs, filter_fn=filter_func, suppress_output=True)


vary = [
    "target_percentage",
]
split = [
    "target_traj_limit",
]
default_vary = {}

plot.split(
    exps,
    ["Success Rate"],
    # ['target-BC Loss'],
    # ['env-picked_up'],
    # ['test-BC Loss'],
    # ['target-BC Loss'],
    # ['target-Classifier Accuracy'],
    vary,
    split,
    default_vary=default_vary,
    smooth=plot.padded_ma_filter(10),
    figsize=(12, 8),
    print_max=False,
    ylim=(0, 1),
    # xlim=(0, 1000),
    # method_order=(0, 2, 1),
    plot_seeds=False,
    plot_error_bars=True,
    print_final=False,
    print_min=False,
    print_plot=True,
)
# Output:
#   {'target_traj_limit': {'25', '5', '10'}}

#   <Figure size 864x1728 with 3 Axes>

dirs = ["/Users/sasha/Desktop/rotation_project/data/val_demos/reconrun2"]

# exps = plot.load_exps(dirs, suppress_output=True)
filter_func = lambda e: e["params"]["action_key"] == "noisy_actions"
exps = plot.load_exps(dirs, filter_fn=filter_func, suppress_output=True)


vary = [
    "target_percentage",
]
split = [
    "target_traj_limit",
]
default_vary = {}

plot.split(
    exps,
    # ['Success Rate'],
    ["test-Recon Loss"],
    # ['env-picked_up'],
    # ['test-BC Loss'],
    # ['target-BC Loss'],
    # ['target-Classifier Accuracy'],
    vary,
    split,
    default_vary=default_vary,
    smooth=plot.padded_ma_filter(10),
    figsize=(12, 8),
    print_max=False,
    # ylim=(0, 1),
    # xlim=(0, 1000),
    # method_order=(0, 2, 1),
    plot_seeds=False,
    plot_error_bars=True,
    print_final=False,
    print_min=False,
    print_plot=True,
)
# Output:
#   {'target_traj_limit': {'25', '5', '50', '10'}}

#   <Figure size 864x2304 with 4 Axes>

dirs = ["/Users/sasha/Desktop/rotation_project/data/val_demos/new_env/run4"]

# exps = plot.load_exps(dirs, suppress_output=True)
filter_func = lambda e: e["params"]["action_key"] == "actions"
exps = plot.load_exps(dirs, filter_fn=filter_func, suppress_output=True)


vary = [
    "target_percentage",
]
split = [
    "target_traj_limit",
]
default_vary = {}

plot.split(
    exps,
    ["Success Rate"],
    # ['target-BC Loss'],
    # ['env-picked_up'],
    # ['test-BC Loss'],
    # ['env-final-obj_goal_distance'],
    # ['target-Classifier Accuracy'],
    vary,
    split,
    default_vary=default_vary,
    smooth=plot.padded_ma_filter(10),
    figsize=(12, 8),
    print_max=False,
    ylim=(0, 1),
    # xlim=(0, 1000),
    # method_order=(0, 2, 1),
    plot_seeds=False,
    plot_error_bars=True,
    print_final=False,
    print_min=False,
    print_plot=True,
)
# Output:
#   {'target_traj_limit': {'25.0'}}

#   <Figure size 864x576 with 1 Axes>

# Basic Experiment
# Note: Horizon=75

dirs = [
    "/Users/sasha/Desktop/rotation_project/data/val_demos/new_env/run5",
    "/Users/sasha/Desktop/rotation_project/data/val_demos/new_env/run6",
]

# exps = plot.load_exps(dirs, suppress_output=True)
filter_func = lambda e: e["params"]["action_key"] == "actions"
exps = plot.load_exps(dirs, filter_fn=filter_func, suppress_output=True)


vary = [
    "target_percentage",
]
split = [
    "target_traj_limit",
]
default_vary = {}

plot.split(
    exps,
    ["Success Rate"],
    # ['target-BC Loss'],
    # ['env-picked_up'],
    # ['test-BC Loss'],
    # ['env-final-obj_goal_distance'],
    # ['target-Classifier Accuracy'],
    vary,
    split,
    default_vary=default_vary,
    smooth=plot.padded_ma_filter(10),
    figsize=(12, 8),
    print_max=False,
    ylim=(0, 1),
    # xlim=(0, 1000),
    # method_order=(0, 2, 1),
    plot_seeds=False,
    plot_error_bars=True,
    print_final=False,
    print_min=False,
    print_plot=True,
    split_order={"target_traj_limit": ["5", "10", "25"]},
)
# Output:
#   {'target_traj_limit': ['5', '10', '25']}

#   <Figure size 864x1728 with 3 Axes>

# Recon Experiment: Non-Detached Gradient
# Note: Horizon=70

dirs = [
    "/Users/sasha/Desktop/rotation_project/data/val_demos/recon/run11",
    "/Users/sasha/Desktop/rotation_project/data/val_demos/recon/run12",
]

exps = plot.load_exps(dirs, suppress_output=True)


vary = [
    "target_percentage",
]
split = [
    "target_traj_limit",
]
default_vary = {}

plot.split(
    exps,
    ["Success Rate"],
    vary,
    split,
    default_vary=default_vary,
    smooth=plot.padded_ma_filter(10),
    figsize=(12, 8),
    print_max=False,
    ylim=(0, 1),
    # xlim=(0, 1000),
    # method_order=(0, 2, 1),
    plot_seeds=False,
    plot_error_bars=True,
    print_final=False,
    print_min=False,
    print_plot=True,
    split_order={"target_traj_limit": ["5", "10", "25"]},
)
# Output:
#   {'target_traj_limit': ['5', '10', '25']}

#   <Figure size 864x1728 with 3 Axes>
#   <Figure size 864x576 with 0 Axes>
#   <Figure size 864x576 with 0 Axes>
#   <Figure size 864x576 with 0 Axes>

# Recon Experiment: Detached Gradient
# Note: Horizon=75

dirs = ["/Users/sasha/Desktop/rotation_project/data/val_demos/recon/run14"]

exps = plot.load_exps(dirs, suppress_output=True)


vary = [
    "target_percentage",
]
split = [
    "target_traj_limit",
]
default_vary = {}

plot.split(
    exps,
    ["test-Recon Loss"],
    vary,
    split,
    default_vary=default_vary,
    smooth=plot.padded_ma_filter(10),
    figsize=(12, 8),
    print_max=False,
    # ylim=(0, .01),
    # xlim=(0, 1000),
    # method_order=(0, 2, 1),
    plot_seeds=False,
    plot_error_bars=True,
    print_final=False,
    print_min=False,
    print_plot=True,
)
# Output:
#   {'target_traj_limit': {'5', '10'}}

#   <Figure size 864x1152 with 2 Axes>

# RUN 1: Real bridge data, sim data of different objects
# RUN 2: Sim bridge data, sim data of target object
# RUN 4: Above, but inlcuding zero shot (so merge with above result)
# RUN 5: RUN 2, in case it goes too slow
# RUN 6: Run 1, but only 3 objects

# Note: Horizon=75


dirs = [
    "/Users/sasha/Desktop/rotation_project/data/val_demos/multiobj/run1"
    #'/Users/sasha/Desktop/rotation_project/data/val_demos/multiobj/run6'
    #     '/Users/sasha/Desktop/rotation_project/data/val_demos/multiobj/run4',
    #     '/Users/sasha/Desktop/rotation_project/data/val_demos/multiobj/run5',
]

exps = plot.load_exps(dirs, suppress_output=True)


vary = [
    "target_percentage",
]
default_vary = {}

plot.split(
    exps,
    ["Success Rate"],
    vary,
    split,
    default_vary=default_vary,
    smooth=plot.padded_ma_filter(50),
    figsize=(12, 8),
    print_max=False,
    ylim=(0, 1),
    # xlim=(0, 1000),
    # method_order=(0, 2, 1),
    plot_seeds=False,
    plot_error_bars=True,
    print_final=False,
    print_min=False,
    print_plot=True,
    split_order={"target_traj_limit": ["5"]},
)
# Output:
#   {'target_traj_limit': ['5']}

#   <Figure size 864x576 with 1 Axes>

dirs = [
    "/Users/sasha/Desktop/data/minimal_randomization/run1",
    "/Users/sasha/Desktop/data/minimal_randomization/run2",
    "/Users/sasha/Desktop/data/minimal_randomization/run3",
    "/Users/sasha/Desktop/data/minimal_randomization/run4",
]

exps = plot.load_exps(dirs, suppress_output=True)


vary = [
    "seed",
]

default_vary = {}

plot.comparison(
    exps,
    ["Success Rate"],
    vary,
    default_vary=default_vary,
    smooth=plot.padded_ma_filter(50),
    figsize=(12, 8),
    print_max=False,
    ylim=(0, 1),
    plot_seeds=False,
    plot_error_bars=True,
    print_final=False,
    print_min=False,
    print_plot=True,
)
# Output:
#   [<matplotlib.lines.Line2D at 0x7f92683ca730>,

#    <matplotlib.lines.Line2D at 0x7f92683ca640>,

#    <matplotlib.lines.Line2D at 0x7f92683ca220>,

#    <matplotlib.lines.Line2D at 0x7f92683eaa90>]
#   <Figure size 432x288 with 1 Axes>

dirs = [
    "/Users/sasha/Desktop/data/multi_object/run1",
    "/Users/sasha/Desktop/data/multi_object/run2",
    "/Users/sasha/Desktop/data/multi_object/run3",
    "/Users/sasha/Desktop/data/multi_object/run4",
]

exps = plot.load_exps(dirs, suppress_output=True)


vary = ["env_kwargs.num_scenes", "env_kwargs.num_objs"]

default_vary = {}

plot.comparison(
    exps,
    "Success Rate",
    vary,
    title="Eval Env == Train Env",
    default_vary=default_vary,
    smooth=plot.padded_ma_filter(50),
    figsize=(50, 100),
    print_max=False,
    ylim=(0, 1),
    plot_seeds=False,
    plot_error_bars=True,
    print_final=False,
    print_min=False,
    print_plot=True,
)
# Output:
#   [<matplotlib.lines.Line2D at 0x7f9268685280>,

#    <matplotlib.lines.Line2D at 0x7f9268685820>,

#    <matplotlib.lines.Line2D at 0x7f9268685ee0>,

#    <matplotlib.lines.Line2D at 0x7f92587b5a30>]
#   <Figure size 432x288 with 1 Axes>
#   <Figure size 3600x7200 with 0 Axes>

dirs = [
    "/Users/sasha/Desktop/data/multi_object_test/run1",
    "/Users/sasha/Desktop/data/multi_object_test/run2",
    "/Users/sasha/Desktop/data/multi_object_test/run3",
    "/Users/sasha/Desktop/data/multi_object_test/run4",
]

exps = plot.load_exps(dirs, suppress_output=True)


vary = ["env_kwargs.num_scenes", "env_kwargs.num_objs"]

default_vary = {}

plot.comparison(
    exps,
    "Success Rate",
    vary,
    title="Eval Env != Train Env",
    default_vary=default_vary,
    smooth=plot.padded_ma_filter(50),
    figsize=(50, 100),
    print_max=False,
    ylim=(0, 1),
    plot_seeds=False,
    plot_error_bars=True,
    print_final=False,
    print_min=False,
    print_plot=True,
)
# Output:
#   [<matplotlib.lines.Line2D at 0x7f922ac19640>,

#    <matplotlib.lines.Line2D at 0x7f922ac0c0d0>,

#    <matplotlib.lines.Line2D at 0x7f922ac0c790>,

#    <matplotlib.lines.Line2D at 0x7f922ac0ce50>]
#   <Figure size 432x288 with 1 Axes>



================================================
FILE: scripts/labeling/label_data.py
================================================
import json
import os
from collections import defaultdict

from droid.camera_utils.wrappers.recorded_multi_camera_wrapper import RecordedMultiCameraWrapper
from droid.training.data_loading.trajectory_sampler import collect_data_folderpaths
from droid.trajectory_utils.misc import visualize_timestep
from droid.trajectory_utils.trajectory_reader import TrajectoryReader

# Prepare Calibration Info #
dir_path = os.path.dirname(os.path.realpath(__file__))
task_label_filepath = os.path.join(dir_path, "task_label_filepath.json")


def load_task_info():
    if not os.path.isfile(task_label_filepath):
        return {}
    with open(task_label_filepath, "r") as jsonFile:
        task_labels = json.load(jsonFile)
    return task_labels


def update_task_label(folderpath, traj_id):
    task_labels = load_task_info()
    task_labels[folderpath] = traj_id

    with open(task_label_filepath, "w") as jsonFile:
        json.dump(task_labels, jsonFile)


# Classify Traj #
def label_trajectory(
    filepath,
    recording_folderpath=None,
    remove_skipped_steps=False,
    camera_kwargs={},
    max_width=1000,
    max_height=500,
    aspect_ratio=1.5,
):
    traj_reader = TrajectoryReader(filepath, read_images=True)
    if recording_folderpath:
        if camera_kwargs is {}:
            camera_kwargs = defaultdict(lambda: {"image": True})
        camera_reader = RecordedMultiCameraWrapper(recording_folderpath, camera_kwargs)

    horizon = traj_reader.length()
    camera_failed = False

    while True:
        for i in range(horizon):
            # Get HDF5 Data #
            timestep = traj_reader.read_timestep()

            # If Applicable, Get Recorded Data #
            if recording_folderpath:
                timestamp_dict = timestep["observation"]["timestamp"]["cameras"]
                camera_obs = camera_reader.read_cameras(timestamp_dict=timestamp_dict)
                camera_failed = camera_obs is None

                # Add Data To Timestep #
                if not camera_failed:
                    timestep["observation"].update(camera_obs)

            # Filter Steps #
            step_skipped = not timestep["observation"]["controller_info"].get("movement_enabled", True)
            delete_skipped_step = step_skipped and remove_skipped_steps
            delete_step = delete_skipped_step or camera_failed
            if delete_step:
                continue

            # Get Image Info #
            assert "image" in timestep["observation"]
            img_obs = timestep["observation"]["image"]
            camera_ids = list(img_obs.keys())
            len(camera_ids)
            camera_ids.sort()

            # Skip #
            if (i % 10) != 0:
                continue

            # Visualize Timestep #
            visualize_timestep(
                timestep, max_width=max_width, max_height=max_height, aspect_ratio=aspect_ratio, pause_time=15
            )

            # Check Termination #
            key_pressed = input("Enter Label (Putting In: A, Taking Out: S):\n")

            if key_pressed in ["a", "s", "b"]:
                # Close Readers #
                traj_reader.close()
                if recording_folderpath:
                    camera_reader.disable_cameras()

                if key_pressed == "b":
                    return -1
                return key_pressed == "a"


# Check Traj #
def check_trajectory(
    filepath,
    recording_folderpath=None,
    remove_skipped_steps=False,
    camera_kwargs={},
    max_width=1000,
    max_height=500,
    aspect_ratio=1.5,
):
    traj_reader = TrajectoryReader(filepath, read_images=True)
    if recording_folderpath:
        if camera_kwargs is {}:
            camera_kwargs = defaultdict(lambda: {"image": True})
        camera_reader = RecordedMultiCameraWrapper(recording_folderpath, camera_kwargs)

    horizon = traj_reader.length()
    camera_failed = False

    while True:
        for i in range(horizon):
            # Get HDF5 Data #
            timestep = traj_reader.read_timestep()

            # If Applicable, Get Recorded Data #
            if recording_folderpath:
                timestamp_dict = timestep["observation"]["timestamp"]["cameras"]
                camera_obs = camera_reader.read_cameras(timestamp_dict=timestamp_dict)
                camera_failed = camera_obs is None

                # Add Data To Timestep #
                if not camera_failed:
                    timestep["observation"].update(camera_obs)

            # Filter Steps #
            step_skipped = not timestep["observation"]["controller_info"].get("movement_enabled", True)
            delete_skipped_step = step_skipped and remove_skipped_steps
            delete_step = delete_skipped_step or camera_failed
            if delete_step:
                continue

            # Get Image Info #
            assert "image" in timestep["observation"]
            img_obs = timestep["observation"]["image"]
            camera_ids = list(img_obs.keys())
            len(camera_ids)
            camera_ids.sort()

            # Skip #
            if (i % 10) != 0:
                continue

            # Visualize Timestep #
            visualize_timestep(
                timestep, max_width=max_width, max_height=max_height, aspect_ratio=aspect_ratio, pause_time=15
            )

            # Check Termination #
            key_pressed = input("Enter Label (Putting In: A, Taking Out: S):\n")

            if key_pressed in ["a", "s", "b", " "]:
                # Close Readers #
                traj_reader.close()
                if recording_folderpath:
                    camera_reader.disable_cameras()

                if key_pressed == " ":
                    return None
                elif key_pressed == "b":
                    return -1
                return key_pressed == "a"


def filter_func(h5_metadata):
    curr_task = h5_metadata["current_task"]
    desired_task = "Move object into or out of container"
    keep = desired_task in curr_task
    return keep


traj_folders = collect_data_folderpaths(filter_func=filter_func, remove_failures=True)
labeled_trajectories = list(load_task_info().keys())
labeled_traj_dict = load_task_info()

i = 0
label = 0

# while i < len(traj_folders):
# 	folderpath = traj_folders[i]
# 	name = folderpath.split('/')[-1]

# 	filepath = os.path.join(folderpath, 'trajectory.h5')
# 	recording_folderpath = os.path.join(folderpath, 'recordings')
# 	if name in labeled_trajectories and labeled_traj_dict[name] == 1:
# 		label = check_trajectory(filepath, recording_folderpath=recording_folderpath, remove_skipped_steps=True)

# 		if label != None:
# 			print('relabling')
# 			update_task_label(name, label)

# 	i += 1


while i < len(traj_folders):
    folderpath = traj_folders[i]
    name = folderpath.split("/")[-1]
    if (label != -1) and (name in labeled_trajectories):
        i += 1
        continue

    filepath = os.path.join(folderpath, "trajectory.h5")
    recording_folderpath = os.path.join(folderpath, "recordings", "SVO")
    try:
        label = label_trajectory(filepath, recording_folderpath=recording_folderpath, remove_skipped_steps=True)
    except:
        i += 1
        continue

    if label == -1:
        i -= 1
    else:
        update_task_label(name, label)
        i += 1



================================================
FILE: scripts/labeling/task_label_filepath.json
================================================
{"Fri_Mar__3_16:33:25_2023": false, "Fri_Mar__3_15:31:24_2023": false, "Fri_Mar__3_16:33:59_2023": true, "Fri_Mar__3_16:00:05_2023": false, "Fri_Mar__3_15:33:52_2023": false, "Fri_Mar__3_15:30:01_2023": false, "Fri_Mar__3_15:32:00_2023": true, "Fri_Mar__3_15:59:46_2023": true, "Fri_Mar__3_15:59:12_2023": false, "Fri_Mar__3_15:54:59_2023": true, "Fri_Mar__3_15:56:29_2023": false, "Fri_Mar__3_15:33:36_2023": true, "Fri_Mar__3_15:57:02_2023": true, "Fri_Mar__3_15:33:17_2023": false, "Fri_Mar__3_15:28:29_2023": false, "Fri_Mar__3_15:29:08_2023": false, "Fri_Mar__3_15:28:11_2023": false, "Fri_Mar__3_15:27:18_2023": false, "Fri_Mar__3_15:34:16_2023": true, "Fri_Mar__3_16:00:23_2023": true, "Fri_Mar__3_15:30:49_2023": false, "Fri_Mar__3_15:57:26_2023": false, "Fri_Mar__3_15:57:41_2023": true, "Fri_Mar__3_15:29:23_2023": true, "Fri_Mar__3_15:55:48_2023": false, "Fri_Mar__3_15:27:34_2023": true, "Fri_Mar__3_15:27:03_2023": true, "Fri_Mar__3_15:56:05_2023": true, "Fri_Mar__3_15:55:17_2023": false, "Fri_Mar__3_15:54:39_2023": false, "Fri_Mar__3_15:32:53_2023": true, "Fri_Mar__3_15:35:14_2023": true, "Fri_Mar__3_15:31:05_2023": true, "Fri_Mar__3_15:30:17_2023": true, "Fri_Mar__3_15:01:02_2023": true, "Fri_Mar__3_15:32:17_2023": false, "Wed_Mar__8_17:01:26_2023": true, "Wed_Mar__8_18:49:29_2023": true, "Wed_Mar__8_16:44:51_2023": false, "Wed_Mar__8_14:39:06_2023": false, "Wed_Mar__8_17:13:22_2023": true, "Wed_Mar__8_13:24:50_2023": false, "Wed_Mar__8_13:25:31_2023": true, "Wed_Mar__8_13:57:43_2023": false, "Wed_Mar__8_19:46:24_2023": false, "Wed_Mar__8_19:01:15_2023": false, "Wed_Mar__8_16:42:39_2023": false, "Wed_Mar__8_14:34:01_2023": true, "Wed_Mar__8_13:56:25_2023": false, "Wed_Mar__8_14:50:13_2023": true, "Wed_Mar__8_17:15:44_2023": false, "Wed_Mar__8_19:34:33_2023": true, "Wed_Mar__8_13:22:39_2023": true, "Wed_Mar__8_16:47:52_2023": true, "Wed_Mar__8_14:37:30_2023": true, "Wed_Mar__8_16:52:44_2023": false, "Wed_Mar__8_17:49:46_2023": true, "Wed_Mar__8_17:14:17_2023": false, "Wed_Mar__8_13:30:21_2023": false, "Wed_Mar__8_19:39:17_2023": false, "Wed_Mar__8_19:06:58_2023": false, "Wed_Mar__8_19:56:56_2023": false, "Wed_Mar__8_16:45:49_2023": false, "Wed_Mar__8_13:24:13_2023": false, "Wed_Mar__8_16:40:21_2023": true, "Wed_Mar__8_17:41:54_2023": false, "Wed_Mar__8_13:18:32_2023": false, "Wed_Mar__8_13:47:43_2023": false, "Wed_Mar__8_17:17:57_2023": false, "Wed_Mar__8_19:03:33_2023": true, "Wed_Mar__8_19:06:05_2023": false, "Wed_Mar__8_19:08:03_2023": true, "Wed_Mar__8_14:48:05_2023": true, "Wed_Mar__8_19:36:27_2023": true, "Wed_Mar__8_19:48:00_2023": false, "Wed_Mar__8_16:46:31_2023": false, "Wed_Mar__8_17:55:21_2023": false, "Wed_Mar__8_19:50:12_2023": false, "Wed_Mar__8_19:04:38_2023": true, "Wed_Mar__8_19:36:45_2023": false, "Wed_Mar__8_17:05:10_2023": true, "Wed_Mar__8_19:39:36_2023": true, "Wed_Mar__8_14:45:53_2023": false, "Wed_Mar__8_19:46:41_2023": true, "Wed_Mar__8_19:06:20_2023": true, "Wed_Mar__8_14:41:35_2023": true, "Wed_Mar__8_19:55:58_2023": true, "Wed_Mar__8_17:48:46_2023": true, "Wed_Mar__8_13:29:11_2023": true, "Wed_Mar__8_19:43:36_2023": false, "Wed_Mar__8_17:04:36_2023": true, "Wed_Mar__8_17:37:15_2023": false, "Wed_Mar__8_18:51:36_2023": false, "Wed_Mar__8_13:55:10_2023": false, "Wed_Mar__8_14:33:42_2023": false, "Wed_Mar__8_17:37:52_2023": true, "Wed_Mar__8_18:47:13_2023": true, "Wed_Mar__8_17:14:01_2023": true, "Wed_Mar__8_13:47:24_2023": true, "Wed_Mar__8_14:42:57_2023": true, "Wed_Mar__8_17:40:12_2023": false, "Wed_Mar__8_13:21:53_2023": true, "Wed_Mar__8_19:33:00_2023": false, "Wed_Mar__8_18:55:11_2023": false, "Wed_Mar__8_14:39:50_2023": true, "Wed_Mar__8_18:50:19_2023": true, "Wed_Mar__8_16:53:12_2023": true, "Wed_Mar__8_19:42:43_2023": false, "Wed_Mar__8_19:33:17_2023": true, "Wed_Mar__8_14:41:22_2023": false, "Wed_Mar__8_19:35:29_2023": false, "Wed_Mar__8_14:52:01_2023": true, "Wed_Mar__8_14:39:35_2023": false, "Wed_Mar__8_17:38:30_2023": true, "Wed_Mar__8_13:59:37_2023": true, "Wed_Mar__8_19:40:20_2023": false, "Wed_Mar__8_14:51:28_2023": true, "Wed_Mar__8_16:48:44_2023": false, "Wed_Mar__8_18:50:35_2023": false, "Wed_Mar__8_13:47:08_2023": false, "Wed_Mar__8_14:49:26_2023": false, "Wed_Mar__8_14:43:38_2023": true, "Wed_Mar__8_13:59:07_2023": true, "Wed_Mar__8_19:01:54_2023": true, "Wed_Mar__8_19:49:08_2023": true, "Wed_Mar__8_19:53:27_2023": true, "Wed_Mar__8_17:55:38_2023": true, "Wed_Mar__8_19:48:17_2023": true, "Wed_Mar__8_19:50:41_2023": true, "Wed_Mar__8_17:02:06_2023": false, "Wed_Mar__8_14:32:53_2023": true, "Wed_Mar__8_17:50:55_2023": true, "Wed_Mar__8_19:54:14_2023": true, "Wed_Mar__8_19:44:52_2023": false, "Wed_Mar__8_18:49:44_2023": true, "Wed_Mar__8_19:02:14_2023": false, "Wed_Mar__8_14:40:41_2023": true, "Wed_Mar__8_19:29:53_2023": false, "Wed_Mar__8_18:45:59_2023": true, "Wed_Mar__8_17:35:56_2023": false, "Wed_Mar__8_19:32:03_2023": true, "Wed_Mar__8_13:57:08_2023": false, "Wed_Mar__8_19:30:13_2023": true, "Wed_Mar__8_17:00:47_2023": false, "Wed_Mar__8_19:33:43_2023": true, "Wed_Mar__8_16:52:23_2023": true, "Wed_Mar__8_16:43:16_2023": false, "Wed_Mar__8_13:52:26_2023": false, "Wed_Mar__8_17:40:54_2023": false, "Wed_Mar__8_17:02:21_2023": true, "Wed_Mar__8_14:40:26_2023": false, "Wed_Mar__8_19:49:54_2023": true, "Wed_Mar__8_17:52:27_2023": true, "Wed_Mar__8_16:56:45_2023": true, "Wed_Mar__8_16:42:53_2023": true, "Wed_Mar__8_16:46:50_2023": true, "Wed_Mar__8_16:55:37_2023": false, "Wed_Mar__8_17:40:29_2023": true, "Wed_Mar__8_13:54:21_2023": true, "Wed_Mar__8_17:16:35_2023": true, "Wed_Mar__8_16:49:57_2023": false, "Wed_Mar__8_19:51:18_2023": true, "Wed_Mar__8_13:20:13_2023": false, "Wed_Mar__8_14:35:36_2023": true, "Wed_Mar__8_17:46:16_2023": false, "Wed_Mar__8_16:56:24_2023": false, "Wed_Mar__8_18:51:13_2023": true, "Wed_Mar__8_19:45:34_2023": false, "Wed_Mar__8_14:44:02_2023": false, "Wed_Mar__8_19:34:49_2023": false, "Wed_Mar__8_19:33:30_2023": false, "Wed_Mar__8_17:38:09_2023": false, "Wed_Mar__8_14:38:14_2023": false, "Wed_Mar__8_18:55:39_2023": true, "Wed_Mar__8_17:51:17_2023": false, "Wed_Mar__8_14:34:18_2023": false, "Wed_Mar__8_19:38:58_2023": true, "Wed_Mar__8_14:42:04_2023": false, "Wed_Mar__8_19:48:39_2023": false, "Wed_Mar__8_19:05:22_2023": true, "Wed_Mar__8_14:44:20_2023": true, "Wed_Mar__8_16:51:49_2023": false, "Wed_Mar__8_13:55:29_2023": true, "Wed_Mar__8_16:17:00_2023": true, "Wed_Mar__8_19:05:07_2023": false, "Wed_Mar__8_13:28:54_2023": false, "Wed_Mar__8_17:50:24_2023": false, "Wed_Mar__8_17:54:03_2023": false, "Wed_Mar__8_18:53:27_2023": false, "Wed_Mar__8_13:31:50_2023": true, "Wed_Mar__8_19:45:49_2023": true, "Wed_Mar__8_14:45:03_2023": false, "Wed_Mar__8_19:49:36_2023": false, "Wed_Mar__8_18:46:31_2023": true, "Wed_Mar__8_13:57:22_2023": true, "Wed_Mar__8_16:54:46_2023": true, "Wed_Mar__8_14:43:18_2023": false, "Wed_Mar__8_19:35:15_2023": true, "Wed_Mar__8_16:50:47_2023": true, "Wed_Mar__8_13:20:48_2023": true, "Wed_Mar__8_18:54:30_2023": true, "Wed_Mar__8_18:48:34_2023": false, "Wed_Mar__8_14:00:08_2023": false, "Wed_Mar__8_14:48:37_2023": false, "Wed_Mar__8_19:52:49_2023": false, "Wed_Mar__8_19:36:09_2023": false, "Wed_Mar__8_17:49:21_2023": false, "Wed_Mar__8_19:50:57_2023": false, "Wed_Mar__8_13:27:05_2023": true, "Wed_Mar__8_13:29:47_2023": true, "Wed_Mar__8_16:36:52_2023": false, "Wed_Mar__8_19:02:53_2023": true, "Wed_Mar__8_13:59:21_2023": false, "Wed_Mar__8_14:48:54_2023": true, "Wed_Mar__8_18:51:52_2023": true, "Wed_Mar__8_17:56:48_2023": true, "Wed_Mar__8_14:36:58_2023": true, "Wed_Mar__8_14:39:20_2023": true, "Wed_Mar__8_17:42:53_2023": false, "Wed_Mar__8_16:53:39_2023": false, "Wed_Mar__8_19:52:01_2023": true, "Wed_Mar__8_14:03:02_2023": true, "Wed_Mar__8_16:46:08_2023": true, "Wed_Mar__8_16:41:18_2023": false, "Wed_Mar__8_13:19:12_2023": true, "Wed_Mar__8_16:43:54_2023": true, "Wed_Mar__8_14:50:41_2023": false, "Wed_Mar__8_19:43:20_2023": true, "Wed_Mar__8_17:36:25_2023": true, "Wed_Mar__8_17:04:17_2023": false, "Wed_Mar__8_17:54:36_2023": true, "Wed_Mar__8_13:26:01_2023": false, "Wed_Mar__8_18:46:54_2023": false, "Wed_Mar__8_13:31:34_2023": false, "Wed_Mar__8_17:04:54_2023": false, "Wed_Mar__8_19:35:53_2023": true, "Wed_Mar__8_18:48:51_2023": true, "Wed_Mar__8_19:31:02_2023": false, "Wed_Mar__8_13:28:38_2023": true, "Wed_Mar__8_17:52:45_2023": false, "Wed_Mar__8_17:56:10_2023": false, "Wed_Mar__8_19:33:59_2023": false, "Wed_Mar__8_19:38:42_2023": false, "Wed_Mar__8_17:42:26_2023": true, "Wed_Mar__8_13:51:44_2023": true, "Wed_Mar__8_18:50:03_2023": false, "Wed_Mar__8_19:58:20_2023": false, "Wed_Mar__8_16:51:09_2023": false, "Wed_Mar__8_19:32:25_2023": false, "Wed_Mar__8_16:45:10_2023": true, "Wed_Mar__8_17:17:39_2023": true, "Wed_Mar__8_17:39:15_2023": true, "Wed_Mar__8_19:44:01_2023": true, "Wed_Mar__8_17:13:05_2023": false, "Wed_Mar__8_16:51:31_2023": true, "Wed_Mar__8_18:53:42_2023": true, "Wed_Mar__8_14:32:36_2023": false, "Wed_Mar__8_17:41:10_2023": true, "Wed_Mar__8_13:30:46_2023": true, "Wed_Mar__8_14:52:32_2023": false, "Wed_Mar__8_19:57:46_2023": true, "Wed_Mar__8_18:46:15_2023": false, "Wed_Mar__8_13:55:56_2023": false, "Wed_Mar__8_14:00:45_2023": true, "Wed_Mar__8_14:45:19_2023": true, "Wed_Mar__8_14:47:21_2023": false, "Wed_Mar__8_19:45:08_2023": true, "Wed_Mar__8_14:38:35_2023": true, "Wed_Mar__8_14:46:52_2023": false, "Wed_Mar__8_17:45:54_2023": true, "Wed_Mar__8_19:03:12_2023": false, "Wed_Mar__8_13:29:25_2023": false, "Wed_Mar__8_19:55:40_2023": false, "Wed_Mar__8_14:52:47_2023": true, "Wed_Mar__8_17:13:40_2023": false, "Wed_Mar__8_18:52:42_2023": true, "Wed_Mar__8_16:41:33_2023": true, "Wed_Mar__8_17:17:16_2023": false, "Wed_Mar__8_19:32:43_2023": true, "Wed_Mar__8_14:49:39_2023": true, "Wed_Mar__8_17:47:53_2023": true, "Wed_Mar__8_17:09:23_2023": true, "Wed_Mar__8_14:47:04_2023": true, "Wed_Mar__8_19:38:24_2023": true, "Wed_Mar__8_14:49:59_2023": false, "Wed_Mar__8_17:05:26_2023": false, "Wed_Mar__8_16:56:04_2023": true, "Wed_Mar__8_19:51:36_2023": false, "Wed_Mar__8_19:47:46_2023": true, "Wed_Mar__8_14:36:28_2023": true, "Wed_Mar__8_13:56:12_2023": true, "Wed_Mar__8_14:51:46_2023": false, "Wed_Mar__8_18:47:34_2023": false, "Wed_Mar__8_16:49:36_2023": true, "Wed_Mar__8_13:23:39_2023": true, "Wed_Mar__8_13:28:07_2023": false, "Wed_Mar__8_19:41:15_2023": true, "Wed_Mar__8_19:52:34_2023": true, "Wed_Mar__8_19:55:12_2023": true, "Wed_Mar__8_16:36:04_2023": true, "Wed_Mar__8_13:24:30_2023": true, "Wed_Mar__8_19:53:45_2023": true, "Wed_Mar__8_17:53:49_2023": true, "Thu_Mar__2_17:44:32_2023": true, "Thu_Mar__2_18:24:16_2023": false, "Thu_Mar__2_16:51:35_2023": false, "Thu_Mar__2_16:19:05_2023": true, "Thu_Mar__2_22:03:44_2023": false, "Thu_Mar__2_22:03:04_2023": false, "Thu_Mar__2_22:08:09_2023": true, "Thu_Mar__2_14:59:06_2023": false, "Thu_Mar__2_17:44:45_2023": false, "Thu_Mar__2_15:20:32_2023": false, "Thu_Mar__2_17:33:41_2023": true, "Thu_Mar__2_16:19:23_2023": false, "Thu_Mar__2_16:37:19_2023": true, "Thu_Mar__2_21:55:25_2023": true, "Thu_Mar__2_18:27:05_2023": true, "Thu_Mar__2_17:42:46_2023": true, "Thu_Mar__2_17:28:56_2023": true, "Thu_Mar__2_17:01:05_2023": false, "Thu_Mar__2_22:03:19_2023": true, "Thu_Mar__2_16:09:11_2023": false, "Thu_Mar__2_15:11:12_2023": false, "Thu_Mar__2_16:21:08_2023": false, "Thu_Mar__2_17:30:46_2023": false, "Thu_Mar__2_15:22:54_2023": true, "Thu_Mar__2_18:13:38_2023": true, "Thu_Mar__2_18:32:45_2023": true, "Thu_Mar__2_15:58:43_2023": false, "Thu_Mar__2_16:35:27_2023": true, "Thu_Mar__2_21:55:09_2023": false, "Thu_Mar__2_16:57:04_2023": true, "Thu_Mar__2_21:49:43_2023": true, "Thu_Mar__2_14:58:51_2023": true, "Thu_Mar__2_15:14:31_2023": true, "Thu_Mar__2_17:32:35_2023": false, "Thu_Mar__2_18:22:50_2023": true, "Thu_Mar__2_15:18:29_2023": false, "Thu_Mar__2_16:32:41_2023": false, "Thu_Mar__2_15:12:06_2023": false, "Thu_Mar__2_16:52:26_2023": false, "Thu_Mar__2_15:01:03_2023": true, "Thu_Mar__2_15:21:36_2023": true, "Thu_Mar__2_16:58:28_2023": false, "Thu_Mar__2_15:24:53_2023": true, "Thu_Mar__2_18:31:38_2023": false, "Thu_Mar__2_15:15:50_2023": true, "Thu_Mar__2_22:06:34_2023": false, "Thu_Mar__2_14:57:27_2023": true, "Thu_Mar__2_16:46:19_2023": true, "Thu_Mar__2_22:04:03_2023": true, "Thu_Mar__2_21:53:15_2023": false, "Thu_Mar__2_17:30:12_2023": true, "Thu_Mar__2_15:59:27_2023": false, "Thu_Mar__2_22:05:48_2023": false, "Thu_Mar__2_21:49:07_2023": false, "Thu_Mar__2_16:46:53_2023": false, "Thu_Mar__2_21:57:41_2023": true, "Thu_Mar__2_18:30:44_2023": true, "Thu_Mar__2_15:00:02_2023": true, "Thu_Mar__2_16:35:46_2023": false, "Thu_Mar__2_21:56:33_2023": false, "Thu_Mar__2_16:16:02_2023": false, "Thu_Mar__2_16:58:49_2023": true, "Thu_Mar__2_16:30:36_2023": false, "Thu_Mar__2_21:48:51_2023": true, "Thu_Mar__2_16:01:58_2023": false, "Thu_Mar__2_17:34:13_2023": false, "Thu_Mar__2_17:00:45_2023": true, "Thu_Mar__2_17:31:39_2023": false, "Thu_Mar__2_21:52:18_2023": true, "Thu_Mar__2_16:18:44_2023": false, "Thu_Mar__2_22:07:33_2023": false, "Thu_Mar__2_22:02:40_2023": true, "Thu_Mar__2_16:30:17_2023": true, "Thu_Mar__2_16:19:42_2023": true, "Thu_Mar__2_17:01:47_2023": false, "Thu_Mar__2_17:32:54_2023": true, "Thu_Mar__2_15:21:18_2023": false, "Thu_Mar__2_15:02:15_2023": true, "Thu_Mar__2_15:20:06_2023": true, "Thu_Mar__2_16:39:33_2023": false, "Thu_Mar__2_16:59:31_2023": false, "Thu_Mar__2_16:53:07_2023": true, "Thu_Mar__2_16:40:33_2023": true, "Thu_Mar__2_17:42:31_2023": false, "Thu_Mar__2_15:21:53_2023": false, "Thu_Mar__2_17:45:34_2023": true, "Thu_Mar__2_15:11:49_2023": true, "Thu_Mar__2_21:56:09_2023": true, "Thu_Mar__2_22:07:52_2023": false, "Thu_Mar__2_15:16:31_2023": false, "Thu_Mar__2_22:08:45_2023": false, "Thu_Mar__2_21:52:39_2023": false, "Thu_Mar__2_16:54:33_2023": false, "Thu_Mar__2_21:53:52_2023": true, "Thu_Mar__2_16:55:30_2023": true, "Thu_Mar__2_16:58:04_2023": true, "Thu_Mar__2_17:28:16_2023": true, "Thu_Mar__2_17:44:59_2023": true, "Thu_Mar__2_17:46:11_2023": false, "Thu_Mar__2_15:17:35_2023": false, "Thu_Mar__2_22:06:53_2023": true, "Thu_Mar__2_16:00:19_2023": true, "Thu_Mar__2_21:50:17_2023": true, "Thu_Mar__2_17:01:28_2023": true, "Thu_Mar__2_15:12:50_2023": true, "Thu_Mar__2_21:52:58_2023": true, "Thu_Mar__2_15:23:14_2023": false, "Thu_Mar__2_21:51:04_2023": false, "Thu_Mar__2_15:01:40_2023": true, "Thu_Mar__2_16:17:34_2023": false, "Thu_Mar__2_16:45:55_2023": false, "Thu_Mar__2_16:37:00_2023": false, "Thu_Mar__2_16:36:40_2023": true, "Thu_Mar__2_16:32:05_2023": true, "Thu_Mar__2_16:30:55_2023": true, "Thu_Mar__2_15:18:52_2023": true, "Thu_Mar__2_15:20:48_2023": true, "Thu_Mar__2_17:29:20_2023": false, "Thu_Mar__2_18:23:35_2023": false, "Thu_Mar__2_15:03:35_2023": true, "Thu_Mar__2_16:29:38_2023": false, "Thu_Mar__2_18:29:55_2023": false, "Thu_Mar__2_16:56:03_2023": false, "Thu_Mar__2_17:33:27_2023": false, "Thu_Mar__2_17:31:58_2023": true, "Thu_Mar__2_16:52:05_2023": true, "Thu_Mar__2_18:21:35_2023": false, "Thu_Mar__2_16:53:46_2023": true, "Thu_Mar__2_21:51:21_2023": true, "Thu_Mar__2_14:57:54_2023": false, "Thu_Mar__2_15:04:20_2023": true, "Thu_Mar__2_22:05:30_2023": true, "Thu_Mar__2_22:04:33_2023": false, "Thu_Mar__2_16:45:35_2023": true, "Thu_Mar__2_16:53:25_2023": false, "Thu_Mar__2_15:17:53_2023": true, "Thu_Mar__2_15:13:56_2023": false, "Thu_Mar__2_22:06:11_2023": true, "Thu_Mar__2_15:01:57_2023": false, "Thu_Mar__2_16:02:18_2023": true, "Thu_Mar__2_18:26:46_2023": false, "Thu_Mar__2_16:17:08_2023": true, "Thu_Mar__2_18:23:50_2023": true, "Thu_Mar__2_15:03:01_2023": false, "Thu_Mar__2_18:28:03_2023": true, "Thu_Mar__2_17:31:02_2023": true, "Thu_Mar__2_15:00:48_2023": false, "Thu_Mar__2_17:28:34_2023": false, "Thu_Mar__2_18:26:28_2023": true, "Thu_Mar__2_15:26:13_2023": true, "Thu_Mar__2_21:51:55_2023": false, "Thu_Mar__2_15:04:02_2023": false, "Thu_Mar__2_16:41:02_2023": false, "Thu_Mar__2_15:01:22_2023": false, "Thu_Mar__2_15:50:04_2023": false, "Thu_Mar__2_15:25:41_2023": false, "Thu_Mar__2_15:16:51_2023": true, "Thu_Mar__2_15:14:58_2023": false, "Thu_Mar__2_18:32:20_2023": false, "Thu_Mar__2_16:31:25_2023": false, "Thu_Mar__2_21:50:00_2023": false, "Thu_Mar__2_17:39:23_2023": true, "Thu_Mar__2_16:28:51_2023": true, "Thu_Mar__2_17:43:01_2023": false, "Thu_Mar__2_16:55:08_2023": false, "Thu_Mar__2_21:58:18_2023": false, "Thu_Mar__2_18:31:56_2023": true, "Thu_Mar__2_15:59:03_2023": true, "Thu_Mar__9_19:30:35_2023": true, "Thu_Mar__9_18:10:27_2023": true, "Thu_Mar__9_18:31:40_2023": false, "Thu_Mar__9_18:38:30_2023": true, "Thu_Mar__9_19:51:57_2023": false, "Thu_Mar__9_19:50:00_2023": false, "Thu_Mar__9_18:36:35_2023": true, "Thu_Mar__9_18:04:09_2023": true, "Thu_Mar__9_19:37:02_2023": false, "Thu_Mar__9_19:38:37_2023": false, "Thu_Mar__9_18:37:08_2023": false, "Thu_Mar__9_18:05:58_2023": true, "Thu_Mar__9_18:32:30_2023": false, "Thu_Mar__9_19:42:55_2023": false, "Thu_Mar__9_19:51:39_2023": true, "Thu_Mar__9_18:31:16_2023": true, "Thu_Mar__9_18:33:31_2023": false, "Thu_Mar__9_19:49:40_2023": true, "Thu_Mar__9_18:29:56_2023": false, "Thu_Mar__9_19:28:14_2023": false, "Thu_Mar__9_18:20:45_2023": false, "Thu_Mar__9_17:57:41_2023": false, "Thu_Mar__9_18:40:22_2023": true, "Thu_Mar__9_19:48:30_2023": false, "Thu_Mar__9_17:55:01_2023": false, "Thu_Mar__9_18:43:39_2023": false, "Thu_Mar__9_19:32:19_2023": true, "Thu_Mar__9_18:03:25_2023": true, "Thu_Mar__9_19:39:56_2023": false, "Thu_Mar__9_17:58:35_2023": false, "Thu_Mar__9_19:35:22_2023": true, "Thu_Mar__9_19:31:46_2023": false, "Thu_Mar__9_18:38:49_2023": false, "Thu_Mar__9_19:34:44_2023": false, "Thu_Mar__9_18:54:03_2023": true, "Thu_Mar__9_18:52:03_2023": false, "Thu_Mar__9_19:37:28_2023": true, "Thu_Mar__9_18:31:57_2023": true, "Thu_Mar__9_18:42:16_2023": false, "Thu_Mar__9_18:34:04_2023": false, "Thu_Mar__9_18:45:09_2023": true, "Thu_Mar__9_19:02:25_2023": false, "Thu_Mar__9_19:42:36_2023": true, "Thu_Mar__9_18:22:08_2023": true, "Thu_Mar__9_19:41:03_2023": false, "Thu_Mar__9_19:25:31_2023": true, "Thu_Mar__9_19:50:27_2023": true, "Thu_Mar__9_18:51:46_2023": true, "Thu_Mar__9_19:27:39_2023": true, "Thu_Mar__9_18:29:21_2023": true, "Thu_Mar__9_18:09:22_2023": false, "Thu_Mar__9_19:23:14_2023": false, "Thu_Mar__9_19:26:31_2023": true, "Thu_Mar__9_19:47:44_2023": false, "Thu_Mar__9_19:44:20_2023": true, "Thu_Mar__9_19:06:29_2023": false, "Thu_Mar__9_18:50:48_2023": false, "Thu_Mar__9_18:18:52_2023": false, "Thu_Mar__9_19:33:39_2023": false, "Thu_Mar__9_18:35:41_2023": false, "Thu_Mar__9_17:58:51_2023": true, "Thu_Mar__9_18:41:24_2023": true, "Thu_Mar__9_18:01:58_2023": true, "Thu_Mar__9_18:14:34_2023": true, "Thu_Mar__9_18:30:36_2023": false, "Thu_Mar__9_18:06:34_2023": false, "Thu_Mar__9_18:21:03_2023": true, "Thu_Mar__9_18:02:54_2023": false, "Thu_Mar__9_18:02:33_2023": true, "Thu_Mar__9_19:43:41_2023": false, "Thu_Mar__9_17:59:58_2023": true, "Thu_Mar__9_18:53:40_2023": false, "Thu_Mar__9_18:18:24_2023": true, "Thu_Mar__9_17:59:34_2023": false, "Thu_Mar__9_18:41:07_2023": false, "Thu_Mar__9_19:45:10_2023": true, "Thu_Mar__9_19:52:13_2023": true, "Thu_Mar__9_18:34:20_2023": true, "Thu_Mar__9_18:10:47_2023": false, "Thu_Mar__9_19:43:17_2023": true, "Thu_Mar__9_17:55:55_2023": false, "Thu_Mar__9_18:33:11_2023": true, "Thu_Mar__9_18:37:26_2023": true, "Thu_Mar__9_19:34:23_2023": true, "Thu_Mar__9_18:13:04_2023": true, "Thu_Mar__9_18:03:45_2023": false, "Thu_Mar__9_19:05:15_2023": true, "Thu_Mar__9_19:05:56_2023": true, "Thu_Mar__9_19:36:07_2023": false, "Thu_Mar__9_18:46:12_2023": false, "Thu_Mar__9_18:00:50_2023": false, "Thu_Mar__9_18:53:09_2023": true, "Thu_Mar__9_18:14:17_2023": false, "Thu_Mar__9_18:30:16_2023": true, "Thu_Mar__9_19:22:35_2023": false, "Thu_Mar__9_19:27:25_2023": false, "Thu_Mar__9_19:25:09_2023": false, "Thu_Mar__9_18:47:35_2023": true, "Thu_Mar__9_17:54:36_2023": true, "Thu_Mar__9_19:48:15_2023": true, "Thu_Mar__9_19:24:29_2023": false, "Thu_Mar__9_19:49:26_2023": false, "Thu_Mar__9_18:35:09_2023": true, "Thu_Mar__9_19:04:56_2023": false, "Thu_Mar__9_17:56:28_2023": true, "Thu_Mar__9_19:45:26_2023": false, "Thu_Mar__9_19:51:24_2023": false, "Thu_Mar__9_18:12:22_2023": false, "Thu_Mar__9_19:39:11_2023": true, "Thu_Mar__9_18:01:17_2023": true, "Thu_Mar__9_18:05:39_2023": false, "Thu_Mar__9_18:01:38_2023": false, "Thu_Mar__9_17:57:04_2023": true, "Thu_Mar__9_18:07:38_2023": true, "Thu_Mar__9_19:40:23_2023": true, "Thu_Mar__9_19:46:10_2023": true, "Thu_Mar__9_18:21:49_2023": false, "Thu_Mar__9_19:22:55_2023": true, "Thu_Mar__9_19:24:00_2023": true, "Thu_Mar__9_18:47:19_2023": true, "Thu_Mar__9_19:42:00_2023": false, "Thu_Mar__9_19:44:40_2023": false, "Thu_Mar__9_18:45:45_2023": true, "Thu_Mar__9_18:43:18_2023": true, "Thu_Mar__9_17:56:45_2023": false, "Thu_Mar__9_18:22:27_2023": false, "Thu_Mar__9_18:19:19_2023": true, "Thu_Mar__9_18:08:47_2023": true, "Thu_Mar__9_18:04:41_2023": false, "Thu_Mar__9_17:54:19_2023": false, "Thu_Mar__9_19:38:18_2023": true, "Thu_Mar__9_18:36:55_2023": false, "Thu_Mar__9_18:11:54_2023": true, "Thu_Mar__9_19:25:54_2023": false, "Thu_Mar__9_18:45:27_2023": false, "Thu_Mar__9_19:24:47_2023": true, "Thu_Mar__9_19:48:45_2023": true, "Thu_Mar__9_19:36:25_2023": true, "Thu_Mar__9_18:05:20_2023": true, "Thu_Mar__9_17:55:34_2023": true, "Thu_Mar__9_19:41:23_2023": true, "Thu_Mar__9_18:38:07_2023": false, "Thu_Mar__9_17:57:58_2023": true, "Thu_Mar__9_18:07:57_2023": false, "Wed_Mar__1_15:36:32_2023": false, "Wed_Mar__1_20:45:42_2023": true, "Wed_Mar__1_15:59:02_2023": false, "Wed_Mar__1_21:25:33_2023": false, "Wed_Mar__1_20:38:11_2023": false, "Wed_Mar__1_15:51:14_2023": true, "Wed_Mar__1_15:53:17_2023": false, "Wed_Mar__1_15:26:27_2023": false, "Wed_Mar__1_20:37:13_2023": false, "Wed_Mar__1_16:04:29_2023": true, "Wed_Mar__1_20:29:20_2023": true, "Wed_Mar__1_15:52:46_2023": true, "Wed_Mar__1_19:54:33_2023": true, "Wed_Mar__1_21:27:39_2023": false, "Wed_Mar__1_19:24:05_2023": true, "Wed_Mar__1_19:15:31_2023": false, "Wed_Mar__1_20:34:27_2023": true, "Wed_Mar__1_16:25:11_2023": true, "Wed_Mar__1_15:46:14_2023": false, "Wed_Mar__1_15:47:58_2023": false, "Wed_Mar__1_20:00:25_2023": true, "Wed_Mar__1_21:35:44_2023": false, "Wed_Mar__1_19:13:26_2023": false, "Wed_Mar__1_21:50:13_2023": true, "Wed_Mar__1_15:28:02_2023": true, "Wed_Mar__1_19:55:52_2023": false, "Wed_Mar__1_19:17:05_2023": true, "Wed_Mar__1_15:54:41_2023": true, "Wed_Mar__1_15:29:29_2023": true, "Wed_Mar__1_20:44:51_2023": false, "Wed_Mar__1_20:44:00_2023": true, "Wed_Mar__1_21:31:58_2023": false, "Wed_Mar__1_16:02:35_2023": true, "Wed_Mar__1_21:34:14_2023": true, "Wed_Mar__1_16:04:49_2023": false, "Wed_Mar__1_16:27:43_2023": false, "Wed_Mar__1_16:28:01_2023": true, "Wed_Mar__1_16:01:03_2023": false, "Wed_Mar__1_21:28:26_2023": true, "Wed_Mar__1_20:34:49_2023": false, "Wed_Mar__1_16:27:23_2023": true, "Wed_Mar__1_15:50:11_2023": false, "Wed_Mar__1_20:31:05_2023": false, "Wed_Mar__1_21:35:19_2023": true, "Wed_Mar__1_21:46:33_2023": true, "Wed_Mar__1_20:24:55_2023": true, "Wed_Mar__1_21:32:21_2023": true, "Wed_Mar__1_15:56:59_2023": false, "Wed_Mar__1_20:27:43_2023": false, "Wed_Mar__1_19:56:14_2023": true, "Wed_Mar__1_20:37:52_2023": true, "Wed_Mar__1_15:56:04_2023": true, "Wed_Mar__1_20:35:31_2023": true, "Wed_Mar__1_21:26:47_2023": true, "Wed_Mar__1_16:28:44_2023": true, "Wed_Mar__1_20:01:54_2023": false, "Wed_Mar__1_20:32:56_2023": true, "Wed_Mar__1_15:58:43_2023": true, "Wed_Mar__1_21:26:25_2023": false, "Wed_Mar__1_20:01:38_2023": true, "Wed_Mar__1_19:14:17_2023": true, "Wed_Mar__1_19:11:44_2023": true, "Wed_Mar__1_19:58:54_2023": true, "Wed_Mar__1_16:29:03_2023": false, "Wed_Mar__1_19:55:04_2023": false, "Wed_Mar__1_20:23:44_2023": false, "Wed_Mar__1_19:12:04_2023": false, "Wed_Mar__1_15:45:53_2023": true, "Wed_Mar__1_21:45:46_2023": false, "Wed_Mar__1_16:04:12_2023": false, "Wed_Mar__1_15:37:08_2023": false, "Wed_Mar__1_15:49:53_2023": true, "Wed_Mar__1_16:26:17_2023": true, "Wed_Mar__1_19:10:06_2023": true, "Wed_Mar__1_21:30:44_2023": false, "Wed_Mar__1_15:40:34_2023": true, "Wed_Mar__1_21:43:59_2023": false, "Wed_Mar__1_15:55:39_2023": false, "Wed_Mar__1_16:03:10_2023": false, "Wed_Mar__1_21:33:34_2023": false, "Wed_Mar__1_21:29:02_2023": false, "Wed_Mar__1_15:47:21_2023": false, "Wed_Mar__1_19:16:14_2023": true, "Wed_Mar__1_21:44:56_2023": true, "Wed_Mar__1_20:33:43_2023": false, "Wed_Mar__1_21:34:44_2023": false, "Wed_Mar__1_21:46:52_2023": false, "Wed_Mar__1_20:01:18_2023": false, "Wed_Mar__1_15:55:20_2023": true, "Wed_Mar__1_19:57:49_2023": true, "Wed_Mar__1_20:31:48_2023": false, "Wed_Mar__1_15:36:54_2023": true, "Wed_Mar__1_15:46:45_2023": true, "Wed_Mar__1_19:16:31_2023": false, "Wed_Mar__1_21:49:17_2023": true, "Wed_Mar__1_15:35:02_2023": true, "Wed_Mar__1_20:46:37_2023": false, "Wed_Mar__1_16:00:32_2023": true, "Wed_Mar__1_19:19:00_2023": true, "Wed_Mar__1_21:44:22_2023": true, "Wed_Mar__1_19:59:46_2023": true, "Wed_Mar__1_15:58:22_2023": false, "Wed_Mar__1_21:31:25_2023": true, "Wed_Mar__1_19:55:33_2023": true, "Wed_Mar__1_15:57:57_2023": true, "Wed_Mar__1_20:23:26_2023": true, "Wed_Mar__1_21:43:34_2023": true, "Wed_Mar__1_15:47:39_2023": true, "Wed_Mar__1_21:29:28_2023": true, "Wed_Mar__1_19:10:48_2023": false, "Wed_Mar__1_15:51:30_2023": false, "Wed_Mar__1_21:49:42_2023": false, "Wed_Mar__1_21:48:14_2023": false, "Wed_Mar__1_20:35:50_2023": false, "Wed_Mar__1_15:26:48_2023": true, "Wed_Mar__1_20:26:41_2023": true, "Wed_Mar__1_21:25:57_2023": true, "Wed_Mar__1_19:15:08_2023": true, "Wed_Mar__1_15:49:15_2023": false, "Wed_Mar__1_20:31:28_2023": true, "Wed_Mar__1_20:00:05_2023": false, "Wed_Mar__1_16:26:36_2023": false, "Wed_Mar__1_19:23:29_2023": false, "Wed_Mar__1_16:03:54_2023": true, "Wed_Mar__1_19:14:51_2023": false, "Wed_Mar__1_15:55:01_2023": false, "Wed_Mar__1_20:29:40_2023": false, "Wed_Mar__1_20:30:28_2023": true, "Wed_Mar__1_20:38:29_2023": false, "Wed_Mar__1_21:30:20_2023": true, "Wed_Mar__1_15:51:48_2023": true, "Wed_Mar__1_21:47:54_2023": true, "Wed_Mar__1_15:52:26_2023": false, "Wed_Mar__1_21:29:48_2023": false, "Wed_Mar__1_16:25:31_2023": false, "Wed_Mar__1_19:17:23_2023": false, "Mon_Mar_13_15:03:47_2023": false, "Mon_Mar_13_15:03:32_2023": true, "Mon_Mar_13_15:06:59_2023": false, "Mon_Mar_13_15:13:32_2023": false, "Mon_Mar_13_15:01:29_2023": true, "Mon_Mar_13_15:11:33_2023": true, "Mon_Mar_13_15:12:28_2023": true, "Mon_Mar_13_15:10:14_2023": true, "Mon_Mar_13_15:14:13_2023": true, "Mon_Mar_13_15:04:17_2023": true, "Mon_Mar_13_15:05:19_2023": true, "Mon_Mar_13_15:08:33_2023": true, "Mon_Mar_13_15:08:06_2023": false, "Mon_Mar_13_15:08:50_2023": false, "Mon_Mar_13_15:10:28_2023": false, "Mon_Mar_13_15:10:45_2023": true, "Mon_Mar_13_15:13:13_2023": true, "Mon_Mar_13_15:09:22_2023": true, "Mon_Mar_13_15:12:59_2023": false, "Mon_Mar_13_15:01:42_2023": false, "Mon_Mar_13_15:11:20_2023": false, "Mon_Mar_13_15:04:41_2023": false, "Mon_Mar_13_15:06:17_2023": true, "Mon_Mar_13_15:12:08_2023": false, "Mon_Mar_13_15:07:23_2023": true, "Tue_Feb_28_20:32:26_2023": true, "Tue_Feb_28_21:16:21_2023": false, "Tue_Feb_28_21:03:30_2023": true, "Tue_Feb_28_21:12:34_2023": false, "Tue_Feb_28_21:05:56_2023": true, "Tue_Feb_28_21:10:07_2023": false, "Tue_Feb_28_21:07:36_2023": false, "Tue_Feb_28_20:26:11_2023": false, "Tue_Feb_28_20:26:36_2023": true, "Tue_Feb_28_21:05:16_2023": false, "Tue_Feb_28_20:28:29_2023": true, "Tue_Feb_28_21:04:24_2023": true, "Tue_Feb_28_20:33:14_2023": true, "Tue_Feb_28_21:03:55_2023": false, "Tue_Feb_28_21:09:33_2023": true, "Tue_Feb_28_20:28:13_2023": false, "Tue_Feb_28_20:32:08_2023": false, "Tue_Feb_28_21:13:38_2023": true, "Tue_Feb_28_21:00:44_2023": false, "Tue_Feb_28_21:11:33_2023": true, "Fri_Mar_10_13:04:34_2023": true, "Fri_Mar_10_14:02:11_2023": true, "Fri_Mar_10_13:08:29_2023": true, "Fri_Mar_10_14:01:34_2023": true, "Fri_Mar_10_13:00:22_2023": true, "Fri_Mar_10_13:04:06_2023": false, "Fri_Mar_10_15:09:16_2023": true, "Fri_Mar_10_13:03:06_2023": true, "Fri_Mar_10_16:14:02_2023": true, "Fri_Mar_10_14:10:32_2023": true, "Fri_Mar_10_14:05:39_2023": true, "Fri_Mar_10_14:09:58_2023": false, "Fri_Mar_10_14:05:03_2023": false, "Fri_Mar_10_14:06:41_2023": true, "Mon_Mar__6_17:22:24_2023": true, "Mon_Mar__6_17:22:45_2023": false, "Mon_Mar__6_17:09:39_2023": true, "Mon_Mar__6_16:07:02_2023": true, "Mon_Mar__6_15:38:26_2023": true, "Mon_Mar__6_15:50:44_2023": true, "Mon_Mar__6_15:51:10_2023": false, "Mon_Mar__6_17:23:23_2023": false, "Mon_Mar__6_16:13:42_2023": false, "Mon_Mar__6_16:02:37_2023": true, "Mon_Mar__6_17:21:38_2023": true, "Mon_Mar__6_15:39:56_2023": false, "Mon_Mar__6_16:03:09_2023": true, "Mon_Mar__6_17:37:59_2023": false, "Mon_Mar__6_17:44:07_2023": false, "Mon_Mar__6_16:14:26_2023": true, "Mon_Mar__6_17:05:24_2023": false, "Mon_Mar__6_15:41:02_2023": false, "Mon_Mar__6_15:49:29_2023": false, "Mon_Mar__6_15:39:08_2023": true, "Mon_Mar__6_17:45:22_2023": false, "Mon_Mar__6_17:27:17_2023": false, "Mon_Mar__6_16:04:29_2023": true, "Mon_Mar__6_16:06:43_2023": false, "Mon_Mar__6_17:24:17_2023": true, "Mon_Mar__6_16:11:31_2023": true, "Mon_Mar__6_17:24:37_2023": false, "Mon_Mar__6_17:37:04_2023": true, "Mon_Mar__6_17:06:52_2023": true, "Mon_Mar__6_16:08:09_2023": false, "Mon_Mar__6_15:40:28_2023": true, "Mon_Mar__6_16:13:09_2023": false, "Mon_Mar__6_17:35:49_2023": true, "Mon_Mar__6_17:39:46_2023": true, "Mon_Mar__6_17:38:22_2023": true, "Mon_Mar__6_16:10:54_2023": true, "Mon_Mar__6_15:38:48_2023": false, "Mon_Mar__6_16:05:48_2023": false, "Mon_Mar__6_17:39:11_2023": true, "Mon_Mar__6_16:08:54_2023": true, "Mon_Mar__6_17:05:42_2023": true, "Mon_Mar__6_17:45:07_2023": true, "Mon_Mar__6_15:51:32_2023": true, "Mon_Mar__6_17:06:15_2023": false, "Mon_Mar__6_16:03:51_2023": false, "Mon_Mar__6_17:39:29_2023": false, "Mon_Mar__6_17:09:01_2023": false, "Mon_Mar__6_15:52:15_2023": true, "Mon_Mar__6_17:27:33_2023": true, "Mon_Mar__6_16:15:46_2023": false, "Mon_Mar__6_16:06:23_2023": true, "Mon_Mar__6_16:16:04_2023": true, "Mon_Mar__6_16:10:27_2023": false, "Mon_Mar__6_17:24:53_2023": true, "Mon_Mar__6_16:04:56_2023": false, "Mon_Mar__6_17:44:52_2023": false, "Mon_Mar__6_17:44:35_2023": true, "Mon_Mar__6_16:13:25_2023": true, "Mon_Mar__6_16:05:18_2023": true, "Mon_Mar__6_17:28:44_2023": true, "Mon_Mar__6_17:36:12_2023": false, "Mon_Mar__6_15:52:59_2023": false, "Mon_Mar__6_17:46:06_2023": true, "Fri_Mar_10_13:02:46_2023": false, "Fri_Mar_10_14:01:53_2023": false, "Fri_Mar_10_16:04:11_2023": true, "Fri_Mar_10_13:06:35_2023": false, "Fri_Mar_10_13:01:16_2023": true, "Fri_Mar_10_13:06:17_2023": true, "Fri_Mar_10_14:02:49_2023": false, "Fri_Mar_10_14:04:41_2023": true, "Fri_Mar_10_14:06:16_2023": false, "Fri_Mar_10_15:08:06_2023": false, "Fri_Mar_10_14:07:19_2023": false, "Fri_Mar_10_14:04:21_2023": false, "Fri_Mar_10_13:01:02_2023": false, "Fri_Mar_10_14:07:39_2023": true, "Fri_Mar_10_13:06:02_2023": false, "Fri_Mar_10_15:10:15_2023": false, "Fri_Mar_10_14:01:13_2023": false, "Fri_Mar_10_13:00:05_2023": false, "Fri_Mar_10_13:07:44_2023": false, "Fri_Mar_10_14:11:00_2023": false, "Fri_Mar_10_14:03:19_2023": true, "Fri_Mar_10_16:20:28_2023": false, "Fri_Mar_10_12:59:39_2023": true, "Fri_Mar_10_13:06:55_2023": true, "Fri_Mar_10_14:16:25_2023": true, "Fri_Mar_10_15:16:00_2023": true, "Tue_Mar__7_18:04:56_2023": false, "Tue_Mar__7_17:11:14_2023": false, "Tue_Mar__7_18:06:10_2023": false, "Tue_Mar__7_18:02:14_2023": true, "Tue_Mar__7_15:56:34_2023": true, "Tue_Mar__7_15:32:40_2023": false, "Tue_Mar__7_15:49:04_2023": true, "Tue_Mar__7_17:11:50_2023": true, "Tue_Mar__7_16:09:49_2023": true, "Tue_Mar__7_15:55:16_2023": false, "Tue_Mar__7_16:15:13_2023": false, "Tue_Mar__7_15:33:17_2023": true, "Tue_Mar__7_17:12:27_2023": true, "Tue_Mar__7_17:51:17_2023": true, "Tue_Mar__7_17:49:12_2023": false, "Tue_Mar__7_15:57:56_2023": false, "Tue_Mar__7_17:47:37_2023": true, "Tue_Mar__7_17:06:50_2023": true, "Tue_Mar__7_16:15:39_2023": true, "Tue_Mar__7_15:53:44_2023": true, "Tue_Mar__7_15:58:31_2023": true, "Tue_Mar__7_17:56:12_2023": false, "Tue_Mar__7_17:13:44_2023": true, "Tue_Mar__7_17:10:57_2023": true, "Tue_Mar__7_16:20:16_2023": false, "Tue_Mar__7_15:55:51_2023": true, "Tue_Mar__7_18:02:52_2023": false, "Tue_Mar__7_15:57:36_2023": true, "Tue_Mar__7_15:36:10_2023": true, "Tue_Mar__7_17:57:12_2023": false, "Tue_Mar__7_15:56:55_2023": false, "Tue_Mar__7_18:06:50_2023": false, "Tue_Mar__7_17:06:30_2023": false, "Tue_Mar__7_15:34:38_2023": false, "Tue_Mar__7_17:52:44_2023": true, "Tue_Mar__7_17:10:17_2023": true, "Tue_Mar__7_16:16:01_2023": false, "Tue_Mar__7_18:06:29_2023": true, "Tue_Mar__7_17:12:06_2023": false, "Tue_Mar__7_17:48:06_2023": false, "Tue_Mar__7_17:14:32_2023": true, "Tue_Mar__7_15:31:52_2023": false, "Tue_Mar__7_17:20:57_2023": true, "Tue_Mar__7_15:43:45_2023": true, "Tue_Mar__7_17:56:30_2023": true, "Tue_Mar__7_15:35:03_2023": true, "Tue_Mar__7_16:19:20_2023": true, "Tue_Mar__7_17:01:12_2023": true, "Tue_Mar__7_17:01:58_2023": false, "Tue_Mar__7_17:07:30_2023": false, "Tue_Mar__7_15:32:17_2023": true, "Tue_Mar__7_17:58:04_2023": false, "Tue_Mar__7_17:13:03_2023": true, "Tue_Mar__7_16:18:02_2023": false, "Tue_Mar__7_15:43:24_2023": false, "Tue_Mar__7_17:09:20_2023": false, "Tue_Mar__7_16:17:28_2023": true, "Tue_Mar__7_16:09:26_2023": false, "Tue_Mar__7_17:59:02_2023": false, "Tue_Mar__7_17:20:19_2023": true, "Tue_Mar__7_16:19:59_2023": true, "Tue_Mar__7_15:56:13_2023": false, "Tue_Mar__7_17:13:19_2023": false, "Tue_Mar__7_15:35:49_2023": false, "Tue_Mar__7_17:55:50_2023": true, "Tue_Mar__7_15:54:55_2023": true, "Tue_Mar__7_17:59:54_2023": false, "Tue_Mar__7_17:02:59_2023": true, "Tue_Mar__7_15:31:32_2023": true, "Tue_Mar__7_17:12:45_2023": false, "Tue_Mar__7_17:47:20_2023": false, "Tue_Mar__7_18:04:12_2023": true, "Tue_Mar__7_15:38:09_2023": false, "Tue_Mar__7_17:07:47_2023": true, "Tue_Mar__7_17:21:55_2023": false, "Tue_Mar__7_15:54:28_2023": false, "Tue_Mar__7_16:20:41_2023": true, "Tue_Mar__7_17:08:54_2023": true, "Tue_Mar__7_15:44:05_2023": false, "Tue_Mar__7_15:42:42_2023": true, "Tue_Mar__7_17:09:38_2023": true, "Tue_Mar__7_18:05:32_2023": true, "Tue_Mar__7_15:34:14_2023": true, "Tue_Mar__7_17:54:05_2023": true, "Tue_Mar__7_17:48:39_2023": true, "Tue_Mar__7_15:44:56_2023": true, "Tue_Mar__7_17:02:24_2023": true, "Tue_Mar__7_17:02:42_2023": false, "Tue_Mar__7_17:22:13_2023": true, "Tue_Mar__7_15:48:43_2023": false, "Tue_Mar__7_15:37:02_2023": true, "Tue_Mar__7_16:09:03_2023": true, "Tue_Mar__7_17:14:53_2023": false, "Tue_Mar__7_16:17:03_2023": false, "Tue_Mar__7_18:03:26_2023": true, "Tue_Mar__7_17:46:50_2023": true, "Tue_Mar__7_17:52:05_2023": false, "Tue_Mar__7_15:33:45_2023": false, "Tue_Mar__7_17:57:28_2023": true, "Tue_Mar__7_15:46:19_2023": true, "Tue_Mar__7_15:42:08_2023": false, "Tue_Mar__7_17:53:10_2023": false, "Tue_Mar__7_15:36:40_2023": false, "Tue_Mar__7_17:58:30_2023": true, "Tue_Mar__7_15:39:44_2023": true, "Tue_Mar__7_16:16:38_2023": true, "Fri_Apr__7_13:23:33_2023": true, "Fri_Apr__7_13:34:38_2023": false, "Fri_Apr__7_14:09:11_2023": false, "Fri_Apr__7_13:52:30_2023": false, "Fri_Apr__7_13:56:22_2023": true, "Fri_Apr__7_14:10:25_2023": true, "Fri_Apr__7_13:35:09_2023": true, "Fri_Apr__7_13:31:28_2023": false, "Fri_Apr__7_13:37:35_2023": false, "Fri_Apr__7_13:25:39_2023": true, "Fri_Apr__7_13:34:13_2023": true, "Fri_Apr__7_13:23:53_2023": false, "Fri_Apr__7_14:07:59_2023": false, "Fri_Apr__7_14:09:58_2023": false, "Fri_Apr__7_13:54:42_2023": false, "Fri_Apr__7_14:10:53_2023": false, "Fri_Apr__7_13:49:26_2023": true, "Fri_Apr__7_14:05:45_2023": true, "Fri_Apr__7_13:32:05_2023": true, "Fri_Apr__7_14:09:30_2023": true, "Fri_Apr__7_13:43:59_2023": false, "Fri_Apr__7_13:41:05_2023": false, "Fri_Apr__7_13:29:09_2023": false, "Fri_Apr__7_13:30:39_2023": false, "Fri_Apr__7_14:11:09_2023": true, "Fri_Apr__7_14:12:34_2023": true, "Fri_Apr__7_14:06:07_2023": false, "Fri_Apr__7_13:41:24_2023": true, "Fri_Apr__7_14:12:52_2023": false, "Fri_Apr__7_13:51:11_2023": true, "Fri_Apr__7_14:03:27_2023": true, "Fri_Apr__7_14:08:53_2023": true, "Fri_Apr__7_14:07:23_2023": true, "Fri_Apr__7_13:37:55_2023": true, "Fri_Apr__7_13:26:16_2023": false, "Fri_Apr__7_13:41:49_2023": false, "Fri_Apr__7_13:28:33_2023": true, "Fri_Apr__7_14:15:06_2023": false, "Fri_Apr__7_14:06:24_2023": true, "Fri_Apr__7_13:53:25_2023": true, "Fri_Apr__7_13:26:32_2023": true, "Fri_Apr__7_13:55:32_2023": false, "Fri_Apr__7_13:54:17_2023": true, "Fri_Apr__7_13:28:01_2023": false, "Fri_Apr__7_13:40:13_2023": false, "Fri_Apr__7_13:40:32_2023": true, "Fri_Apr__7_13:55:01_2023": true, "Fri_Apr__7_13:48:45_2023": true, "Fri_Apr__7_14:04:17_2023": true, "Fri_Apr__7_13:42:08_2023": true, "Fri_Apr__7_13:54:01_2023": false, "Fri_Apr__7_13:50:56_2023": false, "Fri_Apr__7_13:50:35_2023": true, "Fri_Apr__7_14:01:41_2023": true, "Fri_Apr__7_14:12:14_2023": false, "Fri_Apr__7_13:32:23_2023": false, "Fri_Apr__7_13:24:31_2023": true, "Fri_Apr__7_14:02:01_2023": false, "Fri_Apr__7_14:13:52_2023": true, "Fri_Apr__7_14:02:37_2023": false, "Fri_Apr__7_14:03:57_2023": false, "Fri_Apr__7_13:52:47_2023": true, "Fri_Apr__7_13:24:53_2023": false, "Fri_Apr__7_13:47:38_2023": true, "Fri_Apr__7_14:02:21_2023": true, "Fri_Apr__7_13:48:00_2023": false, "Fri_Apr__7_13:29:41_2023": true, "Fri_Apr__7_13:53:04_2023": false, "Fri_Apr__7_14:14:15_2023": false, "Fri_Apr__7_14:07:04_2023": false, "Fri_Apr__7_14:04:32_2023": false, "Fri_Apr__7_14:08:13_2023": true, "Fri_Apr__7_14:00:48_2023": false, "Fri_Apr__7_13:32:40_2023": true, "Fri_Apr__7_13:49:06_2023": false, "Fri_Apr__7_14:14:47_2023": true, "Fri_Apr__7_13:31:06_2023": true, "Fri_Apr__7_14:08:29_2023": false}



================================================
FILE: scripts/server/launch_server.sh
================================================
#!/bin/bash
source ~/anaconda3/etc/profile.d/conda.sh
conda activate polymetis-local
cd $( cd -- "$( dirname -- "${BASH_SOURCE[0]}" )" &> /dev/null && pwd )
python run_server.py



================================================
FILE: scripts/server/run_server.py
================================================
import zerorpc

from droid.franka.robot import FrankaRobot

if __name__ == "__main__":
    robot_client = FrankaRobot()
    s = zerorpc.Server(robot_client)
    s.bind("tcp://0.0.0.0:4242")
    s.run()



================================================
FILE: scripts/setup/README.md
================================================
# Overview

The scripts in this directory can be run to setup the NUC and laptop for the DROID project. Before running these scripts the following steps need to be accomplished:

* Install Ubuntu 22.04
* Setup Free Ubuntu Pro Account 
* Complete system parameters file 

# Install latest Ubuntu 

The following [tutorial](https://ubuntu.com/tutorials/install-ubuntu-desktop#1-overview) demonstrates how to install Ubuntu 22.04 on your machine.

# Setup Ubuntu Pro Account

Register an Ubuntu Pro account [here](https://ubuntu.com/pro). Note your Ubuntu Pro token from your Ubuntu Pro dashboard which will be viewable once you have created an account and logged in.

# Complete parameters
You will need to add the IP parameters, sudo password, robot type, robot serial number, Ubuntu Pro token and Charuco board paramerers in the [parameters.py](https://github.com/droid-dataset/droid/blob/main/droid/misc/parameters.py) file.

You will need to add your the IP address of your Franka Emika control server to the `robot_ip` parameter in `config/<your robot type>/franka_hardware.yaml`.

# NUC Setup

Run `sudo ./nuc_setup.sh`. This script accomplishes the following steps:

* Ensure all dependent submodules are pulled locally
* Installs Docker
* Installs realtime kernel and sets to default
* Disables CPU frequency scaling and sets CPU governor to performance
* Builds docker container for control server
* Sets a static IP
* Runs control server docker container with restart policy that ensure it starts on system boot


# Laptop Setup  

Run `sudo ./laptop_setup.sh`. This script accomplishes the following steps:

* Ensure all dependent submodules are pulled locally
* Installs Docker
* Installs and configures nvidia container toolkit
* Installs APK on Oculus device
* Builds docker container for user client
* Sets a static IP
* Runs user client docker container



================================================
FILE: scripts/setup/intro.txt
================================================
 .----------------.  .----------------.  .----------------.  .----------------.                     
| .--------------. || .--------------. || .--------------. || .--------------. |                    
| |  _______     | || |    _____     | || |  ________    | || |    _____     | |                    
| | |_   __ \    | || |   / ___ `.   | || | |_   ___ `.  | || |   / ___ `.   | |                    
| |   | |__) |   | || |  |_/___) |   | || |   | |   `. \ | || |  |_/___) |   | |                    
| |   |  __ /    | || |   .'____.'   | || |   | |    | | | || |   .'____.'   | |                    
| |  _| |  \ \_  | || |  / /____     | || |  _| |___.' / | || |  / /____     | |                    
| | |____| |___| | || |  |_______|   | || | |________.'  | || |  |_______|   | |                    
| |              | || |              | || |              | || |              | |                    
| '--------------' || '--------------' || '--------------' || '--------------' |                    
 '----------------'  '----------------'  '----------------'  '----------------'                     
 .----------------.  .----------------.  .----------------.  .----------------.  .----------------. 
| .--------------. || .--------------. || .--------------. || .--------------. || .--------------. |
| |    _______   | || |  _________   | || |  _________   | || | _____  _____ | || |   ______     | |
| |   /  ___  |  | || | |_   ___  |  | || | |  _   _  |  | || ||_   _||_   _|| || |  |_   __ \   | |
| |  |  (__ \_|  | || |   | |_  \_|  | || | |_/ | | \_|  | || |  | |    | |  | || |    | |__) |  | |
| |   '.___`-.   | || |   |  _|  _   | || |     | |      | || |  | '    ' |  | || |    |  ___/   | |
| |  |`\____) |  | || |  _| |___/ |  | || |    _| |_     | || |   \ `--' /   | || |   _| |_      | |
| |  |_______.'  | || | |_________|  | || |   |_____|    | || |    `.__.'    | || |  |_____|     | |
| |              | || |              | || |              | || |              | || |              | |
| '--------------' || '--------------' || '--------------' || '--------------' || '--------------' |
 '----------------'  '----------------'  '----------------'  '----------------'  '----------------' 
				  _____
                                 |     |
                                 | | | |
                                 |_____|
                           ____ ___|_|___ ____
                          ()___)         ()___)
                          // /|           |\ \\
                         // / |           | \ \\
                        (___) |___________| (___)
                        (___)   (_______)   (___)
                        (___)     (___)     (___)
                        (___)      |_|      (___)
                        (___)  ___/___\___   | |
                         | |  |           |  | |
                         | |  |___________| /___\
                        /___\  |||     ||| //   \\
                       //   \\ |||     ||| \\   //
                       \\   // |||     |||  \\ //
                        \\ // ()__)   (__()
                              ///       \\\
                             ///         \\\
                           _///___     ___\\\_
                          |_______|   |_______|








================================================
FILE: scripts/setup/laptop_setup.sh
================================================
#!/bin/bash

# Function to display devices and ask for confirmation
function confirm_devices {
    devices=$(adb devices)
    
    echo "List of devices:"
    echo "$devices"
    
    read -p "Is your oculus device connected? (y/n): " confirmation
    
    if [ "$confirmation" != "y" ] && [ "$confirmation" != "Y" ]; then
	return 1
    fi
}

# print out nice ascii art
ascii=$(cat ./intro.txt)
echo "$ascii"

echo "Welcome to the DROID setup process."


read -p "Is this your first time setting up the machine? (yes/no): " first_time

# path variables
ROOT_DIR="$(git rev-parse --show-toplevel)"
DOCKER_COMPOSE_DIR="$ROOT_DIR/.docker/laptop"
DOCKER_COMPOSE_FILE="$DOCKER_COMPOSE_DIR/docker-compose-laptop.yaml"


if [ "$first_time" = "yes" ]; then
	# ensure local files are up to date and git lfs is configured
	echo -e "Ensure all submodules are cloned and oculus_reader APK file pulled locally \n"

	eval "$(ssh-agent -s)"
	ssh-add /home/robot/.ssh/id_ed25519
	curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | bash
	apt update && apt install -y git-lfs
	git lfs install # has to be run only once on a single user account
	cd $ROOT_DIR && git submodule update --recursive --remote --init
	
	# install docker
	echo -e "Install docker \n"

	apt-get update
	apt-get install ca-certificates curl gnupg
	install -m 0755 -d /etc/apt/keyrings
	curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg
	chmod a+r /etc/apt/keyrings/docker.gpg
	echo \
	  "deb [arch="$(dpkg --print-architecture)" signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \
	  "$(. /etc/os-release && echo "$VERSION_CODENAME")" stable" | \
	  sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
	apt-get update
	apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin

	# install and configure nvidia container toolkit
	echo -e "Install Nvidia container toolkit \n"

	distribution=$(. /etc/os-release;echo $ID$VERSION_ID) \
	      && curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \
	      && curl -s -L https://nvidia.github.io/libnvidia-container/$distribution/libnvidia-container.list | \
		    sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
		    sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
	apt-get update
	apt-get install -y nvidia-container-toolkit
	nvidia-ctk runtime configure --runtime=docker
	systemctl restart docker

else
	echo -e "\nWelcome back!\n"
fi

read -p "Have you installed the oculus_reader APK file on your Oculus Quest 2? (yes/no): " first_time

if [ "$first_time" = "no" ]; then

	# install APK on Oculus device
	echo -e "Install APK on oculus device \n"

	#usermod -aG plugdev $LOGNAME
	#newgrp plugdev
	apt install -y android-tools-adb android-sdk-platform-tools-common
	adb start-server

	read -p "Connect your Oculus Quest 2 via USB-C, and approve USB debugging within device. Confirm with y when complete? (y/n): " confirmation
	    
	if [ "$confirmation" != "y" ] && [ "$confirmation" != "Y" ]; then
		return 1
	else
		return exit 1
	fi


	# Retry loop
	max_retries=3
	retry_count=0

	while ! confirm_devices; do
	    ((retry_count++))
	    if [ "$retry_count" -ge "$max_retries" ]; then
		echo "Max retry attempts reached. Aborting installation."
		exit 1
	    fi
	    echo "Retrying..."
	done

	echo $ROOT_DIR
	pip3 install -e $ROOT_DIR/droid/oculus_reader
	python3 $ROOT_DIR/droid/oculus_reader/oculus_reader/reader.py
	echo cleaning up threads ...
	sleep 5
	adb kill-server
fi

# expose parameters as environment variables
echo -e "Set environment variables from parameters file \n"

PARAMETERS_FILE="$(git rev-parse --show-toplevel)/droid/misc/parameters.py"
awk -F'[[:space:]]*=[[:space:]]*' '/^[[:space:]]*([[:alnum:]_]+)[[:space:]]*=/ && $1 != "ARUCO_DICT" { gsub("\"", "", $2); print "export " $1 "=" $2 }' "$PARAMETERS_FILE" > temp_env_vars.sh
source temp_env_vars.sh
export ROOT_DIR=$ROOT_DIR
export NUC_IP=$nuc_ip
export LAPTOP_IP=$laptop_ip
export ROBOT_IP=$robot_ip
export SUDO_PASSWORD=$sudo_password
export ROBOT_TYPE=$robot_type
export ROBOT_SERIAL_NUMBER=$robot_serial_number
export HAND_CAMERA_ID=$hand_camera_id
export VARIED_CAMERA_1_ID=$varied_camera_1_id
export VARIED_CAMERA_2_ID=$varied_camera_2_id
export LIBFRANKA_VERSION=$libfranka_version
export ROOT_DIR=$ROOT_DIR
rm temp_env_vars.sh

if [ "$ROBOT_TYPE" == "panda" ]; then
        export LIBFRANKA_VERSION=0.9.0
else
        export LIBFRANKA_VERSION=0.10.0
fi


# ensure GUI window is accessible from container
echo -e "Set Docker Xauth for x11 forwarding \n"

export DOCKER_XAUTH=/tmp/.docker.xauth
rm $DOCKER_XAUTH
touch $DOCKER_XAUTH
xauth nlist $DISPLAY | sed -e 's/^..../ffff/' | xauth -f $DOCKER_XAUTH nmerge -

# build client server container

read -p "Do you want to rebuild the container image? (yes/no): " first_time

if [ "$first_time" = "yes" ]; then
	echo -e "build control server container \n"
	cd $DOCKER_COMPOSE_DIR && docker compose -f $DOCKER_COMPOSE_FILE build
fi

# find ethernet interface on device
echo -e "\n set static ip \n"

echo "Select an Ethernet interface to set a static IP for:"

interfaces=$(ip -o link show | grep -Eo '^[0-9]+: (en|eth|ens|eno|enp)[a-z0-9]*' | awk -F' ' '{print $2}')

# Display available interfaces for the user to choose from
select interface_name in $interfaces; do
    if [ -n "$interface_name" ]; then
        break
    else
        echo "Invalid selection. Please choose a valid interface."
    fi
done

echo "You've selected: $interface_name"

# Add and configure the static IP connection
nmcli connection delete "laptop_static"
nmcli connection add con-name "laptop_static" ifname "$interface_name" type ethernet
nmcli connection modify "laptop_static" ipv4.method manual ipv4.address $LAPTOP_IP/24
nmcli connection up "laptop_static"

echo "Static IP configuration complete for interface $interface_name."

# run docker container
read -p "Please plug in and out each camera connected via usb and press enter once done?: " _

echo "Starting ADB server on host machine"
adb kill-server
adb -a nodaemon server start &> /dev/null &

# Retry loop
max_retries=3
retry_count=0

while ! confirm_devices; do
    ((retry_count++))
    if [ "$retry_count" -ge "$max_retries" ]; then
	echo "Max retry attempts reached. Aborting installation."
	exit 1
    fi
    echo "Retrying..."
done

echo -e "run client application \n"
docker compose -f $DOCKER_COMPOSE_FILE up



================================================
FILE: scripts/setup/nuc_setup.sh
================================================
#!/bin/bash

# print out nice ascii art
ascii=$(cat ./intro.txt)
echo "$ascii"


echo "Welcome to the DROID setup process."

read -p "Is this your first time setting up the machine? (yes/no): " first_time

if [ "$first_time" = "yes" ]; then
        echo "Great! Let's proceed with the setup."

        # ensure submodules are cloned
        echo "Repulling all submodules."
        read -p "Enter the user whose ssh credentials will be used: " USERNAME
        eval "$(ssh-agent -s)"
        ssh-add /home/$USERNAME/.ssh/id_ed25519
        ROOT_DIR="$(git rev-parse --show-toplevel)"
        cd $ROOT_DIR && git submodule update --recursive --remote --init

        # install docker
        echo -e "\nInstall docker \n"

        apt-get update
        apt-get install ca-certificates curl gnupg
        install -m 0755 -d /etc/apt/keyrings
        curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg
        chmod a+r /etc/apt/keyrings/docker.gpg
        echo \
          "deb [arch="$(dpkg --print-architecture)" signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \
          "$(. /etc/os-release && echo "$VERSION_CODENAME")" stable" | \
          sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
        apt-get update
        apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
        systemctl enable docker

        # perform rt-patch
        echo -e "\nPerform realtime patch of kernel \n"

        apt update && apt install ubuntu-advantage-tools
        pro attach $UBUNTU_PRO_TOKEN
        pro enable realtime-kernel

        # cpu frequency scaling
        echo -e "\nSet cpu frequency scaling settings \n"

        apt install cpufrequtils -y
        systemctl disable ondemand
        systemctl enable cpufrequtils
        sh -c 'echo "GOVERNOR=performance" > /etc/default/cpufrequtils'
        systemctl daemon-reload && sudo systemctl restart cpufrequtils

else
    echo -e "\nWelcome back!\n"
fi


# Read the parameter values from the Python script using awk and convert to env variables
echo -e "\nSet environment variables from parameters file\n"

PARAMETERS_FILE="$(git rev-parse --show-toplevel)/droid/misc/parameters.py"
awk -F'[[:space:]]*=[[:space:]]*' '/^[[:space:]]*([[:alnum:]_]+)[[:space:]]*=/ && $1 != "ARUCO_DICT" { gsub("\"", "", $2); print "export " $1 "=" $2 }' "$PARAMETERS_FILE" > temp_env_vars.sh
source temp_env_vars.sh
export ROOT_DIR=$(git rev-parse --show-toplevel)
export NUC_IP=$nuc_ip
export ROBOT_IP=$robot_ip
export LAPTOP_IP=$laptop_ip
export SUDO_PASSWORD=$sudo_password
export ROBOT_TYPE=$robot_type
export ROBOT_SERIAL_NUMBER=$robot_serial_number
export HAND_CAMERA_ID=$hand_camera_id
export VARIED_CAMERA_1_ID=$varied_camera_1_id
export VARIED_CAMERA_2_ID=$varied_camera_2_id
export UBUNTU_PRO_TOKEN=$ubuntu_pro_token
rm temp_env_vars.sh

if [ "$ROBOT_TYPE" == "panda" ]; then
        export LIBFRANKA_VERSION=0.9.0
else
        export LIBFRANKA_VERSION=0.10.0
fi

# build control server container 

read -p "Do you want to rebuild the container image? (yes/no): " first_time

if [ "$first_time" = "yes" ]; then
        echo -e "\n build control server container \n"

        DOCKER_COMPOSE_DIR="$ROOT_DIR/.docker/nuc"
        DOCKER_COMPOSE_FILE="$DOCKER_COMPOSE_DIR/docker-compose-nuc.yaml"
        cd $DOCKER_COMPOSE_DIR && docker-compose -f $DOCKER_COMPOSE_FILE build
fi

# find ethernet interface on device
echo -e "\n set static ip \n"

echo "Select an Ethernet interface to set a static IP for:"

interfaces=$(ip -o link show | grep -Eo '^[0-9]+: (en|eth|ens|eno|enp)[a-z0-9]*' | awk -F' ' '{print $2}')

# Display available interfaces for the user to choose from
select interface_name in $interfaces; do
    if [ -n "$interface_name" ]; then
        break
    else
        echo "Invalid selection. Please choose a valid interface."
    fi
done

echo "You've selected: $interface_name"

# Add and configure the static IP connection
nmcli connection delete "nuc_static"
nmcli connection add con-name "nuc_static" ifname "$interface_name" type ethernet
nmcli connection modify "nuc_static" ipv4.method manual ipv4.address $NUC_IP/24
nmcli connection up "nuc_static"

echo "Static IP configuration complete for interface $interface_name."

# run control server container
echo -e "8. run control server \n"

DOCKER_COMPOSE_FILE="$(git rev-parse --show-toplevel)/.docker/nuc/docker-compose-nuc.yaml"
docker compose -f $DOCKER_COMPOSE_FILE up -d



================================================
FILE: scripts/tests/calibrate_cameras.py
================================================
from droid.controllers.oculus_controller import VRPolicy
from droid.robot_env import RobotEnv
from droid.trajectory_utils.misc import calibrate_camera

# Make the robot env
env = RobotEnv()
controller = VRPolicy()
camera_id = "19824535"

print("Ready")
calibrate_camera(env, camera_id, controller)



================================================
FILE: scripts/tests/collect_trajectory.py
================================================
from droid.controllers.oculus_controller import VRPolicy
from droid.robot_env import RobotEnv
from droid.trajectory_utils.misc import collect_trajectory

# Make the robot env
env = RobotEnv()
controller = VRPolicy()

print("Ready")
collect_trajectory(env, controller=controller)



================================================
FILE: scripts/tests/memory_leak.py
================================================
import glob
import random

import matplotlib.pyplot as plt
import psutil
import pyzed.sl as sl
from tqdm import tqdm

from droid.camera_utils.wrappers.recorded_multi_camera_wrapper import RecordedMultiCameraWrapper
from droid.data_loading.trajectory_sampler import *


def example_script():
    path = "/home/sasha/DROID/data/success/2023-02-15/Wed_Feb_15_19:08:33_2023/recordings/23404442.svo"

    init = sl.InitParameters()
    init.set_from_svo_file(path)

    # init = sl.InitParameters(svo_input_filename=path, svo_real_time_mode=False)
    cam = sl.Camera()
    status = cam.open(init)
    if status != sl.ERROR_CODE.SUCCESS:
        print(repr(status))
        return

    runtime = sl.RuntimeParameters()
    mat = sl.Mat()
    while True:  # for 'q' key
        err = cam.grab(runtime)
        if err == sl.ERROR_CODE.SUCCESS:
            cam.retrieve_image(mat)
        else:
            break
    cam.close()


data_filtering_kwargs = dict(
    train_p=0.5,
    remove_failures=True,
    filter_func=None,
)

traj_loading_kwargs = dict(
    remove_skipped_steps=True,
    num_samples_per_traj=None,
    num_samples_per_traj_coeff=1.5,
)

timestep_filtering_kwargs = dict(
    # Eventually need to enable sweeper to take lists #
    action_space="cartesian_velocity",
    robot_state_keys=[],
    camera_extrinsics=[],
)

image_transform_kwargs = dict(
    remove_alpha=True,
    bgr_to_rgb=True,
    to_tensor=True,
    augment=True,
)

camera_kwargs = dict(
    hand_camera=dict(image=True, depth=False, pointcloud=False, concatenate_images=False, resolution=(128, 128)),
    fixed_camera=dict(image=True, depth=False, pointcloud=False, concatenate_images=False, resolution=(128, 128)),
    varied_camera=dict(image=True, depth=False, pointcloud=False, concatenate_images=False, resolution=(128, 128)),
)

train_folderpaths, test_folderpaths = generate_train_test_split(**data_filtering_kwargs)

traj_sampler = TrajectorySampler(
    train_folderpaths,
    traj_loading_kwargs=traj_loading_kwargs,
    timestep_filtering_kwargs=timestep_filtering_kwargs,
    image_transform_kwargs=image_transform_kwargs,
    camera_kwargs=camera_kwargs,
)


def traj_sampling_script():
    traj_sampler.fetch_samples()


def load_random_traj_script():
    folderpath = random.choice(train_folderpaths)
    filepath = os.path.join(folderpath, "trajectory.h5")
    recording_folderpath = os.path.join(folderpath, "recordings/MP4")

    load_trajectory(
        filepath,
        recording_folderpath=recording_folderpath,
        read_cameras=True,
        camera_kwargs=camera_kwargs,
        **traj_loading_kwargs,
    )


def camera_wrapper_script():
    folderpath = random.choice(train_folderpaths)
    recording_folderpath = os.path.join(folderpath, "recordings")

    camera_reader = RecordedMultiCameraWrapper(recording_folderpath, **camera_kwargs)

    while True:
        camera_obs = camera_reader.read_cameras()
        camera_failed = camera_obs is None
        if camera_failed:
            break

    camera_reader.disable_cameras()


def single_reader_script():
    folderpath = random.choice(train_folderpaths)
    recording_folderpath = os.path.join(folderpath, "recordings")
    all_filepaths = glob.glob(recording_folderpath + "/*.svo")

    path = random.choice(all_filepaths)

    camera = RecordedZedCamera(path, serial_number="")
    camera.set_reading_parameters(resolution=(256, 256))

    frame_count = camera.get_frame_count()
    for _i in range(frame_count):
        camera.read_camera()

    camera.disable_camera()


# from droid.training.data_loading.data_loader import create_train_test_data_loader
# data_processing_kwargs=dict(
#     timestep_filtering_kwargs=dict(
#         # Eventually need to enable sweeper to take lists #
#         action_space='cartesian_velocity',
#         robot_state_keys=[],
#         camera_extrinsics=[],
#         image_views=['hand_camera'],
#         depth_views=[],
#         pointcloud_views=[],
#     ),

#     image_transform_kwargs=dict(
#         remove_alpha=True,
#         bgr_to_rgb=True,
#         to_tensor=True,
#         augment=True,
#     ),
# )

# camera_kwargs=dict(
#     image=True,
#     depth=False,
#     pointcloud=False,

#     resolution_kwargs=dict(
#         hand_camera=(256, 256),
#         fixed_camera=(256, 256),
#         varied_camera=(256, 256),
#     ),
# )

# data_loader_kwargs=dict(
#     batch_size=2,
#     prefetch_factor=1,
#     buffer_size=200,
#     num_workers=4,

#     data_filtering_kwargs=dict(
#         train_p=0.5,
#         remove_failures=True,
#         filter_func=None,
#     ),

#     traj_loading_kwargs=dict(
#         remove_skipped_steps=True,
#         num_samples_per_traj=20,
#         num_samples_per_traj_coeff=1.5,
#     ),
# )

# train_dataloader, test_dataloader = create_train_test_data_loader(
#     data_loader_kwargs=data_loader_kwargs, data_processing_kwargs=data_processing_kwargs,
#     camera_kwargs=camera_kwargs)
# iterator = iter(train_dataloader)

# def training_script():
#     next(iterator)


if __name__ == "__main__":
    memory_usage = []
    for _ in tqdm(range(100)):
        curr_mem_usage = psutil.virtual_memory()[3]
        memory_usage.append(curr_mem_usage)

        # single_reader_script()
        load_random_traj_script()

    try:
        plt.plot(memory_usage)
        plt.ylabel("RAM Used")
        plt.xlabel("SVO Files Read")
        plt.show()
    except:
        pass



================================================
FILE: scripts/tests/replay_trajectory.py
================================================
from droid.robot_env import RobotEnv
from droid.trajectory_utils.misc import replay_trajectory

trajectory_folderpath = "/home/sasha/DROID/data/success/2023-02-16/Thu_Feb_16_16:27:00_2023"
action_space = "joint_position"

# Make the robot env
env = RobotEnv(action_space=action_space)

# Replay Trajectory #
h5_filepath = trajectory_folderpath + "/trajectory.h5"
replay_trajectory(env, filepath=h5_filepath)



================================================
FILE: scripts/training/sweepable_train_policy [wip].py
================================================
from droid.training.model_trainer import exp_launcher

# import rlkit.util.hyperparameter as hyp

"""
THIS IS A WORK IN PROGRESS
"""


variant = dict(
    exp_name="simple_bc",
    use_gpu=True,
    seed=0,
    training_kwargs=dict(
        grad_steps_per_epoch=10,
        num_epochs=100,
        weight_decay=0.0,
        lr=1e-3,
    ),
    camera_kwargs=dict(
        image=True,
        depth=False,
        pointcloud=False,
        resolution_kwargs=dict(
            hand_camera=(256, 256),
            fixed_camera=(256, 256),
            varied_camera=(256, 256),
        ),
    ),
    data_processing_kwars=dict(
        timestep_filtering_kwargs=dict(
            # Eventually need to enable sweeper to take lists #
            action_space="cartesian_delta",
            robot_state_keys=["cartesian_pose", "gripper_position", "joint_positions", "joint_velocities"],
            camera_extrinsics=["hand_camera", "varied_camera", "fixed_camera"],
            image_views=["hand_camera", "varied_camera", "fixed_camera"],
            depth_views=[],
            pointcloud_views=[],
        ),
        image_transform_kwargs=dict(remove_alpha=True, to_tensor=True, augment=True),
    ),
    data_loader_kwargs=dict(
        buffer_size=500,
        batch_size=32,
        num_workers=1,
        train_p=0.9,
        remove_failures=True,
        filter_func=None,
        traj_loading_kwargs=dict(
            remove_skipped_steps=True,
            num_samples_per_traj=100,
            num_samples_per_traj_coeff=2,
        ),
    ),
    model_kwargs=dict(
        embedding_dim=1,
        num_encoder_hiddens=128,
        num_residual_layers=3,
        num_residual_hiddens=64,
        representation_size=100,
        num_img_layers=4,
        num_img_hidden=400,
        num_state_layers=4,
        num_state_hidden=400,
        num_policy_layers=4,
        num_policy_hidden=400,
    ),
)

if __name__ == "__main__":
    exp_launcher(variant, run_id=1, exp_id=0)

    # search_space = {
    #     "seed": range(1),
    # }
    # sweeper = hyp.DeterministicHyperparameterSweeper(
    #     search_space, default_parameters=variant,
    # )

    # exp_id = 0
    # for variant in sweeper.iterate_hyperparameters():
    #     exp_launcher(variant, run_id=1, exp_id=exp_id)
    #     exp_id += 1



================================================
FILE: scripts/training/train_policy.py
================================================
import json

from droid.training.model_trainer import exp_launcher

task_label_filepath = "/home/sasha/DROID/scripts/labeling/task_label_filepath.json"
with open(task_label_filepath, "r") as jsonFile:
    task_labels = json.load(jsonFile)


def filter_func(h5_metadata, put_in_only=False):
    put_in = task_labels.get(h5_metadata["time"], not put_in_only)
    return put_in == put_in_only


variant = dict(
    exp_name="pen_cup_task",
    use_gpu=True,
    seed=0,
    training_kwargs=dict(
        num_epochs=100,
        grad_steps_per_epoch=1000,
        weight_decay=1e-4,
        lr=1e-4,
    ),
    camera_kwargs=dict(
        hand_camera=dict(image=True, concatenate_images=False, resolution=(128, 128), resize_func="cv2"),
        varied_camera=dict(image=False, concatenate_images=False, resolution=(128, 128), resize_func="cv2"),
    ),
    data_processing_kwargs=dict(
        timestep_filtering_kwargs=dict(
            action_space="cartesian_velocity",
            robot_state_keys=["cartesian_position", "gripper_position", "joint_positions"],
            camera_extrinsics=[],
        ),
        image_transform_kwargs=dict(
            remove_alpha=True,
            bgr_to_rgb=True,
            to_tensor=True,
            augment=False,
        ),
    ),
    data_loader_kwargs=dict(
        recording_prefix="MP4",
        batch_size=4,
        prefetch_factor=1,
        buffer_size=1000,
        num_workers=4,
        data_filtering_kwargs=dict(
            train_p=0.9,
            remove_failures=True,
            filter_func=filter_func,
        ),
        traj_loading_kwargs=dict(
            remove_skipped_steps=True,
            num_samples_per_traj=50,
            num_samples_per_traj_coeff=1.5,
            read_cameras=True,
        ),
    ),
    model_kwargs=dict(
        representation_size=50,
        embedding_dim=1,
        num_encoder_hiddens=128,
        num_residual_layers=3,
        num_residual_hiddens=64,
        num_camera_layers=1,
        num_camera_hidden=200,
        num_state_layers=1,
        num_state_hidden=200,
        num_policy_layers=3,
        num_policy_hidden=300,
    ),
)

if __name__ == "__main__":
    exp_launcher(variant, run_id=3, exp_id=0)



================================================
FILE: scripts/training/sanity_check/image_obs.py
================================================
from droid.training.model_trainer import exp_launcher

variant = dict(
    exp_name="sanity_check_image_obs",
    use_gpu=True,
    seed=0,
    training_kwargs=dict(
        num_epochs=100,
        grad_steps_per_epoch=1000,
        weight_decay=0.0,
        lr=1e-4,
    ),
    camera_kwargs=dict(
        hand_camera=dict(image=True, depth=False, pointcloud=False, concatenate_images=False, resolution=(128, 128)),
        fixed_camera=dict(image=True, depth=False, pointcloud=False, concatenate_images=False, resolution=(128, 128)),
        varied_camera=dict(image=True, depth=False, pointcloud=False, concatenate_images=False, resolution=(128, 128)),
    ),
    data_processing_kwargs=dict(
        timestep_filtering_kwargs=dict(
            action_space="cartesian_velocity",
            robot_state_keys=[],
            camera_extrinsics=[],
        ),
        image_transform_kwargs=dict(
            remove_alpha=True,
            bgr_to_rgb=True,
            to_tensor=True,
            augment=True,
        ),
    ),
    data_loader_kwargs=dict(
        recording_prefix="SVO",
        batch_size=4,
        prefetch_factor=1,
        buffer_size=100,
        num_workers=1,
        data_filtering_kwargs=dict(
            train_p=0.9,
            remove_failures=True,
            filter_func=None,
        ),
        traj_loading_kwargs=dict(
            remove_skipped_steps=True,
            num_samples_per_traj=20,
            num_samples_per_traj_coeff=1.5,
            read_cameras=True,
        ),
    ),
    model_kwargs=dict(
        representation_size=50,
        embedding_dim=1,
        num_encoder_hiddens=128,
        num_residual_layers=3,
        num_residual_hiddens=64,
        num_camera_layers=1,
        num_camera_hidden=200,
        num_state_layers=1,
        num_state_hidden=200,
        num_policy_layers=2,
        num_policy_hidden=200,
    ),
)

if __name__ == "__main__":
    exp_launcher(variant, run_id=1, exp_id=0)



================================================
FILE: scripts/training/sanity_check/state_obs.py
================================================
from droid.training.model_trainer import exp_launcher

variant = dict(
    exp_name="sanity_check_state_obs",
    use_gpu=True,
    seed=0,
    training_kwargs=dict(
        num_epochs=100,
        grad_steps_per_epoch=1000,
        weight_decay=0.0,
        lr=1e-4,
    ),
    camera_kwargs=dict(
        hand_camera=dict(image=False, depth=False, pointcloud=False, concatenate_images=False, resolution=(256, 256)),
        fixed_camera=dict(image=False, depth=False, pointcloud=False, concatenate_images=False, resolution=(256, 256)),
        varied_camera=dict(image=False, depth=False, pointcloud=False, concatenate_images=False, resolution=(256, 256)),
    ),
    data_processing_kwargs=dict(
        timestep_filtering_kwargs=dict(
            action_space="cartesian_velocity",
            robot_state_keys=["cartesian_position", "gripper_position"],
            camera_extrinsics=[],
        ),
        image_transform_kwargs=dict(
            remove_alpha=True,
            bgr_to_rgb=True,
            to_tensor=True,
            augment=True,
        ),
    ),
    data_loader_kwargs=dict(
        batch_size=4,
        prefetch_factor=1,
        buffer_size=200,
        num_workers=2,
        data_filtering_kwargs=dict(
            train_p=0.9,
            remove_failures=True,
            filter_func=None,
        ),
        traj_loading_kwargs=dict(
            remove_skipped_steps=True,
            num_samples_per_traj=20,
            num_samples_per_traj_coeff=1.5,
            read_cameras=False,
        ),
    ),
    model_kwargs=dict(
        representation_size=50,
        embedding_dim=1,
        num_encoder_hiddens=128,
        num_residual_layers=3,
        num_residual_hiddens=64,
        num_camera_layers=1,
        num_camera_hidden=200,
        num_state_layers=1,
        num_state_hidden=200,
        num_policy_layers=2,
        num_policy_hidden=200,
    ),
)

if __name__ == "__main__":
    exp_launcher(variant, run_id=1, exp_id=0)



================================================
FILE: scripts/visualizations/create_plots.py
================================================
import os

import matplotlib.pyplot as plt

from droid.plotting.analysis_func import *
from droid.plotting.misc import *
from droid.plotting.text import *

# Gather Graphical Data #
data_path = None  # "/Volumes/DROID_Drive/MyDrive/DROID: Weekly Lab Data/IPRL"
dir_path = os.path.dirname(os.path.realpath(__file__))
data_directory = os.path.join(dir_path, "../../", "data") if data_path is None else data_path
PLOT_FOLDERPATH = os.path.join(dir_path, "../../", "plots")
if not os.path.exists(PLOT_FOLDERPATH):
    os.makedirs(PLOT_FOLDERPATH)

# Run Data Crawler #
data_crawler(data_directory, func_list=[analysis_func])

# Prepare Cumulative Values #
cumulative_dicts = [traj_progress_dict, scene_progress_dict]
for curr_dict in cumulative_dicts:
    all_dict_keys = list(curr_dict.keys())
    for key in all_dict_keys:
        curr_dict[key] = np.cumsum(curr_dict[key])
        curr_dict["DROID"] += curr_dict[key]

# Make Figure #
fig = plt.figure()

# Plot Trajectory Progress #
plt.xticks(all_month_timestamps, labels=all_month_names)
for key in traj_progress_dict:
    lab_progress = traj_progress_dict[key]
    plt.plot(DAY_TIMESTAMPS, lab_progress, label=key)

plt.legend(loc="center left", bbox_to_anchor=(1, 0.5))
plt.title("Trajectories Collected", fontweight="bold")
plt.savefig(PLOT_FOLDERPATH + "/trajectory_progress.png", bbox_inches="tight")

# Plot Scene Progress #
plt.clf()
plt.xticks(all_month_timestamps, labels=all_month_names)
for key in scene_progress_dict:
    lab_progress = scene_progress_dict[key]
    plt.plot(DAY_TIMESTAMPS, lab_progress, label=key)

plt.legend(loc="center left", bbox_to_anchor=(1, 0.5))
plt.title("Scenes Collected", fontweight="bold")
plt.savefig(PLOT_FOLDERPATH + "/scene_progress.png", bbox_inches="tight")

# Plot Week Summary #
plt.clf()
weekly_threshold = 150000
lab_names = list(weekly_progress_dict.keys())
lab_quantities = [weekly_progress_dict[key] for key in lab_names]
plt.bar(lab_names, lab_quantities)
plt.axhline(weekly_threshold, color="black", ls="dotted")
plt.title("Week Summary: Samples Collected", fontweight="bold")
plt.savefig(PLOT_FOLDERPATH + "/week_progress.png", bbox_inches="tight")

# Plot User Progress #
plt.clf()
user_threshold = 500000
user_names = list(user_progress_dict.keys())
user_quantities = [user_progress_dict[key] for key in user_names]
tick_names = ["{0}K".format(100 * i) if i < 10 else "1M" for i in range(6)]
tick_values = [1e5 * i for i in range(len(tick_names))]
plt.xticks(tick_values, labels=tick_names)
plt.barh(user_names, user_quantities)
plt.axvline(user_threshold, color="blue", ls="dotted", label="Standard Requirement")
plt.axvline(user_threshold // 2, color="green", ls="dotted", label="Lab Lead Requirement")
plt.title("Per Person Progress", fontweight="bold")
plt.xlabel("Data Points Collected")
plt.legend(loc="center left", bbox_to_anchor=(1, 0.5))
plt.savefig(PLOT_FOLDERPATH + "/personal_progress.png", bbox_inches="tight")

# Plot Task Distribution #
plt.clf()
task_names = list(task_distribution_dict.keys())
task_quantities = [task_distribution_dict[key] for key in task_names]
sections, texts = plt.pie(task_quantities)
plt.legend(sections, task_names, loc="center left", bbox_to_anchor=(1, 0.5))
plt.title("Task Distribution", fontweight="bold")
plt.savefig(PLOT_FOLDERPATH + "/task_distribution.png", bbox_inches="tight")

# Plot Trajectory Length Distribution #
plt.clf()
plt.hist(all_traj_lengths, bins=100)
plt.xlim([0, 1000])
plt.title("Horizon Distribution", fontweight="bold")
plt.savefig(PLOT_FOLDERPATH + "/horizon_distribution.png", bbox_inches="tight")

# Plot Camera Pose Distribution #
plt.clf()
ax_pos = fig.add_subplot(1, 2, 1, projection="3d")
ax_ang = fig.add_subplot(1, 2, 2, projection="3d")
pos_values, pos_density, ang_values, ang_density = estimate_pos_angle_density(all_camera_poses)
ax_pos.scatter(pos_values[0], pos_values[1], pos_values[2], c=pos_density, cmap="viridis", linewidth=0.5)
ax_ang.scatter(ang_values[0], ang_values[1], ang_values[2], c=ang_density, cmap="viridis", linewidth=0.5)
ax_pos.set_title("Camera Positions", fontweight="bold")
ax_ang.set_title("Camera Angles", fontweight="bold")
ax_pos.view_init(elev=20, azim=-23)
ax_ang.view_init(elev=14, azim=-143)
plt.savefig(PLOT_FOLDERPATH + "/camera_poses.png", bbox_inches="tight")
plt.show()



================================================
FILE: scripts/visualizations/visualize_data.py
================================================
from torch.utils.data.datapipes.iter import Shuffler

from droid.data_loading.dataset import TrajectoryDataset
from droid.data_loading.trajectory_sampler import *
from droid.trajectory_utils.misc import visualize_timestep

all_folderpaths = collect_data_folderpaths()
traj_sampler = TrajectorySampler(all_folderpaths, recording_prefix="SVO")
traj_dataset = TrajectoryDataset(traj_sampler)
shuffled_dataset = Shuffler(traj_dataset, buffer_size=100)
data_iterator = iter(shuffled_dataset)

while True:
    timestep = next(data_iterator)
    visualize_timestep(timestep, pause_time=500)



================================================
FILE: scripts/visualizations/visualize_day.py
================================================
import random

from droid.data_loading.trajectory_sampler import crawler
from droid.trajectory_utils.misc import visualize_trajectory

logdir = "/home/sasha/DROID/data/success/2023-04-20"

all_folderpaths = crawler(logdir)
random.shuffle(all_folderpaths)

camera_kwargs = dict(
    hand_camera=dict(image=True, resolution=(0, 0)),
    varied_camera=dict(image=True, resolution=(0, 0)),
)

for folderpath in all_folderpaths:
    h5_filepath = folderpath + "/trajectory.h5"
    recording_folderpath = folderpath + "/recordings/SVO"
    try:
        visualize_trajectory(
            filepath=h5_filepath, recording_folderpath=recording_folderpath, camera_kwargs=camera_kwargs
        )
    except:
        pass



================================================
FILE: scripts/visualizations/visualize_trajectory.py
================================================
from droid.trajectory_utils.misc import visualize_trajectory

trajectory_folderpath = "/home/sasha/DROID/data/success/2023-03-10/Fri_Mar_10_13:05:35_2023"

camera_kwargs = dict(
    hand_camera=dict(image=True, resolution=(0, 0)),
    varied_camera=dict(image=True, resolution=(0, 0)),
)

h5_filepath = trajectory_folderpath + "/trajectory.h5"
recording_folderpath = trajectory_folderpath + "/recordings/SVO"
visualize_trajectory(filepath=h5_filepath, recording_folderpath=recording_folderpath, camera_kwargs=camera_kwargs)



================================================
FILE: .docker/README.md
================================================
# Overview

In order to simplify the setup and deployment of DROID across different machines we supply Dockerfiles for both the control server (nuc) and the client machine (laptop). The directory structure is broken down as follows: 

    ‚îú‚îÄ‚îÄ nuc                                      # directory for nuc docker setup files
    ‚îú‚îÄ‚îÄ‚îÄ‚îÄ Dockerfile.nuc                         # nuc image definition
    ‚îú‚îÄ‚îÄ‚îÄ‚îÄ docker-compose-nuc.yaml                # nuc container deployment settings
    ‚îú‚îÄ‚îÄ laptop                                   # directory for laptop docker setup files
    ‚îú‚îÄ‚îÄ‚îÄ‚îÄ Dockerfile.laptop                      # laptop image definition
    ‚îú‚îÄ‚îÄ‚îÄ‚îÄ docker-compose-laptop.yaml             # laptop container deployment settings
    ‚îú‚îÄ‚îÄ‚îÄ‚îÄ entrypoint.sh                          # script that is run on entrypoint of Docker container

We recognise that some users may not already be familiar with Docker, the syntax of Dockerfiles and the syntax of docker compose configuration files. We point the user towards the following resources on these topics:

* [Docker Overview](https://docs.docker.com/get-started/overview/)
* [Dockerfile Reference](https://docs.docker.com/engine/reference/builder/)
* [Docker Compose Overview](https://docs.docker.com/compose/)

# NUC Setup
In order to set up the control server on your NUC run `sudo ./nuc_setup.sh` from this [directory](https://github.com/droid-dataset/droid/tree/main/scripts/setup). Running through all the steps in this script will install host system dependencies and ensure the control server runs automatically in a docker container each time your machine is booted.

Further details on the steps in README.md in the `scripts/setup` directory.

# Laptop Setup  
In order to set up the user client on your laptop run `sudo ./laptop_setup.sh` from this [directory](https://github.com/droid-dataset/droid/tree/main/scripts/setup). Running through all the steps in this script will install host system dependencies and ensure the user client can be run in a docker container.

Further details can be found [README.md](https://github.com/droid-dataset/droid/tree/main/scripts/setup/README.md).




================================================
FILE: .docker/laptop/docker-compose-laptop-data-upload.yaml
================================================
version: "3"

services: 
  laptop_setup:
    image: ghcr.io/droid-dataset/droid_laptop:${ROBOT_TYPE}
    environment:
      ROOT_DIR: ${ROOT_DIR}
      DISPLAY: ${DISPLAY}
      XAUTHORITY: ${DOCKER_XAUTH}
      ROBOT_TYPE: ${ROBOT_TYPE}
      LIBFRANKA_VERSION: ${LIBFRANKA_VERSION}
      NUC_IP: ${NUC_IP}
      ROBOT_IP: ${ROBOT_IP}
      LAPTOP_IP: ${LAPTOP_IP}
      NVIDIA_VISIBLE_DEVICES: all
      ANDROID_ADB_SERVER_ADDRESS: host.docker.internal
    volumes:
      - /tmp/.X11-unix:/tmp/.X11-unix:rw
      - ${DOCKER_XAUTH}:${DOCKER_XAUTH}
      - ${ROOT_DIR}/droid/misc/parameters.py:/app/droid/misc/parameters.py
      - ${ROOT_DIR}/droid/calibration/calibration_info.json:/app/droid/calibration/calibration_info.json
      - ${ROOT_DIR}/data:/app/data
      - ${ROOT_DIR}/cache:/app/cache
      - ${ROOT_DIR}/droid-credentials.json:/app/droid-credentials.json
      - ${ROOT_DIR}/scripts/postprocess.py:/app/scripts/postprocess.py
    build: 
      context: ../../
      dockerfile: .docker/laptop/Dockerfile.laptop
    devices:
      - "/dev:/dev"
    runtime: nvidia
    privileged: true
    network_mode: "host"
    command: python /app/scripts/postprocess.py



================================================
FILE: .docker/laptop/docker-compose-laptop.yaml
================================================
version: "3"

services: 
  laptop_setup:
    image: ghcr.io/droid-dataset/droid_laptop:${ROBOT_TYPE}
    environment:
      ROOT_DIR: ${ROOT_DIR}
      DISPLAY: ${DISPLAY}
      XAUTHORITY: ${DOCKER_XAUTH}
      ROBOT_TYPE: ${ROBOT_TYPE}
      LIBFRANKA_VERSION: ${LIBFRANKA_VERSION}
      NUC_IP: ${NUC_IP}
      ROBOT_IP: ${ROBOT_IP}
      LAPTOP_IP: ${LAPTOP_IP}
      NVIDIA_VISIBLE_DEVICES: all
      ANDROID_ADB_SERVER_ADDRESS: host.docker.internal
    volumes:
      - /tmp/.X11-unix:/tmp/.X11-unix:rw
      - ${DOCKER_XAUTH}:${DOCKER_XAUTH}
      - ${ROOT_DIR}/droid/misc/parameters.py:/app/droid/misc/parameters.py
      - ${ROOT_DIR}/droid/calibration/calibration_info.json:/app/droid/calibration/calibration_info.json
      - ${ROOT_DIR}/data:/app/data
      - ${ROOT_DIR}/cache:/app/cache
    build: 
      context: ../../
      dockerfile: .docker/laptop/Dockerfile.laptop
      args:
        ROOT_DIR: ${ROOT_DIR}
        ROBOT_TYPE: ${ROBOT_TYPE}
        NUC_IP: ${NUC_IP}
        ROBOT_IP: ${ROBOT_IP}
    devices:
      - "/dev:/dev"
    runtime: nvidia
    privileged: true
    network_mode: "host"
    command: python /app/scripts/main.py



================================================
FILE: .docker/laptop/Dockerfile.laptop
================================================
FROM nvidia/cuda:12.1.0-devel-ubuntu22.04

# build args
ARG ROBOT_TYPE=${ROBOT_TYPE}
ARG LIBFRANKA_VERSION=${LIBFRANKA_VERSION}
ARG ROBOT_IP=${ROBOT_IP}
ARG NUC_IP=${NUC_IP}
ARG NUC_ROBOT_CONFIG_DIR=/app/config/${ROBOT_TYPE}
ARG NUC_OCULUS_DIR=/app/droid/oculus_reader
ARG NUC_POLYMETIS_DIR=/app/droid/fairo/polymetis
ARG NUC_POLYMETIS_CONFIG_DIR=${NUC_POLYMETIS_DIR}/polymetis/conf

# runtime env vars
ENV ROBOT_TYPE=${ROBOT_TYPE}
ENV LIBFRANKA_VERSION=${LIBFRANKA_VERSION}
ENV ROBOT_IP=${ROBOT_IP}
ENV NUC_IP=${NUC_IP}
ENV NVIDIA_VISIBLE_DEVICES=all
ENV NVIDIA_DRIVER_CAPABILITIES \
    ${NVIDIA_DRIVER_CAPABILITIES:+$NVIDIA_DRIVER_CAPABILITIES,}compute,video,utility

# copy project code to container
COPY . /app
WORKDIR /app

# base system installations
RUN apt-get update && \
    apt-get install -y software-properties-common build-essential sudo git curl wget python3-pip libspdlog-dev \
    libeigen3-dev lsb-release ffmpeg libsm6 libxext6 zstd && \
    apt-get upgrade -y

# install miniconda 
RUN wget \
    https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh \
    && mkdir /root/.conda \
    && bash Miniconda3-latest-Linux-x86_64.sh -b \
    && rm -f Miniconda3-latest-Linux-x86_64.sh
ENV PATH /root/miniconda3/bin:$PATH

# create conda environment
RUN conda create -n "robot" python=3.7
SHELL ["conda", "run", "-n", "robot", "/bin/bash", "-c"]

# install the zed sdk
ARG UBUNTU_RELEASE_YEAR=22
ARG ZED_SDK_MAJOR=4
ARG ZED_SDK_MINOR=0
ARG CUDA_MAJOR=12
ARG CUDA_MINOR=1

RUN echo "Europe/Paris" > /etc/localtime ; echo "CUDA Version ${CUDA_MAJOR}.${CUDA_MINOR}.0" > /usr/local/cuda/version.txt

# setup the ZED SDK
RUN apt-get update -y || true ; apt-get install --no-install-recommends lsb-release wget less udev sudo zstd build-essential cmake python3 python3-pip libpng-dev libgomp1 -y && \ 
    python3 -m pip install numpy opencv-python && \
    wget -q -O ZED_SDK_Linux_Ubuntu${UBUNTU_RELEASE_YEAR}.run https://download.stereolabs.com/zedsdk/${ZED_SDK_MAJOR}.${ZED_SDK_MINOR}/cu${CUDA_MAJOR}${CUDA_MINOR%.*}/ubuntu${UBUNTU_RELEASE_YEAR} && \
    chmod +x ZED_SDK_Linux_Ubuntu${UBUNTU_RELEASE_YEAR}.run && \
    ./ZED_SDK_Linux_Ubuntu${UBUNTU_RELEASE_YEAR}.run -- silent skip_tools skip_cuda && \
    ln -sf /lib/x86_64-linux-gnu/libusb-1.0.so.0 /usr/lib/x86_64-linux-gnu/libusb-1.0.so && \
    rm ZED_SDK_Linux_Ubuntu${UBUNTU_RELEASE_YEAR}.run && \
    rm -rf /var/lib/apt/lists/*
RUN conda install -c conda-forge libstdcxx-ng requests # required for pyzed
RUN python /usr/local/zed/get_python_api.py && python -m pip install --ignore-installed /app/pyzed-4.0-cp37-cp37m-linux_x86_64.whl

# install oculus reader
RUN add-apt-repository universe && \
    apt-get update -y && \ 
    apt-get install -y android-tools-fastboot
RUN pip3 install -e $NUC_OCULUS_DIR && \
    apt install -y android-tools-adb

# python environment setup
RUN pip3 install -e . && \
    pip3 install dm-robotics-moma==0.5.0 --no-deps && \
    pip3 install dm-robotics-transformations==0.5.0 --no-deps && \
    pip3 install dm-robotics-agentflow==0.5.0 --no-deps && \
    pip3 install dm-robotics-geometry==0.5.0 --no-deps && \
    pip3 install dm-robotics-manipulation==0.5.0 --no-deps && \
    pip3 install dm-robotics-controllers==0.5.0 --no-deps


# using miniconda instead of anaconda so overwrite sh scripts
RUN find /app/droid/franka -type f -name "launch_*.sh" -exec sed -i 's/anaconda/miniconda/g' {} \;
RUN find /app/scripts/server -type f -name "launch_server.sh" -exec sed -i 's/anaconda/miniconda/g' {} \;

# set absolute paths
RUN find /app/droid/franka -type f -name "launch_*.sh" -exec sed -i 's|~|/root|g' {} \;
RUN find /app/scripts/server -type f -name "launch_server.sh" -exec sed -i 's|~|/root|g' {} \;

# set polymetis config files
RUN cp ${NUC_ROBOT_CONFIG_DIR}/franka_hardware.yaml ${NUC_POLYMETIS_CONFIG_DIR}/robot_client/franka_hardware.yaml && \
    cp ${NUC_ROBOT_CONFIG_DIR}/franka_panda.yaml ${NUC_POLYMETIS_CONFIG_DIR}/robot_model/franka_panda.yaml

# start the server
RUN chmod +x /app/.docker/laptop/entrypoint.sh
ENTRYPOINT ["/app/.docker/laptop/entrypoint.sh"]



================================================
FILE: .docker/laptop/Dockerfile.laptop_fr3
================================================
FROM nvidia/cuda:12.1.0-devel-ubuntu22.04

# set robot parameters
ENV ROBOT_TYPE=fr3
ENV LIBFRANKA_VERSION=0.10.0
ENV ROBOT_IP=172.16.0.1 
ENV NUC_IP=172.16.0.2
ENV NVIDIA_VISIBLE_DEVICES=all
ENV NVIDIA_DRIVER_CAPABILITIES \
    ${NVIDIA_DRIVER_CAPABILITIES:+$NVIDIA_DRIVER_CAPABILITIES,}compute,video,utility

# set directory structure
ARG NUC_ROBOT_CONFIG_DIR=/app/config/${ROBOT_TYPE}
ARG NUC_OCULUS_DIR=/app/droid/oculus_reader
ARG NUC_POLYMETIS_DIR=/app/droid/fairo/polymetis
ARG NUC_POLYMETIS_CONFIG_DIR=${NUC_POLYMETIS_DIR}/polymetis/conf

# copy project code to container
COPY . /app
WORKDIR /app

# base system installations
RUN apt-get update && \
    apt-get install -y software-properties-common build-essential sudo git curl wget python3-pip libspdlog-dev \
    libeigen3-dev lsb-release ffmpeg libsm6 libxext6 zstd && \
    apt-get upgrade -y

# install miniconda 
RUN wget \
    https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh \
    && mkdir /root/.conda \
    && bash Miniconda3-latest-Linux-x86_64.sh -b \
    && rm -f Miniconda3-latest-Linux-x86_64.sh
ENV PATH /root/miniconda3/bin:$PATH

# create conda environment
RUN conda create -n "robot" python=3.7
SHELL ["conda", "run", "-n", "robot", "/bin/bash", "-c"]

# install the zed sdk
ARG UBUNTU_RELEASE_YEAR=22
ARG ZED_SDK_MAJOR=4
ARG ZED_SDK_MINOR=0
ARG CUDA_MAJOR=12
ARG CUDA_MINOR=1

RUN echo "Europe/Paris" > /etc/localtime ; echo "CUDA Version ${CUDA_MAJOR}.${CUDA_MINOR}.0" > /usr/local/cuda/version.txt

# setup the ZED SDK
RUN apt-get update -y || true ; apt-get install --no-install-recommends lsb-release wget less udev sudo zstd build-essential cmake python3 python3-pip libpng-dev libgomp1 -y && \ 
    python3 -m pip install numpy opencv-python && \
    wget -q -O ZED_SDK_Linux_Ubuntu${UBUNTU_RELEASE_YEAR}.run https://download.stereolabs.com/zedsdk/${ZED_SDK_MAJOR}.${ZED_SDK_MINOR}/cu${CUDA_MAJOR}${CUDA_MINOR%.*}/ubuntu${UBUNTU_RELEASE_YEAR} && \
    chmod +x ZED_SDK_Linux_Ubuntu${UBUNTU_RELEASE_YEAR}.run && \
    ./ZED_SDK_Linux_Ubuntu${UBUNTU_RELEASE_YEAR}.run -- silent skip_tools skip_cuda && \
    ln -sf /lib/x86_64-linux-gnu/libusb-1.0.so.0 /usr/lib/x86_64-linux-gnu/libusb-1.0.so && \
    rm ZED_SDK_Linux_Ubuntu${UBUNTU_RELEASE_YEAR}.run && \
    rm -rf /var/lib/apt/lists/*
RUN conda install -c conda-forge libstdcxx-ng requests # required for pyzed
RUN python /usr/local/zed/get_python_api.py && python -m pip install --ignore-installed /app/pyzed-4.0-cp37-cp37m-linux_x86_64.whl

# install oculus reader
RUN add-apt-repository universe && \
    apt-get update -y && \ 
    apt-get install -y android-tools-fastboot
RUN pip3 install -e $NUC_OCULUS_DIR && \
    apt install -y android-tools-adb

# python environment setup
RUN pip3 install -e . && \
    pip3 install dm-robotics-moma==0.5.0 --no-deps && \
    pip3 install dm-robotics-transformations==0.5.0 --no-deps && \
    pip3 install dm-robotics-agentflow==0.5.0 --no-deps && \
    pip3 install dm-robotics-geometry==0.5.0 --no-deps && \
    pip3 install dm-robotics-manipulation==0.5.0 --no-deps && \
    pip3 install dm-robotics-controllers==0.5.0 --no-deps


# using miniconda instead of anaconda so overwrite sh scripts
RUN find /app/droid/franka -type f -name "launch_*.sh" -exec sed -i 's/anaconda/miniconda/g' {} \;
RUN find /app/scripts/server -type f -name "launch_server.sh" -exec sed -i 's/anaconda/miniconda/g' {} \;

# set absolute paths
RUN find /app/droid/franka -type f -name "launch_*.sh" -exec sed -i 's|~|/root|g' {} \;
RUN find /app/scripts/server -type f -name "launch_server.sh" -exec sed -i 's|~|/root|g' {} \;

# set polymetis config files
RUN cp ${NUC_ROBOT_CONFIG_DIR}/franka_hardware.yaml ${NUC_POLYMETIS_CONFIG_DIR}/robot_client/franka_hardware.yaml && \
    cp ${NUC_ROBOT_CONFIG_DIR}/franka_panda.yaml ${NUC_POLYMETIS_CONFIG_DIR}/robot_model/franka_panda.yaml

# start the server
RUN chmod +x /app/.docker/laptop/entrypoint.sh
ENTRYPOINT ["/app/.docker/laptop/entrypoint.sh"]



================================================
FILE: .docker/laptop/Dockerfile.laptop_panda
================================================
FROM nvidia/cuda:12.1.0-devel-ubuntu22.04

# set robot parameters
ENV ROBOT_TYPE=panda
ENV LIBFRANKA_VERSION=0.9.0
ENV ROBOT_IP=172.16.0.1 
ENV NUC_IP=172.16.0.2
ENV NVIDIA_VISIBLE_DEVICES=all
ENV NVIDIA_DRIVER_CAPABILITIES \
    ${NVIDIA_DRIVER_CAPABILITIES:+$NVIDIA_DRIVER_CAPABILITIES,}compute,video,utility

# set directory structure
ARG NUC_ROBOT_CONFIG_DIR=/app/config/${ROBOT_TYPE}
ARG NUC_OCULUS_DIR=/app/droid/oculus_reader
ARG NUC_POLYMETIS_DIR=/app/droid/fairo/polymetis
ARG NUC_POLYMETIS_CONFIG_DIR=${NUC_POLYMETIS_DIR}/polymetis/conf

# copy project code to container
COPY . /app
WORKDIR /app

# base system installations
RUN apt-get update && \
    apt-get install -y software-properties-common build-essential sudo git curl wget python3-pip libspdlog-dev \
    libeigen3-dev lsb-release ffmpeg libsm6 libxext6 zstd && \
    apt-get upgrade -y

# install miniconda 
RUN wget \
    https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh \
    && mkdir /root/.conda \
    && bash Miniconda3-latest-Linux-x86_64.sh -b \
    && rm -f Miniconda3-latest-Linux-x86_64.sh
ENV PATH /root/miniconda3/bin:$PATH

# create conda environment
RUN conda create -n "robot" python=3.7
SHELL ["conda", "run", "-n", "robot", "/bin/bash", "-c"]

# install the zed sdk
ARG UBUNTU_RELEASE_YEAR=22
ARG ZED_SDK_MAJOR=4
ARG ZED_SDK_MINOR=0
ARG CUDA_MAJOR=12
ARG CUDA_MINOR=1

RUN echo "Europe/Paris" > /etc/localtime ; echo "CUDA Version ${CUDA_MAJOR}.${CUDA_MINOR}.0" > /usr/local/cuda/version.txt

# setup the ZED SDK
RUN apt-get update -y || true ; apt-get install --no-install-recommends lsb-release wget less udev sudo zstd build-essential cmake python3 python3-pip libpng-dev libgomp1 -y && \ 
    python3 -m pip install numpy opencv-python && \
    wget -q -O ZED_SDK_Linux_Ubuntu${UBUNTU_RELEASE_YEAR}.run https://download.stereolabs.com/zedsdk/${ZED_SDK_MAJOR}.${ZED_SDK_MINOR}/cu${CUDA_MAJOR}${CUDA_MINOR%.*}/ubuntu${UBUNTU_RELEASE_YEAR} && \
    chmod +x ZED_SDK_Linux_Ubuntu${UBUNTU_RELEASE_YEAR}.run && \
    ./ZED_SDK_Linux_Ubuntu${UBUNTU_RELEASE_YEAR}.run -- silent skip_tools skip_cuda && \
    ln -sf /lib/x86_64-linux-gnu/libusb-1.0.so.0 /usr/lib/x86_64-linux-gnu/libusb-1.0.so && \
    rm ZED_SDK_Linux_Ubuntu${UBUNTU_RELEASE_YEAR}.run && \
    rm -rf /var/lib/apt/lists/*

RUN conda install -c conda-forge libstdcxx-ng requests # required for pyzed
RUN python /usr/local/zed/get_python_api.py && python -m pip install --ignore-installed /app/pyzed-4.0-cp37-cp37m-linux_x86_64.whl

# install oculus reader
RUN add-apt-repository universe && \
    apt-get update -y && \ 
    apt-get install -y android-tools-fastboot
RUN pip3 install -e $NUC_OCULUS_DIR && \
    apt install -y android-tools-adb && \
    rm -rf /var/lib/apt/lists/*

# python environment setup
RUN pip3 install -e . && \
    pip3 install dm-robotics-moma==0.5.0 --no-deps && \
    pip3 install dm-robotics-transformations==0.5.0 --no-deps && \
    pip3 install dm-robotics-agentflow==0.5.0 --no-deps && \
    pip3 install dm-robotics-geometry==0.5.0 --no-deps && \
    pip3 install dm-robotics-manipulation==0.5.0 --no-deps && \
    pip3 install dm-robotics-controllers==0.5.0 --no-deps


# using miniconda instead of anaconda so overwrite sh scripts
RUN find /app/droid/franka -type f -name "launch_*.sh" -exec sed -i 's/anaconda/miniconda/g' {} \;
RUN find /app/scripts/server -type f -name "launch_server.sh" -exec sed -i 's/anaconda/miniconda/g' {} \;

# set absolute paths
RUN find /app/droid/franka -type f -name "launch_*.sh" -exec sed -i 's|~|/root|g' {} \;
RUN find /app/scripts/server -type f -name "launch_server.sh" -exec sed -i 's|~|/root|g' {} \;

# set polymetis config files
RUN cp ${NUC_ROBOT_CONFIG_DIR}/franka_hardware.yaml ${NUC_POLYMETIS_CONFIG_DIR}/robot_client/franka_hardware.yaml && \
    cp ${NUC_ROBOT_CONFIG_DIR}/franka_panda.yaml ${NUC_POLYMETIS_CONFIG_DIR}/robot_model/franka_panda.yaml

# start the server
RUN chmod +x /app/.docker/laptop/entrypoint.sh
ENTRYPOINT ["/app/.docker/laptop/entrypoint.sh"]



================================================
FILE: .docker/laptop/entrypoint.sh
================================================
#!/bin/bash

# activate conda
source ~/miniconda3/bin/activate
conda activate robot

# run user command
exec "$@"



================================================
FILE: .docker/nuc/docker-compose-nuc.yaml
================================================
version: "3"

services: 
  setup_nuc:
    image: ghcr.io/droid-dataset/droid_nuc:${ROBOT_TYPE}
    environment:
      ROBOT_TYPE: ${ROBOT_TYPE}
      LIBFRANKA_VERSION: ${LIBFRANKA_VERSION}
      NUC_IP: ${NUC_IP}
      ROBOT_IP: ${ROBOT_IP}
      LAPTOP_IP: ${LAPTOP_IP}
    volumes:
      - ${ROOT_DIR}/droid/misc/parameters.py:/app/droid/misc/parameters.py
      - ${ROOT_DIR}/config/${ROBOT_TYPE}/franka_hardware.yaml:/app/droid/fairo/polymetis/polymetis/conf/robot_client/franka_hardware.yaml
      - ${ROOT_DIR}/config/${ROBOT_TYPE}/franka_panda.yaml:/app/droid/fairo/polymetis/polymetis/conf/robot_model/franka_panda.yaml
    build: 
      context: ../../
      dockerfile: .docker/nuc/Dockerfile.nuc
      args:
        ROBOT_TYPE: ${ROBOT_TYPE}
        LIBFRANKA_VERSION: ${LIBFRANKA_VERSION}
        NUC_IP: ${NUC_IP}
        ROBOT_IP: ${ROBOT_IP}
        LAPTOP_IP: ${LAPTOP_IP}
    devices:
      - "/dev:/dev"
    privileged: true
    network_mode: "host"
    cap_add:
      - SYS_NICE
    ulimits:
      rtprio: 99
      memlock: 102400
    deploy:
      restart_policy: 
        condition: any




================================================
FILE: .docker/nuc/Dockerfile.nuc
================================================
# pull ubuntu base image
FROM ubuntu:bionic

# build args
ARG ROBOT_TYPE=${ROBOT_TYPE}
ARG LIBFRANKA_VERSION=${LIBFRANKA_VERSION}
ARG ROBOT_IP=${ROBOT_IP} 
ARG NUC_IP=${NUC_IP}
ARG NUC_POLYMETIS_DIR=/app/droid/fairo/polymetis
ARG NUC_ROBOT_CONFIG_DIR=/app/config/${ROBOT_TYPE}
ARG NUC_POLYMETIS_CONFIG_DIR=${NUC_POLYMETIS_DIR}/polymetis/conf

# runtime env vars
ENV ROBOT_TYPE=${ROBOT_TYPE}
ENV LIBFRANKA_VERSION=${LIBFRANKA_VERSION}
ENV ROBOT_IP=${ROBOT_IP} 
ENV NUC_IP=${NUC_IP}

# base system installations
RUN apt-get update && \
    apt-get install -y software-properties-common build-essential sudo git curl wget python3-pip libspdlog-dev \
    libeigen3-dev lsb-release ffmpeg libsm6 libxext6 && \
    apt-get upgrade -y 

# install latest cmake version
RUN wget -O - https://apt.kitware.com/keys/kitware-archive-latest.asc 2>/dev/null | gpg --dearmor - | tee /etc/apt/trusted.gpg.d/kitware.gpg >/dev/null && \
    apt-add-repository 'deb https://apt.kitware.com/ubuntu/ bionic main' && \  
    apt-get update && \
    apt-get install -y cmake

# copy project code to container
COPY . /app
WORKDIR /app

# install miniconda 
RUN wget \
    https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh \
    && mkdir /root/.conda \
    && bash Miniconda3-latest-Linux-x86_64.sh -b \
    && rm -f Miniconda3-latest-Linux-x86_64.sh
ENV PATH /root/miniconda3/bin:$PATH

# setup polymetis conda env
WORKDIR ${NUC_POLYMETIS_DIR}
RUN conda env create -f ./polymetis/environment.yml && \
    conda run -n polymetis-local pip install -e ./polymetis
SHELL ["conda", "run", "-n", "polymetis-local", "/bin/bash", "-c"]

# build libfranka and polymetis
RUN ./scripts/build_libfranka.sh ${LIBFRANKA_VERSION} && \
    mkdir -p ./polymetis/build && \
    cd ./polymetis/build && \
    cmake .. -DCMAKE_BUILD_TYPE=Release -DBUILD_FRANKA=ON -DBUILD_TESTS=ON -DBUILD_DOCS=ON && \
    make -j

# set polymetis config files
WORKDIR /app
RUN cp ${NUC_ROBOT_CONFIG_DIR}/franka_hardware.yaml ${NUC_POLYMETIS_CONFIG_DIR}/robot_client/franka_hardware.yaml && \
    cp ${NUC_ROBOT_CONFIG_DIR}/franka_panda.yaml ${NUC_POLYMETIS_CONFIG_DIR}/robot_model/franka_panda.yaml

# python environment setup 
RUN pip3 install -e . && \
    pip3 install dm-robotics-moma==0.5.0 --no-deps && \
    pip3 install dm-robotics-transformations==0.5.0 --no-deps && \
    pip3 install dm-robotics-agentflow==0.5.0 --no-deps && \
    pip3 install dm-robotics-geometry==0.5.0 --no-deps && \
    pip3 install dm-robotics-manipulation==0.5.0 --no-deps && \
    pip3 install dm-robotics-controllers==0.5.0 --no-deps

# using miniconda instead of anaconda so overwrite sh scripts
RUN find /app/droid/franka -type f -name "launch_*.sh" -exec sed -i 's/anaconda/miniconda/g' {} \;
RUN find /app/scripts/server -type f -name "launch_server.sh" -exec sed -i 's/anaconda/miniconda/g' {} \;

# set absolute paths
RUN find /app/droid/franka -type f -name "launch_*.sh" -exec sed -i 's|~|/root|g' {} \;
RUN find /app/scripts/server -type f -name "launch_server.sh" -exec sed -i 's|~|/root|g' {} \;

# start the server
RUN chmod +x /app/scripts/server/launch_server.sh
ENTRYPOINT ["/app/scripts/server/launch_server.sh"]




================================================
FILE: .docker/nuc/Dockerfile.nuc_fr3
================================================
# pull ubuntu base image
FROM ubuntu:bionic

# set robot parameters
ENV ROBOT_TYPE=fr3
ENV LIBFRANKA_VERSION=0.10.0
ENV ROBOT_IP=172.16.0.1 
ENV NUC_IP=172.16.0.2

# set directory structure
ARG NUC_POLYMETIS_DIR=/app/droid/fairo/polymetis
ARG NUC_ROBOT_CONFIG_DIR=/app/config/${ROBOT_TYPE}
ARG NUC_POLYMETIS_CONFIG_DIR=${NUC_POLYMETIS_DIR}/polymetis/conf

# base system installations
RUN apt-get update && \
    apt-get install -y software-properties-common build-essential sudo git curl wget python3-pip libspdlog-dev \
    libeigen3-dev lsb-release ffmpeg libsm6 libxext6 && \
    apt-get upgrade -y

# install latest cmake version
RUN wget -O - https://apt.kitware.com/keys/kitware-archive-latest.asc 2>/dev/null | gpg --dearmor - | tee /etc/apt/trusted.gpg.d/kitware.gpg >/dev/null && \
    apt-add-repository 'deb https://apt.kitware.com/ubuntu/ bionic main' && \  
    apt-get update && \
    apt-get install -y cmake

# copy project code to container
COPY . /app
WORKDIR /app

# install miniconda 
RUN wget \
    https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh \
    && mkdir /root/.conda \
    && bash Miniconda3-latest-Linux-x86_64.sh -b \
    && rm -f Miniconda3-latest-Linux-x86_64.sh
ENV PATH /root/miniconda3/bin:$PATH

# setup polymetis conda env
WORKDIR ${NUC_POLYMETIS_DIR}
RUN conda env create -f ./polymetis/environment.yml && \
    conda run -n polymetis-local pip install -e ./polymetis
SHELL ["conda", "run", "-n", "polymetis-local", "/bin/bash", "-c"]

# build libfranka and polymetis
RUN ./scripts/build_libfranka.sh ${LIBFRANKA_VERSION} && \
    mkdir -p ./polymetis/build && \
    cd ./polymetis/build && \
    cmake .. -DCMAKE_BUILD_TYPE=Release -DBUILD_FRANKA=ON -DBUILD_TESTS=ON -DBUILD_DOCS=ON && \
    make -j

# set polymetis config files
WORKDIR /app
RUN cp ${NUC_ROBOT_CONFIG_DIR}/franka_hardware.yaml ${NUC_POLYMETIS_CONFIG_DIR}/robot_client/franka_hardware.yaml && \
    cp ${NUC_ROBOT_CONFIG_DIR}/franka_panda.yaml ${NUC_POLYMETIS_CONFIG_DIR}/robot_model/franka_panda.yaml

# python environment setup 
RUN pip3 install -e . && \
    pip3 install dm-robotics-moma==0.5.0 --no-deps && \
    pip3 install dm-robotics-transformations==0.5.0 --no-deps && \
    pip3 install dm-robotics-agentflow==0.5.0 --no-deps && \
    pip3 install dm-robotics-geometry==0.5.0 --no-deps && \
    pip3 install dm-robotics-manipulation==0.5.0 --no-deps && \
    pip3 install dm-robotics-controllers==0.5.0 --no-deps

# using miniconda instead of anaconda so overwrite sh scripts
RUN find /app/droid/franka -type f -name "launch_*.sh" -exec sed -i 's/anaconda/miniconda/g' {} \;
RUN find /app/scripts/server -type f -name "launch_server.sh" -exec sed -i 's/anaconda/miniconda/g' {} \;

# set absolute paths
RUN find /app/droid/franka -type f -name "launch_*.sh" -exec sed -i 's|~|/root|g' {} \;
RUN find /app/scripts/server -type f -name "launch_server.sh" -exec sed -i 's|~|/root|g' {} \;

# start the server
RUN chmod +x /app/scripts/server/launch_server.sh
ENTRYPOINT ["/app/scripts/server/launch_server.sh"]




================================================
FILE: .docker/nuc/Dockerfile.nuc_panda
================================================
# pull ubuntu base image
FROM ubuntu:bionic

# set robot parameters
ENV ROBOT_TYPE=panda
ENV LIBFRANKA_VERSION=0.9.0
ENV ROBOT_IP=172.16.0.1 
ENV NUC_IP=172.16.0.2

# set directory structure
ARG NUC_POLYMETIS_DIR=/app/droid/fairo/polymetis
ARG NUC_ROBOT_CONFIG_DIR=/app/config/${ROBOT_TYPE}
ARG NUC_POLYMETIS_CONFIG_DIR=${NUC_POLYMETIS_DIR}/polymetis/conf

# base system installations
RUN apt-get update && \
    apt-get install -y software-properties-common build-essential sudo git curl wget python3-pip libspdlog-dev \
    libeigen3-dev lsb-release ffmpeg libsm6 libxext6 && \
    apt-get upgrade -y

# install latest cmake version
RUN wget -O - https://apt.kitware.com/keys/kitware-archive-latest.asc 2>/dev/null | gpg --dearmor - | tee /etc/apt/trusted.gpg.d/kitware.gpg >/dev/null && \
    apt-add-repository 'deb https://apt.kitware.com/ubuntu/ bionic main' && \  
    apt-get update && \
    apt-get install -y cmake

# copy project code to container
COPY . /app
WORKDIR /app

# install miniconda 
RUN wget \
    https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh \
    && mkdir /root/.conda \
    && bash Miniconda3-latest-Linux-x86_64.sh -b \
    && rm -f Miniconda3-latest-Linux-x86_64.sh
ENV PATH /root/miniconda3/bin:$PATH

# setup polymetis conda env
WORKDIR ${NUC_POLYMETIS_DIR}
RUN conda env create -f ./polymetis/environment.yml && \
    conda run -n polymetis-local pip install -e ./polymetis
SHELL ["conda", "run", "-n", "polymetis-local", "/bin/bash", "-c"]

# build libfranka and polymetis
RUN ./scripts/build_libfranka.sh ${LIBFRANKA_VERSION} && \
    mkdir -p ./polymetis/build && \
    cd ./polymetis/build && \
    cmake .. -DCMAKE_BUILD_TYPE=Release -DBUILD_FRANKA=ON -DBUILD_TESTS=ON -DBUILD_DOCS=ON && \
    make -j

# set polymetis config files
WORKDIR /app
RUN cp ${NUC_ROBOT_CONFIG_DIR}/franka_hardware.yaml ${NUC_POLYMETIS_CONFIG_DIR}/robot_client/franka_hardware.yaml && \
    cp ${NUC_ROBOT_CONFIG_DIR}/franka_panda.yaml ${NUC_POLYMETIS_CONFIG_DIR}/robot_model/franka_panda.yaml

# python environment setup 
RUN pip3 install -e . && \
    pip3 install dm-robotics-moma==0.5.0 --no-deps && \
    pip3 install dm-robotics-transformations==0.5.0 --no-deps && \
    pip3 install dm-robotics-agentflow==0.5.0 --no-deps && \
    pip3 install dm-robotics-geometry==0.5.0 --no-deps && \
    pip3 install dm-robotics-manipulation==0.5.0 --no-deps && \
    pip3 install dm-robotics-controllers==0.5.0 --no-deps

# using miniconda instead of anaconda so overwrite sh scripts
RUN find /app/droid/franka -type f -name "launch_*.sh" -exec sed -i 's/anaconda/miniconda/g' {} \;
RUN find /app/scripts/server -type f -name "launch_server.sh" -exec sed -i 's/anaconda/miniconda/g' {} \;

# set absolute paths
RUN find /app/droid/franka -type f -name "launch_*.sh" -exec sed -i 's|~|/root|g' {} \;
RUN find /app/scripts/server -type f -name "launch_server.sh" -exec sed -i 's|~|/root|g' {} \;

# start the server
RUN chmod +x /app/scripts/server/launch_server.sh
ENTRYPOINT ["/app/scripts/server/launch_server.sh"]




================================================
FILE: .github/workflows/build_container_laptop_fr3.yaml
================================================
name: build laptop fr3 container and host on ghcr.io

on:
  push:
    branches: ['main']
    paths:
      - .gitmodules
      - droid/**
      - .docker/laptop/*
      - .github/workflows/build_container_laptop_fr3
  workflow_dispatch:

env:
  REGISTRY: ghcr.io
  ORG_NAME: droid-dataset
  IMAGE_NAME: droid_laptop

jobs:
  build-and-push-image:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write

    steps:

      - name: Checkout repository
        uses: actions/checkout@v3
        with:
          submodules: recursive
      
      - name: Free Space
        run: |
          sudo rm -rf /opt/ghc
          sudo rm -rf /opt/hostedtoolcache
          sudo rm -rf /usr/share/dotnet
          sudo rm -rf "$AGENT_TOOLSDIRECTORY"
          sudo rm -rf ${GITHUB_WORKSPACE}/.git

      - name: Set up QEMU
        uses: docker/setup-qemu-action@v2
      
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v2

      - name: Log in to the Container registry
        uses: docker/login-action@f054a8b539a109f9f41c372932f1ae047eff08c9
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Build and push Docker image
        uses: docker/build-push-action@v3
        with:
          context: .
          file: .docker/laptop/Dockerfile.laptop_fr3
          push: true
          no-cache: true
          tags: ${{ env.REGISTRY }}/${{ env.ORG_NAME }}/${{ env.IMAGE_NAME }}:fr3



================================================
FILE: .github/workflows/build_container_laptop_panda.yaml
================================================
name: build laptop panda container and host on ghcr.io

on:
  push:
    branches: ['main']
    paths:
      - .gitmodules
      - droid/**
      - .docker/laptop/*
      - .github/workflows/build_container_laptop_panda
  workflow_dispatch:

env:
  REGISTRY: ghcr.io
  ORG_NAME: droid-dataset
  IMAGE_NAME: droid_laptop

jobs:
  build-and-push-image:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write

    steps:

      - name: Checkout repository
        uses: actions/checkout@v3
        with:
          submodules: recursive

      - name: Free Space
        run: |
          sudo rm -rf /opt/ghc
          sudo rm -rf /opt/hostedtoolcache
          sudo rm -rf /usr/share/dotnet
          sudo rm -rf "$AGENT_TOOLSDIRECTORY"
          sudo rm -rf ${GITHUB_WORKSPACE}/.git

      - name: Set up QEMU
        uses: docker/setup-qemu-action@v2
      
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v2

      - name: Log in to the Container registry
        uses: docker/login-action@f054a8b539a109f9f41c372932f1ae047eff08c9
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Build and push Docker image
        uses: docker/build-push-action@v3
        with:
          context: .
          file: .docker/laptop/Dockerfile.laptop_panda
          push: true
          no-cache: true
          tags: ${{ env.REGISTRY }}/${{ env.ORG_NAME }}/${{ env.IMAGE_NAME }}:panda



================================================
FILE: .github/workflows/build_container_nuc_fr3.yaml
================================================
name: build nuc fr3 container and host on ghcr.io

on:
  push:
    branches: ['main']
    paths:
      - .gitmodules
      - droid/**
      - .docker/nuc/*
      - .github/workflows/build_container_nuc_fr3
  workflow_dispatch:

env:
  REGISTRY: ghcr.io
  ORG_NAME: droid-dataset
  IMAGE_NAME: droid_nuc

jobs:
  build-and-push-image:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write

    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
        with:
          submodules: recursive

      - name: Set up QEMU
        uses: docker/setup-qemu-action@v2
      
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v2

      - name: Log in to the Container registry
        uses: docker/login-action@f054a8b539a109f9f41c372932f1ae047eff08c9
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Build and push Docker image
        uses: docker/build-push-action@v3
        with:
          context: .
          file: .docker/nuc/Dockerfile.nuc_fr3
          push: true
          no-cache: true
          tags: ${{ env.REGISTRY }}/${{ env.ORG_NAME }}/${{ env.IMAGE_NAME }}:fr3



================================================
FILE: .github/workflows/build_container_nuc_panda.yaml
================================================
name: build nuc panda container and host on ghcr.io

on:
  push:
    branches: ['main']
    paths:
      - .gitmodules
      - droid/**
      - .docker/nuc/*
      - .github/workflows/build_container_nuc_panda
  workflow_dispatch:

env:
  REGISTRY: ghcr.io
  ORG_NAME: droid-dataset
  IMAGE_NAME: droid_nuc

jobs:
  build-and-push-image:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write

    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
        with:
          submodules: recursive

      - name: Set up QEMU
        uses: docker/setup-qemu-action@v2
      
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v2

      - name: Log in to the Container registry
        uses: docker/login-action@f054a8b539a109f9f41c372932f1ae047eff08c9
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Build and push Docker image
        uses: docker/build-push-action@v3
        with:
          context: .
          file: .docker/nuc/Dockerfile.nuc_panda
          push: true
          no-cache: true
          tags: ${{ env.REGISTRY }}/${{ env.ORG_NAME }}/${{ env.IMAGE_NAME }}:panda



================================================
FILE: .github/workflows/pages.yaml
================================================
name: Deploy Documentation

on:
  push:
    branches: 
      - "main"
    paths:
      - "docs/**"
  workflow_dispatch:

permissions:
  contents: read
  pages: write
  id-token: write

# Allow one concurrent deployment
concurrency:
  group: "pages"
  cancel-in-progress: true

jobs:
  # Build job
  build:
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: docs
    steps:
      - name: Checkout
        uses: actions/checkout@v3
      - name: Setup Ruby
        uses: ruby/setup-ruby@v1
        with:
          ruby-version: '3.1' # Not needed with a .ruby-version file
          bundler-cache: true # runs 'bundle install' and caches installed gems automatically
          cache-version: 0 # Increment this number if you need to re-download cached gems
          working-directory: '${{ github.workspace }}/docs'
      - name: Setup Pages
        id: pages
        uses: actions/configure-pages@v3
      - name: Build with Jekyll
        run: bundle exec jekyll build --baseurl "${{ steps.pages.outputs.base_path }}"
        env:
          JEKYLL_ENV: production
      - name: Upload artifact
        uses: actions/upload-pages-artifact@v1
        with:
          path: "docs/_site/"

  deploy:
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    runs-on: ubuntu-latest
    needs: build
    steps:
      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v2



================================================
FILE: .github/workflows/pre-commit.yaml
================================================
name: pre-commit

on:
  workflow_dispatch:
  schedule:
    - cron: "0 9 * * 1"
  pull_request:
  push:
    branches: [main]

jobs:
  pre-commit:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v2
      with:
        fetch-depth: 0
    - uses: actions/setup-python@v2
    - uses: pre-commit/action@v2.0.3
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
        extra_args: --files


